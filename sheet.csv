,,,,General,,,,,Scaling Law Hypothesis,,,,Training Setup,,,,,,,Data Extraction,,,,,Fitting the Scaling Law,,,,,,
Title,Link ,citation,Paper,Domain,Training Code?,Analysis Code?,Check- points?,Metric Scores?,Power Law Form,"Purpose of Power Law (e.g., Performance Prediction, Optimal ratio)",# Power Law parameters,# of scaling laws,Training Runs / Law,Max. Training FLOPs,Max. Training Params,Max. Training Data,Data Described?,Hyperparameters Described?,"How are model params counted (e.g., w/ or w/out embeddings)",Data points per law?,Scaling law metric,Modification of final metric?,Subsets of Data Used,Bootstrapping?,Curve-fitting Method,Loss Objective,Hyperparameters Reported?,Initialization,Are scaling laws validated?,Anecdotes ,flop counts definition?
A constructive prediction of the generalization error across scales,[1909.12673] A Constructive Prediction of the Generalization Error Across Scales,rosenfeld2019constructive,\cite{rosenfeld2019constructive},"Vision, LM",N,N,N,N,"$\tilde{\epsilon}(m, n)=a n^{-\alpha}+b m^{-\beta}+c_{\infty}$",Performance Prediction,5-6,8,42-49,,0.7M-70M,100M words / 1.2M images,Y,Y,Non-embedding,42-49,Loss / Top1 Error,N,N,N,Least Squares Regression,Custom error term,N/A,Random,Y,,Y
A scaling law for synthetic-to-real transfer: How much is your pre-training effective?,[2108.11018] A Scaling Law for Synthetic-to-Real Transfer: How Much Is Your Pre-training Effective?,mikamiscaling,\cite{mikamiscaling},Vision,N,Y,Y,Y,"$L(n, s)=\delta\left(\gamma+n^{-\alpha}\right) s^{-\beta}$",Performance Prediction,4,3,7,,ResNet-101,64k-1.28M images,Y,Y,NA,7,Error Rate,N,N,N,Non-linear Least Squares in log-log space,,N/A,N/A,Y,"we fixed a common D for all the cases and estimated α for each. To determine D, we used the following procedure. First, we prepared two global parameters α, ˆ Dˆ and set 0.5 as their initial values.",NA
Are emergent abilities of large language models a mirage?,[2304.15004] Are Emergent Abilities of Large Language Models a Mirage?,schaeffer2023emergent,\cite{schaeffer2023emergent},LM,N,N,N,N,None,N/A,NA,NA,4,,$10^{11}$,NA,Y,NA,Non-embedding,NA,Various downstream,NA,NA,NA,NA,NA,NA,NA,NA,,NA
Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws,https://www.semanticscholar.org/paper/Beyond-Chinchilla-Optimal%3A-Accounting-for-Inference-Sardana-Frankle/82f75d838e92196864131bad25b1abc3b5d40a6f?utm_content=title&utm_medium=unfurl&utm_source=slackbot,sardana2023beyond,\cite{sardana2023beyond},LM,N,N,N,N,"$L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} $; $N^*\left(\ell, D_{\text {inf }}\right), D_{\text {tr }}^*\left(\ell, D_{\text {inf }}\right)={\arg \min } _{N, D_{\mathrm{tr}} \mid L\left(N, D_{\mathrm{tr}}\right)=\ell}$",Performance Prediction,5,4,47,,150M-6B,1.5B-1.25T tokens,N,Y,NA,NS,Loss,NS,NS,N,L-BFGS,Huber Loss,Y,Grid Search,N,,$6ND$
Beyond neural scaling laws: beating power law scaling via data pruning,[2206.14486] Beyond neural scaling laws: beating power law scaling via data pruning,sorscher2022beyond,\cite{sorscher2022beyond},Vision,N,N,N,Y,"$c \cdot \alpha^{-\beta} ,  c \cdot \exp (-b \alpha)$",Performance Prediction,2,34,~60,,86M (ViT),200 epochs,Y,Y,NA,~60,Error Rate,NA,NA,NA,NA,NA,NA,NA,NA,,NA
Broken neural scaling laws,[2210.14891] Broken Neural Scaling Laws,caballero2022broken,\cite{caballero2022broken},LM,N,Y,N,Y,$y=a+\left(b x^{-c_0}\right) \prod_{i=1}^n\left(1+\left(\frac{x}{d_i}\right)^{1 / f_i}\right)^{-c_i * f_i}$,Performance Prediction,5+,100+,3-40,,NS,NS,N,N,NS,3-40,"FID, Loss, Error Rate, Elo Score",N,NS,N,Least Squares Regression,MSLE,N/A,"Grid Search, optimize one",Y,,N
Chinchilla Scaling: A replication attempt,[2404.10102] Chinchilla Scaling: A replication attempt,besiroglu2024chinchilla,\cite{besiroglu2024chinchilla},LM,N/A,Y,N,Y,"$L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} $",Performance Prediction,5,1,NA,NA,NA,NA,Y,NA,Non-embedding,245,Loss,N,N,Y,L-BFGS,Huber Loss,Y,Grid Search,Y,,NA
Data and Parameter Scaling Laws for Neural Machine Translation,Data and Parameter Scaling Laws for Neural Machine Translation - ACL Anthology,gordon2021data,\cite{gordon2021data},NMT,Y,Y,Y,Y,"$L(N, D) = \left[ \left( \frac{N}{N_c}\right)^{\frac{\alpha_N}{\alpha_D}} + \frac{D}{D_c} \right]^{\alpha_D} $",Performance Prediction,4,3,45-55,,56M,28.3M-51.1M examples,Y,Y,Non-embedding,45-55,Loss,N,N,N,Least Squares Regression,,N/A,N.S.,N,,NA
Data scaling laws in nmt: The effect of noise and architecture,[2202.01994] Data Scaling Laws in NMT: The Effect of Noise and Architecture,bansal2022data,\cite{bansal2022data},NMT,N,N,N,N,$L(D)=\alpha\left(D^{-1}+C\right)^p$,Performance Prediction,3,20,10,,170M-800M,500K-512M sentences (28B tokens),Y,Y,NS,NS,"Loss, BLEU",NS,NS,NS,NS,NS,N,NS,N,,NA
"Deep Learning Scaling is Predictable, Empirically","[1712.00409] Deep Learning Scaling is Predictable, Empirically",hestness2017deep,\cite{hestness2017deep},"NMT, LM, Vision, Speech",N,N,N,N,$\varepsilon(m) \sim \alpha m^{\beta_g}+\gamma$,Performance Prediction,3,17,~9,,upto 193M ,"$2^{19}-2^{28}$ tokens, upto $2^9$ images, $2k$ audio hours",Y,Y,NS,NS,"Token Error, CER, Error Rate, Loss",Median min. validation error across multiple training runs with separate random seeds,NS,NS,NS,RMSE,N,NS,Y,,NA
DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,https://arxiv.org/abs/2401.02954,bi2024deepseek,\cite{bi2024deepseek},LM,N,N,N,N,"$\begin{aligned} M_{\mathrm{opt}} & =M_{\mathrm{base}} \cdot C^a \\ D_{\mathrm{opt}} & =D_{\mathrm{base}} \cdot C^b\end{aligned}$, $\begin{aligned} & \eta_{\mathrm{opt}}=0.3118 \cdot C^{-0.1250} \\ & B_{\mathrm{opt}}=0.2920 \cdot C^{0.3271}\end{aligned}$","Optimal Ratio, Performance Prediction",2,5,80,$1e17-3e20$,,,Y,Y,Non-embedding,upto 80,Validation bits-per-byte,NS,NS,NS,NS,NS,N,NS,Y,This one accounts for attention costs,"$C=MD$, $M=72 n_{\text {layer }} d_{\text {model }}^2+12 n_{\text {layer }} d_{\text {model }} l_{\text {seq }}$"
Explaining Neural Scaling Laws,https://arxiv.org/abs/2102.06701,bahri2021explaining,\cite{bahri2021explaining},Vision,N,N,N,N,"$L(D) \propto D^{-\alpha_K}, \quad L(P) \propto P^{-\alpha_K}$",Performance Prediction,2,35,8-27,,36.5M,upto 78k steps; 100 epochs,Y,Y,NS,upto 100,Loss,NS,NS,NS,NS,NS,N,NS,N,,NA
"How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization",https://arxiv.org/abs/2210.06441,geiping2022much,\cite{geiping2022much},Vision,Y,Y,N,N,"$f(x)=a x^{-c}+b$, $v_{\text {Effective Extra Samples from Augmentations }}(x)=f_{\text {ref }}^{-1}\left(f_{\text {aug }}(x)\right)-x$",Performance Prediction,3,~50,13,,ResNet-18,upto 7.6M images,Y,Y,NS,~50,Effective Extra Samples,Interpolation,NS,NS,Non-linear Least Squares,,NA,Non-augmented parameters,Y,,NA
Mechanistic Design and Scaling of Hybrid Architectures,[2403.17844] Mechanistic Design and Scaling of Hybrid Architectures,poli2024mechanistic,\cite{poli2024mechanistic},LM,N,N,N,N,$\log N^* \propto a \log C$ and $\log D^* \propto b \log C$,Performance Prediction,2,,500 total,8.00E+19,70M-7B,,Y,Y,Non-embedding ,NS,Loss,NS,NS,NS,NS,NS,N,NS,N,,Varies
MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies,[2404.06395] MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies,hu2024minicpm,\cite{hu2024minicpm},LM,Y,N,N,N,"$L(N, D)=C_N N^{-\alpha}+C_D D^{-\beta}+L_0$",Performance Prediction,5,6,36,,40M-2B,400M-120B tokens,Y,Y,Non-embedding ,NS,Loss,NS,NS,NS,scipy curvefit,NS,N,NS,N,,6ND
Model Performance Scaling with Multiple Data Sources,Model Performance Scaling with Multiple Data Sources,hashimoto2021model,\cite{hashimoto2021model},NLP,N,N,N,N,"$\min _{\lambda, \alpha} \mathbb{E}_{\hat{q}, \hat{n}}\left[\left(\log (R(\hat{n}, \hat{q})-\epsilon)-\alpha \log (\hat{n})+\log \left(C_\lambda(\hat{q})\right)\right)^2\right]$ $R(\hat{n}, \hat{q})=\mathbb{E}\left[\ell\left(\hat{\theta}\left(p_{\hat{n}, \hat{q}}\right) ; x, y\right)\right]$",Performance Prediction,2+n(data mixes),4,,,,upto 600k sentences,Y,Y,NA,NS,Loss,NS,NS,NS,Adagrad,Custom Loss,Y,Xavier,Y,,NA
Observational Scaling Laws and the Predictability of Language Model Performance,[2405.10938] Observational Scaling Laws and the Predictability of Language Model Performance,ruan2024observational,\cite{ruan2024observational},LM,Y,Y,N,Y,$E_m \approx h \sigma\left(\beta^{\top} S_m+\alpha\right)$,Performance Prediction,3,,27* -77*,,70B-180B,3T-6T tokens,N/A,N/A,N.S.,,Various downstream,N,N,N,Linear Least Squares,Various,N/A,N/A,Y,,
PaLM 2 technical report,https://arxiv.org/abs/2305.10403,anil2023palm,\cite{anil2023palm},LM,N,N,N,N,$N^{\star}(C) \approx N_0^{\star} \cdot C^a$,Performance Prediction,2,1,12,1.00E+22,15B,4.00E+11,N,N,Non-embedding,12,Loss,N,N,Interpolation?,Polynomial Regression (Quadratic),N.S.,N,N.S.,Y,,
Reconciling Kaplan and Chinchilla Scaling Laws,https://arxiv.org/abs/2406.12907,pearce2024reconciling,\cite{pearce2024reconciling},LM,N,Y,N,N,$N^*_{{\setminus E}} = b C_{{\setminus E}}^m$ $L = bC^m$,"Optimal Ratio, Performance Prediction",2,1,"20 (simulated), 25 (real)",,"1.5B (simulated), 4.6M (real)","23B (simulated), 500M (real) tokens",Y,Y,w/ Embedding and Non-embedding considered separately,"20, 5",Loss,N,N,N,Polynomial Least Squares,MSE on Log-loss,N/A,N/A,N,,
Reproducible scaling laws for contrastive language-image learning,https://arxiv.org/abs/2212.07143,cherti2023reproducible,\cite{cherti2023reproducible},VLM,Y,Y,Y,Y,$E=\beta C^{\alpha}$,Performance Prediction,2,8,3* - 29,,214M,"34B (pretrain), 2B (finetune) examples ",Y,Y,N.S,3-29,Error Rate,N,N,N,Linear Least Squares,MSE,N/A,N/A,N,,
Resolving Discrepancies in Compute-Optimal Scaling of Language Models,https://arxiv.org/pdf/2406.19146,porian2024resolving,\cite{porian2024resolving},LM,Y,Y,N,Y,$N^{\star}(C) \approx N_0^{\star} \cdot C^a$,Optimal Ratio,2,6,16,2.00E+19,901M,,Y,Y,w/ Embedding and Non-embedding considered separately,12,Loss,N,N,Interpolation?,Weighted Linear Regression,weighted SE on Log-loss,N/A,N/A,Y,,
Revisiting Neural Scaling Laws in Language and Vision,[2209.06640] Revisiting Neural Scaling Laws in Language and Vision,alabdulmohsin2022revisiting,\cite{alabdulmohsin2022revisiting},"LM, Vision",N,Y,Y,Y,$\varepsilon_x=\beta x^c$; $\varepsilon_x - $/varepsilon_\infty=\beta x^c$; $\varepsilon_x=\beta (x^{-1} + \gamma)^{-c}$;   $\varepsilon_x=\gamma(x)(1+\gamma(x))^{-1} \varepsilon_0+(1+\gamma(x))^{-1} \varepsilon_{\infty}$,Performance Prediction,2-4,~600,1*,,110M-1B,1e6-1e10 ex / 3e11 tokens,Mixed,N,N/A,N.S.,Loss / Accuracy,N,N/A,N,Least Squares Regression,MSE,Y,N.S.,Y,,
Scaling and evaluating sparse autoencoders,https://arxiv.org/abs/2406.04093,gao2024scalingevaluatingsparseautoencoders,\cite{gao2024scalingevaluatingsparseautoencoders},NLP,Y,Y,Y,N,"$L(n, k)=\exp \left(\alpha+\beta_k \log (k)+\beta_n \log (n)+\gamma \log (k) \log (n)\right)+\exp (\zeta+\eta \log (k))$",Performance Prediction,2-6,1,N.S,N.S,N.S,N.S,N,N,N.S,N.S,MSE,N.S,N.S,N.S,N.S,N.S,N.S,N.S,N.S,,
Scaling Data-Constrained Language Models,[2305.16264] Scaling Data-Constrained Language Models,muennighoff2024scaling,\cite{muennighoff2024scaling},LM,Y,Y,Y,N,"$L\left(U_N, U_D, R_N, R_D\right)=\frac{A}{\left(U_N+U_N R_N^*\left(1-e^{\frac{-R_N}{R_N^*}}\right)\right)^\alpha}+\frac{B}{\left(U_D+U_D R_D^*\left(1-e^{\frac{-R_D}{R_D^*}}\right)\right)^\beta}+E$",Performance Prediction,2 (+4),1,142,,"8,7B",900B tokens,Y,Y,w/ embedding,142,Loss,N,Outliers removed,N,L-BFGS,Huber on Log-loss,Y,"Grid Search, optimize all",Y,ties alpha and beta to be equal (assume linear relation b/t/ N and D,
"Scaling language models: Methods, analysis & insights from training gopher. a","[2112.11446] Scaling Language Models: Methods, Analysis & Insights from Training Gopher",rae2021scaling,\cite{rae2021scaling},LM,N,N,N,N,None,Performance Prediction,N/A,N/A,4,6.31E+23,280B,,Y,Y,Non-embedding,4,Loss,N/A,N/A,N/A,None,None,N/A,N/A,N,,
Scaling Law for Recommendation Models: Towards General-purpose User Representations: Towards General-purpose User Representations,https://arxiv.org/pdf/2111.11294,shin2023scaling,\cite{shin2023scaling},RecSys,N,N,N,N,None,Scaling trend,NA,NA,17,~0.1 PF Days,160M,500M-50B tokens,Y,Y,NA,NA,Loss,NA,NA,NA,NA,NA,NA,NA,NA,,
Scaling Laws and Interpretability of Learning from Repeated Data,https://arxiv.org/pdf/2205.10487,hernandez2022scaling,\cite{hernandez2022scaling},LM,N,N,N,N,$E=k * N^\alpha$,Optimal Ratio,2,1,56,,1.5M-800M,100B tokens,N,N,NS,NS,Loss,N,N,NS,NS,NS,NS,NS,NS,some poor performance regions modifications (double descent?),NA
Scaling Laws Beyond Backpropagation,https://arxiv.org/pdf/2210.14593,filipovich2022scaling,\cite{filipovich2022scaling},LM,N,N,N,N,$\mathcal{L}(C)=\left(C_c C\right)^{\alpha_C}$,Performance Prediction,2,3,4,,57-509M,30B token,Y,N,NS,NS,Loss,N,N,NS,NS,NS,NS,NS,NS,,
Scaling Laws for a Multi-Agent Reinforcement Learning Model,[2210.00849] Scaling Laws for a Multi-Agent Reinforcement Learning Model,neumann2022scaling,\cite{neumann2022scaling},RL,Y,Y*,Y,N,"$N_{\text {opt }}(C)=\left(\frac{C}{C_0}\right)^{\alpha_C^{o p t}}$, $E_i=\frac{1}{1+\left(N_j / N_i\right)^{\alpha_N}}$",Performance Prediction,2,3 * 2,14,,~$5*10^5$,$10^4$ steps,Y,Y,NS,238,Elo Score,N,N,NS,NS,NS,NS,NS,NS,,
Scaling Laws for Acoustic Models,[2106.09488] Scaling Laws for Acoustic Models,droppo2021scaling,\cite{droppo2021scaling},Speech,N,N,N,N,"$L(N, D)=\left[\left(L_{\infty}\right)^{\frac{1}{\alpha}}+\left(\frac{N_C}{N}\right)^{\frac{\alpha_N}{\alpha}}+\left(\frac{D_C}{D}\right)^{\frac{\alpha_D}{\alpha}}\right]^\alpha$",Performance Prediction,6,3,5-21,,~$10^7$,134-23k hrs speech,Y,Y,NS,NS,Loss,N,N,NS,NS,NS,NS,NS,NS,,
Scaling Laws for Autoregressive Generative Modeling,[2010.14701] Scaling Laws for Autoregressive Generative Modeling,henighan2020scaling,\cite{henighan2020scaling},"LM, Vision, Video, VLM",N,N,N,N,$L(x)=L_{\infty}+\left(\frac{x_0}{x}\right)^{\alpha_x}$,Performance Prediction,3,36,6-10,,~$10^11$,~$10^12$ tokens,Y,Y,Non-embedding,NS,"Loss, Error Rate",NS,Drop smaller models,NS,NS,NS,NS,NS,NS,,$6ND$
Scaling Laws for Data Filtering -- Data Curation cannot be Compute Agnostic,[2404.07177] Scaling Laws for Data Filtering -- Data Curation cannot be Compute Agnostic,goyal2024scaling,\cite{goyal2024scaling},"LM, Vision, VLM",N,Y,N,Y,$y_k=a \cdot n_1^{b_1} \prod_{j=2}^k\left(\frac{n_j}{n_{j-1}}\right)^{b_j}+d$,Performance Prediction,2+ 2*n(data mixes),1,5,,CLIP L/14 - ~300M +63M,32-640M samples,Y,Y,Embedding,NS,Error Rate,N,N,N,Grid Search,L2 error,Y,NA,Y,,
Scaling Laws for Generative Mixed-Modal Language Models,[2301.03728] Scaling Laws for Generative Mixed-Modal Language Models,aghajanyan2023scaling,\cite{aghajanyan2023scaling},Multimodal LM,N,N,N,N,"$L(N, D_j)=E_j + \frac{A_j}{N^{\alpha_j}} + \frac{B_j}{|D_j|^{\beta_j}}$, $L(N, D_i, D_j) = [\frac{L(N, D_i) + L(N, D_j)}{2}] - C_{i,j} + \frac{A_{i,j}}{N^{\alpha_{i,j}}} + \frac{B_{i,j}}{|D_i|+|D_j|^{\beta_{i,j}}}$",Performance Prediction,5,14,21,,8M-6.7B,5-100B tokens,Y,Y,Non-embedding,NS,Perplexity,N,N,N,L-BFGS,Huber on Log-loss,Y,"Grid Search, optimize all",Y,,
Scaling laws for neural language models.,[2001.08361] Scaling Laws for Neural Language Models,kaplan2020scaling,\cite{kaplan2020scaling},LM,N,N,N,N,"L(N, D) = \left[ \left( \frac{N}{N_c}\right)^{\frac{\alpha_N}{\alpha_D}} + \frac{D}{D_c} \right]^{\alpha_D}     ",Performance Prediction,4,~7,~40-150,,1.5B,23B tokens,Y,Y,Non-embedding,NS,Loss,NS,NS,NS,NS,NS,NS,NS,N,,"We used the estimated training compute $C \approx 6NBS$, which did not include contributions proportional to nctx (see Section 2.1). So our scalings with compute may be confounded in practice in the regime of very large nctx, specifically where nctx & 12dmodel."
Scaling Laws for Neural Machine Translation,[2109.07740] Scaling Laws for Neural Machine Translation,ghorbani2021scaling,\cite{ghorbani2021scaling},NMT,N,Y,N,N,"$\mathrm{BLEU}=c_B L^{-p_B}$, $\hat{L}_{o p t}(B)=\alpha^* B^{-\left(p_d+p_e\right)}+L_{\infty}, \quad \alpha^* \equiv \alpha\left(\frac{\bar{N}_e\left(p_e+p_d\right)}{p_e}\right)^{p_e}\left(\frac{\bar{N}_d\left(p_e+p_d\right)}{p_d}\right)^{p_d}$","Optimal Ratio, Performance Prediction",6,~8,12-14,,191-3B,NS,Y,Y,Non-embedding,NS,"Loss, BLEU",Median of last 50k steps,,NS,"Trust Region Reflective algorithm, Least Squares",Soft-L1 Loss,Y,Fixed,Y,,
Scaling Laws for Reward Model Overoptimization,[2210.10760] Scaling Laws for Reward Model Overoptimization,gao2023scaling,\cite{gao2023scaling},RL/LM,N,N,N,N,"$\begin{aligned} & R_{\mathrm{bo} n}(d)=d\left(\alpha_{\mathrm{bo} n}-\beta_{\mathrm{bo} n} d\right), \\ & R_{\mathrm{RL}}(d)=d\left(\alpha_{\mathrm{RL}}-\beta_{\mathrm{RL}} \log d\right)\end{aligned}$",Performance Prediction,2,2,9,,3B,120-90k,N,Y,NS,~90,RM Score,NS,NS,NS,NS,NS,NS,NS,Y,,
Scaling laws for single-agent reinforcement learning,https://arxiv.org/pdf/2301.13442,hilton2023scaling,\cite{hilton2023scaling},RL,N,N,N,N,$I^{-\beta}=\left(\frac{N_c}{N}\right)^{\alpha_N}+\left(\frac{E_c}{E}\right)^{\alpha_E}$,"Optimal Ratio, Performance Prediction",5,3,NS,$10^{20}$,,,Y,Y,NS,NS,Intrinsic Performance,Smoothing learning curve,Exclude early checkpoints,N,CMA-ES+Linear Regression,L2 log loss,Y,Fixed,Y,,
Scaling laws for sparsely-connected foundation models.,[2309.08520] Scaling Laws for Sparsely-Connected Foundation Models,frantar2023scaling,\cite{frantar2023scaling},"LM, Vision",N,N,N,N,"$L(S, N, D)=\left(a_S(1-S)^{b_S}+c_S\right) \cdot\left(\frac{1}{N}\right)^{b_N}+\left(\frac{a_D}{D}\right)^{b_D}+c$","Optimal Ratio, Performance Prediction",7,2,48 and 112,,0.66M-85M ,"1.8B images, 65B tokens",Y,Y,Non-embedding,48 and 112,Loss,NS,NS,NS,BFGS,Huber on Log-loss,Y,N Random Trials,Y,,
Scaling Laws for the Few-Shot Adaptation of Pre-trained Image Classifiers,[2110.06990] Scaling Laws for the Few-Shot Adaptation of Pre-trained Image Classifiers,prato2021scaling,\cite{prato2021scaling},Vision,Y*,Y,Y,Y,"$\begin{aligned} & \operatorname{Err}(N)=\operatorname{Err}_{\infty}+k N^\alpha, \\ & \operatorname{Err}(C)=\operatorname{Err}_{\infty}+k C^\alpha,\end{aligned}$",Performance Prediction,3,12,5,,,$10^6$ samples,Y,N,NA,5,Error Rate,NS,NS,NS,NS,NS,NS,NS,NS,,
Scaling Laws for the Value of Individual Data Points in Machine Learning,[2405.20456] Scaling Laws for the Value of Individual Data Points in Machine Learning,covert2024scaling,\cite{covert2024scaling},LM,Y,Y,N,N,$\log \left|\psi_k(z)\right| \approx \log |c(z)|-\alpha(z) \log (k)$,Performance Prediction,2,Many,10,,NA,1000 samples for IMDB,Y,Y,NA,(1000-5000 )*10,Expectation ,NS,N,N,Adam,Custom Loss,Y,NS,Y,,
Scaling Laws for Transfer,[2102.01293] Scaling Laws for Transfer,hernandez2021scaling,\cite{hernandez2021scaling},LM,N,N,N,N,$L \approx\left[\left(\frac{N_C}{N}\right)^{\frac{\alpha_N}{\alpha_D}}+\frac{D_C}{k\left(D_F\right)^\alpha(N)^\beta}\right]^{\alpha_D}$,Performance Prediction,3,1,NS,$10^{21}$,$10^8$,,Y,N,Non-embedding,40-120,Loss,NS,NS,NS,NS,NS,NS,NS,Y,,
Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments,https://arxiv.org/pdf/2202.06387,ivgi2022scaling,\cite{ivgi2022scaling},NLP,N,N,N,N,NS,Performance Prediction,NA,NA,5-8,,$10^4-10^8$,varies; 500k steps PT,Y,Y,Non-embedding,5-8,Loss,N,"[2.5, 97.5] percentile",Y,Linear Least Squares in Log-Log space,MSE,NA,NS,Y,,
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,[2207.10551] Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,tay2022scaling,\cite{tay2022scaling},LM,N,N,N,N,None,Scaling trend,NA,NA,,,16-30B,$2^19$,Y,Y,NA,NA,"Loss, Accuracy",NA,NA,NA,NA,NA,NA,NA,NA,,
Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies,https://arxiv.org/pdf/2407.13623,tao2024scaling,\cite{tao2024scaling},LM,N,Y,N,Y,"$N_{\mathrm{v}}^{\mathrm{opt}}=N_{\mathrm{v}}^0 *\left(\frac{N_{\mathrm{nv}}}{N_{\mathrm{nv}}^0}\right)^\gamma$, $\mathcal{L}_u=-E+\frac{A_1}{N_{\mathrm{nv}}^{\alpha_1}}+\frac{A_2}{N_{\mathrm{v}}^{\alpha_2}}+\frac{B}{D^\beta}$","Optimal Ratio, Performance Prediction",7,2,60,,33M-1.13B NV + 4-96k V,4.3B-509B Characters,Y,Y,Embedding and Non-embedding considered separately,20*60,Loss,Interpolation,NS,NS,"L-BFGS, Least Squares",Huber on Log-loss,Y,N Random Trials from Grid,Y,,
Scaling Scaling Laws with Board Games,[2104.03113] Scaling Scaling Laws with Board Games,jones2021scaling,\cite{jones2021scaling},RL,Y,Y,N,Y,"$\begin{aligned} \text { plateau } & =m_{\text {boardsize }}^{\text {plateau }} \cdot \text { boardsize }+c^{\text {plateau }} \\ \text { incline } & =m_{\text {boardsize }}^{\text {incline }} \cdot \text { boardsize }+m_{\text {flops }}^{\text {incline }} \cdot \log \text { flop }+c^{\text {incline }} \\ \text { elo } & =\text { incline.clamp }(\text { plateau }, 0)\end{aligned}$",Performance Prediction,5,1,200,1E+12-1E+17,,4E+08-2E+09,Y,Y,NA,2800,Elo Score,NS,NS,NS,L-BFGS,NS,NS,NS,NS,,
Scaling Vision Transformers,https://arxiv.org/pdf/2106.04560.pdf,zhai2022scaling,\cite{zhai2022scaling},Vision,Y,N,N,N,$E=\alpha+\beta(C+\gamma)^{-\mu}$,Performance Prediction,4,3,44,,5.4M-1.8B,1-13M images,Y,Y,NA,NS,Accuracy,NS,NS,NS,NS,NS,NS,NS,NS,,
The case for 4-bit precision: k-bit Inference Scaling Laws,https://arxiv.org/pdf/2212.09720,dettmers2023case,\cite{dettmers2023case},LM,N,N,N,N,None,Scaling trend,NA,NA,4,,19M-176B,NA,NA,Y,NA,NA,Accuracy,NA,NA,NA,NA,NA,NA,NA,NA,,
The Llama 3 Herd of Models (Section 3.2.1),https://scontent-sea1-1.xx.fbcdn.net/v/t39.2365-6/453304228_1160109801904614_7143520450792086005_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=c2Jg6fuPmP8Q7kNvgF4Aru_&_nc_ht=scontent-sea1-1.xx&oh=00_AYC_FwbKTdvQCkWZFhqUth4XCuEObbB0E3_Oyy2ILrRGtQ&oe=66C57D07,dubey2024llama,\cite{dubey2024llama},LM,N,N,N,N,$N^{\star}(C)=A C^\alpha$.,Optimal Ratio ,2,2,NS,$6*10^{18}-10^22$,40M-16B,,Y*,Y,NS,~150,"Loss, Accuracy",NS,NS,NS,NS,NS,NS,NS,Y,,
Training compute-optimal large language models,[2203.15556] Training Compute-Optimal Large Language Models,hoffmann2022training,\cite{hoffmann2022training},LM,N,N,N,N,"A3: $L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} $","Optimal Ratio, Performance Prediction",5,3,~200-450,$6*10^{18}-3*10^{21}$,16B,5B-400B tokens,Y,Y,Non-embedding,upto 1500,Loss,N,"Lowest loss model of a FLOP count, last 15% of checkpoints",Interpolation,L-BFGS,Huber on Log-loss,Y,"Grid Search, optimize all",Y,,
Understanding Scaling Laws for Recommendation Models,[2208.08489] Understanding Scaling Laws for Recommendation Models,ardalani2022understanding,\cite{ardalani2022understanding},RecSys,N,N,N,N,$\left(\alpha x^{-\beta}+\gamma\right)$,Performance Prediction,3,3,NS,$10^2$-$10^6$ TFlops,,~5M-5B samples,N,N,All are considered,~130,Loss,NS,NS,NS,NS,NS,NS,NS,NS,,
Unified scaling laws for routed language models,[2202.01169] Unified Scaling Laws for Routed Language Models,clark2022unified,\cite{clark2022unified},LM,N,Y,N,Y,"$\log L(N, E) \triangleq+a \log N+b \log E+c \log N \log E+d$",Performance Prediction,4,3,56,,15M-1.3B,130B tokens,Y,Y,Non-embedding,~26*56,Loss,Log,NS,NS,L-BFGS-B,L2 Loss,Y,Fixed,NS,,
,,,,,37,19,8,14,,,,,,,,,40,39,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,11 nos,,15 NSes,,,,22 - NS,,18 - no details,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,5 - NA,,,,,,