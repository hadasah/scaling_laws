# -*- coding: utf-8 -*-
"""fit_epoch_ai_paper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yyoWrNEs8380PcSxDRnew3DKQpCRsMXw
"""

# Imports
import autograd.numpy as np
from autograd import grad
import json
import matplotlib
import pandas as pd
import os
import re
import warnings

import matplotlib.pyplot as plt
# I believe autograd.numpy gets precendence
import numpy as np
import scipy.interpolate
from copy import copy
from functools import partial
from itertools import product
from scipy.optimize import curve_fit, minimize, least_squares, OptimizeWarning

# reduce number of warnings
np.seterr(over='ignore')
np.seterr(invalid='ignore')
warnings.filterwarnings("ignore", category=OptimizeWarning)

# set directory for data and plot saving
from IPython import get_ipython
ip = get_ipython()
if 'IPKernelApp' in ip.config:
    LOCAL_DIR = os.getcwd()
else:
    LOCAL_DIR = os.path.dirname(os.path.abspath(__file__))
if LOCAL_DIR == '/content':
  from google.colab import drive
  drive.mount('/content/drive')
  LOCAL_DIR = "/content/drive/MyDrive/scaling_law_survey/"
print(LOCAL_DIR)
# verify contents of folder
print(os.listdir(f"{LOCAL_DIR}/data"))



# values reported by Hoffman, et al and Kaplan, et al
# A, B, e, alpha, beta
KAPLAN_PARAMS_LOG_ORIGINAL = np.array([np.log(6.4e13), np.log(1.8e13), 0, 0.076, 0.103])
KAPLAN_PARAMS_LOG = np.array([np.log(6.4e13), np.log(1.8e13), 0, 0.076/0.103, 0.103])
#found in latex source code of hoffman
CHINCHILLA_PARAMS_LOG = np.array([6.0073404, 6.0179186, 0.5267228, 0.33917084, 0.2849083])
BESIROGLU_PARAMS_LOG = np.array([np.log(482.01), np.log(2085.43), np.log(1.8172), 0.3478, 0.3658])

"""## Data"""

# from Porian, et al https://github.com/formll/resolving-scaling-law-discrepancies
# with our modifications, for clarity and added functionality

COLORS = [c for c in matplotlib.colors.TABLEAU_COLORS]
COLORS.append('indigo')
COLORS.append('tan')
COLORS.append('k')

# dataset, hparams, warmup, decay, param_count, val = config

ISOFLOP_ARGS = {
    ('kaplan', 'train'): dict(loss_key='train/loss_smoothed', flop_per_token_key='flops_per_token_no_att_no_embed', n_key='params_no_embed'),
    ('standard', 'val'):  dict(loss_key='val/loss', flop_per_token_key='flops_per_token', n_key='params'),
    ('standard', 'train'): dict(loss_key='train/loss_smoothed', flop_per_token_key='flops_per_token', n_key='params'),
    ('attention', 'train'): dict(loss_key='train/loss_smoothed', flop_per_token_key='flops_per_token_att', n_key='eff_params_att'),
}

NAME_TO_CONFIG_DICT = {
    'rsld': ('rw', 'base', 'short', 'chinchilla', 'standard', 'train'), #: 'Cosine decay', # original Chinchilla?
    'misfitting': ('c4', 'misfitting', 'misfitting', 'cosine', 'standard', 'train'), # ours
}

def get_color(i, clist=COLORS):
    return clist[i % len(clist)]

DEFAULT_FLOP_VALUES = 1e16 * (2**np.linspace(0, 11, 12))

RW_SEED_CONFIG = dict(noise_low=0.002, noise_high=0.05, l_threshold_high=7, l_threshold_low=3)

def maybe_get_item(x, i):
    try:
        return x[i]
    except:
        return x

def fetch_flop(df, flop, loss_key='train/loss_smoothed', warmup_remove_factor=1e-12, n_key='params',
               seq_len=2048, bs_key='bs', lr_key='lr', keep_bs_lr_keys=False,
               flop_per_token_key='flops_per_token', flop_tolerance=0.1):
    out = []
    for _, row in df.iterrows():
        if len(row[loss_key]) == 0:
            continue
        # mask out step, val pairs that are too early in training (e.g. early during warmup)
        loss_vals = row[loss_key].dropna().groupby(level=0).mean().sort_index()
        step_vals = loss_vals.index
        if seq_len:
            mask = step_vals >= ((warmup_remove_factor * row.warmup_tokens) / row.bs / seq_len)
        else:
            mask = step_vals >= ((warmup_remove_factor * row.warmup_tokens) / row.bs / row.seq_len)
        loss_vals = loss_vals[mask]
        loss_vals.index = loss_vals.index.astype(float) * seq_len * row[bs_key] * row[flop_per_token_key]
        flop_vals = loss_vals.index

        if len(loss_vals) == 0:
            continue
        # find closest actual flop count to target
        flop_ind = loss_vals.index.searchsorted(flop)
        # maybe shift index down to actual closest
        if flop_ind > 0:
            flop_ind += -1 + np.abs(np.log(flop_vals[flop_ind-1:flop_ind+1]/flop)).argmin()
        # if actual flops too far from target, reject
        rel_err = np.exp(np.abs(np.log(flop_vals[flop_ind]/flop))) - 1
        if rel_err > flop_tolerance:
            continue

        if len(flop_vals) > 1:
            # if there are multipe value flop values, use up to 10 to interpolate and estimate (flop, loss)
            flop_slice = flop_vals[max(0,flop_ind-5):flop_ind+5]
            loss_slide = loss_vals.iloc[max(0,flop_ind-5):flop_ind+5]
            loss_interp = np.exp(np.interp(np.log(flop), np.log(flop_slice), np.log(loss_slide)))
            out.append(dict(n=row[n_key], t=flop / row[flop_per_token_key], loss=loss_interp))
        else:
            # if there's only valid flop value to include, use raw (flop, loss)
            out.append(dict(n=row[n_key], t=loss_vals.index[flop_ind] / row[flop_per_token_key], loss=loss_vals.iloc[flop_ind]))
        if keep_bs_lr_keys: # if keeping bs, lr, interp_flop will optimize over eta, bs
            out[-1].update({k: row[k] for k in [bs_key, lr_key]})

    return pd.DataFrame(out) # could have len > 1 if multiple models (diff N, D) have a datapoint with C ~= flop


def fit_loss_with_saturation(df, weighted=False, fit_min_flop=1e16, fit_max_flop=2e19):
    def model_func(F, a, e, alpha):
        return np.logaddexp(a - alpha * np.log(F), e)

    # Define the Huber loss function
    def custom_huber_loss(y_true, y_pred, delta=1e-3):
        # Calculate the difference
        diff = y_true - y_pred
        # Calculate the condition for Huber loss
        cond = np.abs(diff) <= delta
        # Apply Huber loss formula
        loss = np.where(cond, 0.5 * diff**2, delta * (np.abs(diff) - 0.5 * delta))
        return np.sum(loss)

    def isoflop_huber_loss_objective(params, F, losses):
        a, e, alpha = params
        predictions = np.logaddexp(a - alpha * np.log(F), e)
        return custom_huber_loss(np.log(losses), predictions, delta=1e-3)

    df = df.dropna().query(f'flops > {fit_min_flop} and flops < {fit_max_flop}')
    loss = df['loss'].values
    flops = df['flops'].values

    alpha_vals = np.arange(0, 0.4, 0.1)
    e_vals = np.arange(-1, 1.5, 0.5)
    a_vals = np.arange(0, 30, 5)
    best_loss = np.inf
    best_params = None
    results_dict = {}
    for alpha, e, a in list(product(alpha_vals, e_vals, a_vals)):
        init_params = [a, e, alpha]
        try:
            if weighted:
                popt, _ = curve_fit(model_func, flops, np.log(loss), p0=init_params, sigma=1 / df['loss_star_std'].values ** 2, method='trf', ftol=1e-6, xtol=1e-6, max_nfev=100)
            else:
                popt, _ = curve_fit(model_func, flops, np.log(loss), p0=init_params, method='trf', ftol=1e-6, xtol=1e-6, max_nfev=100)
            result_loss = isoflop_huber_loss_objective(popt, flops, loss)
            results_dict[tuple(init_params)] = {'params': popt, 'loss': result_loss}
            if result_loss < best_loss:
                best_loss = result_loss
                best_params = popt
        except RuntimeError:
            continue

    if best_params is not None:
        A = np.exp(best_params[0])
        E = np.exp(best_params[1])
        alpha = best_params[2]
        return {'A': A, 'E': E, 'alpha': alpha}
    else:
        return None


def power_law_fit(df, x, y, weighted=False):
    """Fit a power law for each y separately, and compile all into a dict
    If y is not loss, use linear regression
    If y is loss, use scipy.optimize.curve_fit
    """

    from sklearn.linear_model import LinearRegression
    if isinstance(y, (list, tuple)):
        out = {}
        for yy in y:
            if 'loss' not in yy:
                out.update(power_law_fit(df, x, yy, weighted=weighted))
            else:
                df = df.copy()
                out.update(fit_loss_with_saturation(df, weighted=weighted))
        return out
    else:
        X_data = np.log(df.dropna()[x].values).reshape(-1, 1)
        y_data = np.log(df.dropna()[y].values)
        std_key = f'{y}_star_std'
        if weighted and std_key in df.columns:
            y_data_std = df.dropna()[std_key].values
            w = 1 / y_data_std ** 2
        else:
            w = None

        clf = LinearRegression().fit(X_data, y_data, sample_weight=w)
        return {f'{y}_exponent': clf.coef_.item(),
                f'{y}_coef': np.exp(clf.intercept_),
                f'{y}_r2': clf.score(X_data, y_data)}


def fit_compute_optimal_power_laws(optimal_pairs, bootstrap_data, bootstrap_num=None, bootstrap_num_loss=200, fit_loss=True):
    keys_to_fit = ['n', 't', 'multiplier']
    if fit_loss:
        keys_to_fit.append('loss')
    out = {'basic': power_law_fit(optimal_pairs.reset_index(), 'flops', keys_to_fit),
           'weighted': power_law_fit(optimal_pairs.reset_index(), 'flops', keys_to_fit, weighted=True)}
    bootstrap_samples = bootstrap_data.dropna().set_index('flops')[
        ['n_stars', 't_stars', 'multiplier_stars', 'loss_stars', 'n_star_std', 't_star_std', 'loss_star_std']].rename(
        columns=lambda x: x.replace('_stars', ''))
    if bootstrap_num is None:
        bootstrap_num = bootstrap_samples[['n', 't', 'multiplier']].applymap(len).min().min()

    for name, is_weighted in dict(bootstrap=False, bootstrap_weighted=True).items():
        bs_smaples_arr = [
            power_law_fit(bootstrap_samples.map(lambda x: maybe_get_item(x, i)).reset_index(),
            'flops', ['loss'], weighted=is_weighted)
            for i in range(bootstrap_num_loss)
            ] if fit_loss else []
        bs_smaples_arr.extend([power_law_fit(
            bootstrap_samples.map(lambda x: maybe_get_item(x, i)).reset_index(),
            'flops', ['n', 't', 'multiplier'], weighted=is_weighted)
            for i in range(bootstrap_num)])
        bs_samples_dict = {k:[d.get(k) for d in bs_smaples_arr] for k in bs_smaples_arr[0].keys()}
        out[name] = bs_samples_dict
    bootstrap_medians = bootstrap_samples.map(np.median)
    out.update({
        'bs_median': power_law_fit(bootstrap_medians.reset_index(), 'flops', keys_to_fit),
        'bs_median_weighted': power_law_fit(bootstrap_medians.reset_index(), 'flops', keys_to_fit, weighted=True)})
    return out


def get_noise_for_loss(loss, bootstrap_iters, noise_low=0.005, noise_high=0.1, l_threshold_high=6, l_threshold_low=3):
    basic_noise = np.random.normal(0, 1, (bootstrap_iters, len(loss) // bootstrap_iters))
    noise_adjusted_losses = np.zeros((bootstrap_iters, len(loss) // bootstrap_iters))

    for i in range(len(loss) // bootstrap_iters):
        if np.log(loss[i]) >= l_threshold_high:
            log_noise = np.log(noise_high)
        elif np.log(loss[i]) <= l_threshold_low:
            log_noise = np.log(noise_low)
        else:
            log_noise = np.interp(np.log(loss[i]), [np.log(l_threshold_low), np.log(l_threshold_high)], [np.log(noise_low), np.log(noise_high)])
        noise_factor = np.exp(log_noise)
        noise_adjusted_losses[:, i] = loss[i] + noise_factor * basic_noise[:, i]

    return noise_adjusted_losses.flatten()


def vectorized_interp_with_seed_noise(df, n_interp_, bootstrap_iters, seed_noise=RW_SEED_CONFIG,
                                      min_std_factor=0.33, tok_or_n='n'):
    # noise losses
    interp_num = len(n_interp_)
    stacked_df = pd.concat([df] * bootstrap_iters).reset_index(drop=True)
    stacked_df['loss'] = get_noise_for_loss(stacked_df.loss, bootstrap_iters=bootstrap_iters, **seed_noise)

    batch_ids = np.repeat(np.arange(bootstrap_iters), len(df))
    stacked_df['batch_id'] = batch_ids
    stacked_df.sort_values(by=['batch_id', tok_or_n], inplace=True)

    def batch_interp(batch):
        interp = scipy.interpolate.Akima1DInterpolator(np.log(batch[tok_or_n]), np.log(batch['loss']))
        return np.exp(interp(np.log(n_interp_)))

    # find interpolated values based on each batch of noised losses
    interpolated_values = stacked_df.groupby('batch_id').apply(batch_interp)

    # Find the index of the minimum interpolated loss value per batch
    min_indices = interpolated_values.apply(np.argmin)
    results = [n_interp_[idx] if idx != 0 and idx != interp_num - 1 else None for idx in min_indices] # ignore if largest or smallest is min
    valid_results_loss = [interpolated_values[i][idx] for i, idx in enumerate(min_indices) if idx != 0 and idx != interp_num - 1]
    # Filter None values and calculate statistics
    valid_results = [result for result in results if result is not None]
    if len(valid_results) < bootstrap_iters // 2: #too few valid loss values
        return None, 0, None, None, None
    else:
        n_star_std_ = np.std(np.log(valid_results))
        min_std = min_std_factor * np.log(n_interp_[1] / n_interp_[0])  # this assumes a roughly uniform grid
        n_star_std_ = max(n_star_std_, min_std) * (bootstrap_iters / len(valid_results))
        loss_star_std_ = np.std(np.log(valid_results_loss))
        min_std_loss = min_std_factor * min([np.log(df.loss.iloc[i+1] / df.loss.iloc[i]) for i in range(len(df) - 1)])
        loss_star_std_ = max(loss_star_std_, min_std_loss) * (bootstrap_iters / len(valid_results_loss))
        return n_star_std_, None, valid_results, valid_results_loss, loss_star_std_


def interpolation(df_, interp_num, bootstrap_iters, min_std_factor, interp_num_multiplier, std_method, col):
    star_std_, noised_stars_, noised_loss, loss_star_std = None, None, None, None
    if len(df_) > 1:
        interp_ = np.geomspace(df_[col].min(), df_[col].max(), interp_num)
        df_ = df_.sort_values(col)
        interpolator = scipy.interpolate.Akima1DInterpolator(np.log(df_[col]), np.log(df_.loss))
        loss_interp_ = np.exp(interpolator(np.log(interp_))) #get interp_num values for interpolated losses
        star_ind_ = loss_interp_.argmin() # across entire model size range, lowest interpolated loss index

        if std_method == 'add_seed_noise' :
            star_std_, _, noised_stars_, noised_loss, loss_star_std = vectorized_interp_with_seed_noise(
                df_, interp_, bootstrap_iters, min_std_factor * interp_num_multiplier, tok_or_n=col)
    else:
        interp_ = df_[col].values
        loss_interp_ = df_.loss.values
        star_ind_ = df_[col].values[0]
    return star_ind_, star_std_, noised_stars_, interp_, loss_interp_, noised_loss, loss_star_std


# For hparams sweep - basic interpolation
def minimize_with_interp(df, x_key='lr', y_key='loss', interp_num=100, groupby_action='min', interpolator=scipy.interpolate.Akima1DInterpolator):
    df = df.copy().reset_index()
    if groupby_action == 'min':
        df = df.loc[df.groupby([x_key])[y_key].idxmin()]
        # take the best value of lr, etc., if there are multiple ones - could potentially do better by interpolating here too
        df = df.set_index(x_key)
    elif groupby_action == 'mean':
        df = df.groupby(x_key).mean()
    else:
        raise ValueError(f'Unknown groupby_action {groupby_action}')
    df = df.sort_index()

    if len(df) < 2:
        return pd.DataFrame({x_key: [np.nan], y_key: [np.nan], 'on_edge': True})

    xlog, ylog = np.log(df.index.values), np.log(df[y_key].values)
    interp = interpolator(xlog, ylog)

    xlog_i = np.linspace(xlog.min(), xlog.max(), interp_num)
    ylog_i = interp(xlog_i)

    x_i, y_i = np.exp(xlog_i), np.exp(ylog_i)

    argmin_xlog_i = xlog_i[y_i.argmin()]
    argmin_x_i = x_i[y_i.argmin()]
    on_edge = argmin_x_i < np.exp(xlog[1]) or argmin_x_i > np.exp(xlog[-2])
    out = {x_key: [argmin_x_i], 'on_edge': int(on_edge)+1}

    for key in df.columns:
        if key == 'index':
            continue
        out[key] = [np.exp(interpolator(xlog, np.log(df[key].values))(argmin_xlog_i))]

    if x_key + '_star' in df.columns:
        interp_l = interpolator(xlog, np.log(df[y_key + '_star'].values))
        out['loss_star'] = [np.exp(interp_l(np.log(df[x_key + '_star'].values[0])))]

    return pd.DataFrame(out).set_index(x_key)


def interp_flop(df, loss_key, flop_vals=DEFAULT_FLOP_VALUES, groupby_action='min',
                warmup_remove_factor=1e-12,
                interp_num_multiplier=25,
                n_key='params', n_star_std_method='add_seed_noise', t_star_std_method='add_seed_noise',
                bootstrap_iters=1000,
                min_std_factor=0.33,
                flop_tolerance=0.1,
                flop_per_token_key='flops_per_token',
                bs_median_as_obs=True,
                keep_bs_lr_keys=False,
                bs_key='bs',
                lr_key='lr',
                min_df_size=3,
                ):
    out = [] # will contain all interpolated loss data
    optimal_pairs = [] # will contain only the optimal loss data for each flop count (C)
    max_loss, min_loss = 0, 1e12

    for c in flop_vals:
        # get estimated/interpolated (flop, loss) data
        df_ = fetch_flop(df, c, loss_key=loss_key,
                         warmup_remove_factor=warmup_remove_factor, n_key=n_key,
                         flop_per_token_key=flop_per_token_key,
                         flop_tolerance=flop_tolerance, keep_bs_lr_keys=keep_bs_lr_keys)

        # only one or two models with ~= this flop value found, not enough to interpolate
        if len(df_) < min_df_size:
            out.append(dict(n_interp=None, loss_interp=None, t_interp=None,
                            loss_interp_tok=None, opt_ind=None, opt_tok_ind=None, flops=c))
            continue
        if bs_key in df_.columns and lr_key in df_.columns:
            df_sweep_opt_eta = df_.groupby(['n', bs_key]).apply(minimize_with_interp).drop([bs_key, 'n'], axis=1).reset_index()
            df_sweep_opt_eta_and_bs = df_sweep_opt_eta.groupby(['n']).apply(lambda x: minimize_with_interp(x, x_key=bs_key)).drop('n', axis=1).reset_index()
            df_ = df_sweep_opt_eta_and_bs[['n']]
        elif groupby_action == 'min': # if multiple runs w/ models of same size, take min loss
            df_ = df_.loc[df_.groupby(['n']).loss.idxmin()]
        elif groupby_action == 'mean': # if multiple runs w/ models of same size, take mean loss
            df_ = df_.groupby('n').mean()
        else:
            raise ValueError(f'Unknown groupby_action {groupby_action}')
        df_ = df_.reset_index()

        interp_num = (len(df_) - 1) * interp_num_multiplier

        max_loss, min_loss = max(max_loss, df_.loss.max()), min(min_loss, df_.loss.min())

        # interp across n for values of C
        n_star_ind_, n_star_std_, noised_n_stars_, n_interp_, loss_interp_, noised_loss, loss_star_std = interpolation(
            df_, interp_num, bootstrap_iters, min_std_factor, interp_num_multiplier, n_star_std_method, 'n')

        # interp across t for values of C
        t_star_ind_, t_star_std_, noised_t_stars_, t_interp_, loss_interp_tok_, _noised_loss, _ = interpolation(
            df_, interp_num, bootstrap_iters, min_std_factor, interp_num_multiplier, t_star_std_method, 't')

        if n_star_ind_ != 0 and n_star_ind_ != interp_num -1 and noised_n_stars_ is not None:
            optimal_pairs.append(
                dict(flops=c, n=n_interp_[n_star_ind_], t=t_interp_[t_star_ind_], multiplier=c / 6 / (n_interp_[n_star_ind_]**2),
                    loss=loss_interp_.min(), loss_t=loss_interp_tok_.min(),
                    n_vals=df_.n.values, t_vals=df_.t.values, loss_vals=df_.loss
                    )
            )
        else:
            optimal_pairs.append(
                dict(flops=c, n=None, t=None, loss=None, loss_t=None,
                        n_vals=df_.n.values, t_vals=df_.t.values, loss_vals=df_.loss
                    )
            )
        out.append(
            dict(n_interp=n_interp_, loss_interp=loss_interp_,
                 t_interp=t_interp_, loss_interp_tok=loss_interp_tok_,
                 opt_ind=n_star_ind_, opt_tok_ind=t_star_ind_, flops=c,
                 orig_n=df_.n, orig_t=df_.t, orig_loss=df_.loss)
            )
        if n_star_std_method == 'add_seed_noise':
            out[-1]['n_star_std'] = n_star_std_
            out[-1]['n_stars'] = noised_n_stars_
            out[-1]['multiplier_stars'] = (c / (6 * np.array(noised_n_stars_)**2)) if noised_n_stars_ is not None else None
            optimal_pairs[-1]['n_star_std'] = n_star_std_

            out[-1]['multiplier_star_std'] = 2 * n_star_std_ if n_star_std_ is not None else None
            optimal_pairs[-1]['multiplier_star_std'] = 2 * n_star_std_ if n_star_std_ is not None else None

            out[-1]['t_star_std'] = t_star_std_
            out[-1]['t_stars'] = noised_t_stars_
            optimal_pairs[-1]['t_star_std'] = t_star_std_

            out[-1]['loss_stars'] = noised_loss
            out[-1]['loss_star_std'] = loss_star_std
            optimal_pairs[-1]['loss_star_std'] = loss_star_std

    out_df = pd.DataFrame(out)
    optimal_pairs_df = pd.DataFrame(optimal_pairs)

    if bs_median_as_obs:
        for ind, row in optimal_pairs_df.iterrows():
            if row['n'] is None or np.isnan(row['n']):
                continue
            flop = row['flops']
            data_row = out_df.set_index('flops').loc[flop]
            for key in ['n', 't', 'multiplier', 'loss']:
                optimal_pairs_df.at[ind, key] = np.median(data_row[key + '_stars']) if data_row[key + '_stars'] is not None else row[key]

    return out_df, optimal_pairs_df, max_loss, min_loss


def precise_flops_per_token_chinchilla(width, depth):
    seq_len = 2048
    vocab_size = 50432
    num_heads = 4
    width = width.astype(float)
    depth = depth.astype(float)

    embeddings = 2 * seq_len * width

    attention = 2 * 3 * seq_len * (width ** 2)
    kq_logits = 2 * seq_len * seq_len * width
    softmax = 3 * num_heads * seq_len * seq_len
    softmax_q_red = 2 * seq_len * seq_len * width
    final_linear = 2 * seq_len * (width ** 2)
    attention += kq_logits + softmax + softmax_q_red + final_linear

    ffw_size = 4 * width # check this, in the paper it is 4 * width
    dense_block = 4 * seq_len * width * ffw_size
    final_logits = 2 * seq_len * width * vocab_size
    forward_pass = embeddings + depth * attention + depth * dense_block + final_logits
    backward_pass = 2 * forward_pass
    return (forward_pass + backward_pass) / seq_len

def precise_param_count_open_lm(width, depth, vocab_size=50432):
    d_ff = 256 * (((2 * 4 * width / 3).astype(int) + 256 - 1) // 256)
    return (4 * width + 3 * d_ff) * width * depth + vocab_size * width

def apply_smoothing_filter(df, filter_func, compensate_for_logging_delay=True, key='train/loss', **filter_args):
    out = []
    for _, row in df.iterrows():
        if len(row[key]) == 0:
            out.append(None)
            continue
        filtered = filter_func(row[key].dropna(), **filter_args)
        if compensate_for_logging_delay:
            filtered.index = filtered.index - np.diff(filtered.index, prepend=0)/2
        out.append(filtered)
    return out

def proportional_sliding_window_filter(x, p=0.05):
    # assert that the index of x has constant increments?
    x_cumsum = x.cumsum().values
    x_cumsum_pad = np.concatenate([[0], x_cumsum])
    inds = np.arange(len(x))
    inds_up = np.minimum(inds + np.floor(p * inds).astype(int), len(x)-1)
    inds_down = np.maximum(0, inds - np.floor(p * inds).astype(int))
    inds_new = (inds_up + inds_down)/2
    index_new = np.interp(inds_new, inds, x.index)
    try:
        x_series = pd.Series((x_cumsum[inds_up] - x_cumsum_pad[inds_down]) / (inds_up - inds_down+1),
                     index=index_new, name=x.name + '_smoothed')
    except:
        x_series = pd.Series((x_cumsum[inds_up] - x_cumsum_pad[inds_down]) / (inds_up - inds_down+1),
                     index=index_new, name="" + '_smoothed')
    return x_series


### New code

def add_columns_to_rsld_df(df):
    """ Calculate C, N, etc., for Porian, e.t. al data
    """
    df = df.copy()

    # Counting parameters
    df['params_active'] = (12 * (df.width**2) * df.depth + df.vocab_size * df.width).astype(float)
    df['params_active_precise'] = precise_param_count_open_lm(df.width, df.depth)
    df['params_no_embed'] = precise_param_count_open_lm(df.width, df.depth, vocab_size=0)
    df['params_all'] = 12 * (df.width**2) * df.depth + (df.seq_len + 2 * df.vocab_size) * df.width

    # Counting FLOPs
    df['flops_per_token_att_no_embed'] = 6 * df['params_no_embed'] + 6 * df.seq_len * df.width * df.depth
    df['flops_per_token_att'] = 6 * df['params_active_precise']  + 6 * df.seq_len * df.width * df.depth
    df['flops_per_token_cc'] = precise_flops_per_token_chinchilla(df['width'], df['depth'])
    df['flops_per_token_no_att'] = 6 * df['params_active_precise']
    df['flops_per_token_no_att_no_embed'] = 6 * df['params_no_embed']
    df['flops_per_token'] = df['flops_per_token_no_att']

    df['params'] = df['flops_per_token'] / 6
    df['eff_params_att'] = df['flops_per_token_att'] / 6


    df['train/loss_smoothed'] = apply_smoothing_filter(df, proportional_sliding_window_filter, compensate_for_logging_delay=True, key='train/loss')
    for k in df:
        if k.startswith('train/') and k.endswith('_loss'):
            df[k + '_smoothed'] = apply_smoothing_filter(df, proportional_sliding_window_filter, compensate_for_logging_delay=False, key=k)
    return df

def get_local_data(
    csv_file,
    data_selections=None,
    default_data_selections=None,
    n_field_name="N",
    d_field_name="D",
    loss_field_name='loss',
    c_field_name="C",
    col_names=None
):
    """ Load scaling law data in from csv file, then apply filters

    Args:
      csv_file (str): path to csv file containing scaling law data
      data_selections (Optional[List[str]]): list of data filtering rules to apply to the data
      n_field_name (str): name of the column containing the model size
      d_field_name (str): name of the column containing the data size
      loss_field_name (str): name of the column containing the loss values
      c_field_name (str): name of the column containing the overall compute cost in FLOPs
      col_names (Optional[List[str]]): list of column names to read from the csv file

    Returns: Pandas DataFrame containing the filtered data
    """
    mins_only = []
    df = pd.read_csv(csv_file, usecols=col_names,)
    df.dropna(subset=[loss_field_name], inplace=True)

    if 'lr' in df.columns:
        df = df.loc[(df['lr'] >= 0)]
    if d_field_name not in df.columns:
        df[d_field_name] = df[c_field_name] / (df[n_field_name] * 6)
    return df


def maybe_filter_data(
    df,
    data_selections=None,
    default_data_selections=None,
    n_field_name="N",
    d_field_name="D",
    loss_field_name='loss',
    c_field_name="C",
    lr_field_name="peak_lr",
):
    """ Filter dataframes as specified
    """

    if data_selections or default_data_selections:
        if default_data_selections:
            new_data_selections = copy(default_data_selections)
            if data_selections:
                new_data_selections.update(data_selections)
            data_selections = new_data_selections
        for k, (op, v) in data_selections.items():
            ## LR selection
            if k == "lr":
                if op is None:
                    continue
                if op == "opt":
                    n_vals = df[n_field_name].unique()
                    d_vals = sorted(df[d_field_name].unique())
                    rows_to_keep = []
                    for n in n_vals:
                        for d in d_vals:
                            cd_df = df[(df[n_field_name] == n) & (df[d_field_name] == d)]
                            if cd_df.empty:
                                continue
                            min_index = cd_df[loss_field_name].idxmin()
                            rows_to_keep.append(cd_df.loc[min_index])
                    df = pd.DataFrame(rows_to_keep)
                else:
                    lr = float(v)
                    if op in ["=", "=="]:
                        df = df[df[lr_field_name] == lr]
                    else:
                        raise RuntimeError(f"This data selection filter {k} {op} {v} does not exist")
            ## Model size selection
            elif k == "N":
                if op is None:
                    continue
                max_size = re.match("(\d+[b|m|k])", v).group(1)
                if max_size[-1] == "b":
                    max_size = float(max_size[:-1]) * 1e9
                elif max_size[-1] == "m":
                    max_size = float(max_size[:-1]) * 1e6
                elif max_size[-1] == "k":
                    max_size = float(max_size[:-1]) * 1e3
                max_size *= 1.25 ## buffer
                if op == "<":
                    df = df[df[n_field_name] < max_size]
                elif op == ">":
                    df = df[df[n_field_name] > max_size]
                else:
                    raise RuntimeError(f"This data selection filter {k} {op} {v} does not exist")
            ## D/N Ratio selection
            elif k == "ratio":
                if op is None:
                    continue
                floor = float(v)
                if op == "<":
                  df = df[df[d_field_name]/df[n_field_name] < floor ]
                elif op == ">":
                  df = df[df[d_field_name]/df[n_field_name] > floor ]
                else:
                    raise RuntimeError(f"This data selection filter {k} {op} {v} does not exist")
            ## Checkpoint selection
            elif k == "ckpt":
                if op is None:
                    continue
                ckpt_r = float(v)
                if op == "<":
                    df = df[df["portion_steps_elapsed"] < ckpt_r]
                elif op == ">":
                    df = df[df["portion_steps_elapsed"] > ckpt_r]
                else:
                    raise RuntimeError(f"This data selection filter {k} {op} {v} does not exist")
            else:
                raise RuntimeError(f"This data selection filter {k} {op} {v} does not exist")

    return df

def adapt_df_for_isoflop(
        df, data_name='misfitting',
        n_key="N", d_key="D", c_key="C", total_steps_key="total_steps",
        step_key='current_steps', flops_per_step_key=None,
        bs_key="bs", lr_key="peak_lr",
        bs_value=None,
        seq_len_key="seq_len",
        seq_len_value=None,
        warmup_tokens_key="warmup_tokens",
        warmup_steps_key="warmup_steps",
        original_loss_key="loss", new_loss_key="train/loss",
        do_adapt=True,
    ):
    """ Convert mid-training checkpoint data from our format to that used by Porian, e.t. al
    By collapsing all points for a single model training trajectory into one row
    """
    if do_adapt:
        new_df = []
        if step_key is None:
            pass

        if seq_len_key not in df.columns:
            if seq_len_value:
                df[seq_len_key] = seq_len_value

        if bs_key not in df.columns:
            if bs_value:
                df[bs_key] = bs_value
            else:
                df[bs_key] = df['train_tokens'] / (df['current_steps'].astype(int) * df[seq_len_key].astype(int))

        if warmup_tokens_key not in df.columns:
            if warmup_steps_key and warmup_steps_key in df.columns:
                df[warmup_tokens_key] = df[bs_key] * df[warmup_steps_key]
            else:
                df[warmup_tokens_key] = df[bs_key] * 50

        for n in df[n_key].unique():
            for s in df[total_steps_key].unique():
                for bs in df[bs_key].unique():
                    for lr in df[lr_key].unique():
                        model_df = df[
                            (df[n_key] == n) & (df[total_steps_key] == s) & (df[bs_key] == bs) & (df[lr_key] == lr)
                        ]
                        if len(model_df) == 0:
                            continue
                        loss_series = pd.Series({step: model_df.loc[model_df[step_key] == step].iloc[0][original_loss_key] for step in sorted(model_df[step_key].unique())})
                        loss_series.index.name = "step"
                        row = model_df.iloc[0].to_dict()
                        row['train/loss'] = loss_series
                        new_df.append(row)
        df = pd.DataFrame(new_df)

    if 'misfitting' in data_name:
        df['params_no_embed'] = df['N_no_emb'].astype(float)
        df['params_active'] = df['N'].astype(float)
        df['params_all'] = df['N'].astype(float) # + seq_len * num_layers

        df['flops_per_token_att_no_embed'] = df["C_no_emb"].astype(float) / df["D"]
        df['flops_per_token_att'] = df["C"].astype(float) / df["D"]
        df['flops_per_token_no_att'] = df["C_6ND"].astype(float) / df["D"]
        df['flops_per_token_no_att_no_embed'] = df["C_6ND_no_emb"].astype(float) / df["D"]
        df['flops_per_token'] = df['flops_per_token_no_att'].astype(float)

        df['params'] = df['flops_per_token'].astype(float) / 6
        df['eff_params_att'] = df['flops_per_token_att'].astype(float) / 6

        df['train/loss_smoothed'] = apply_smoothing_filter(df, proportional_sliding_window_filter, compensate_for_logging_delay=True, key=new_loss_key)

    elif 'rsld' in data_name:

        df['params_active'] = (12 * (df.width**2) * df.depth + df.vocab_size * df.width).astype(float)
        df['params_active_precise'] = precise_param_count_open_lm(df.width, df.depth)
        df['params_no_embed'] = precise_param_count_open_lm(df.width, df.depth, vocab_size=0)
        df['params_all'] = 12 * (df.width**2) * df.depth + (df.seq_len + 2 * df.vocab_size) * df.width

        # Counting FLOPs
        df['flops_per_token_att_no_embed'] = 6 * df['params_no_embed'] + 6 * df.seq_len * df.width * df.depth
        df['flops_per_token_att'] = 6 * df['params_active_precise']  + 6 * df.seq_len * df.width * df.depth
        df['flops_per_token_cc'] = precise_flops_per_token_chinchilla(df['width'], df['depth'])
        df['flops_per_token_no_att'] = 6 * df['params_active_precise']
        df['flops_per_token_no_att_no_embed'] = 6 * df['params_no_embed']
        df['flops_per_token'] = df['flops_per_token_no_att']

        df['params'] = df['flops_per_token'] / 6
        df['eff_params_att'] = df['flops_per_token_att'] / 6

        df['train/loss_smoothed'] = apply_smoothing_filter(df, proportional_sliding_window_filter, compensate_for_logging_delay=True, key=new_loss_key)

    return df

def fit_isoflop_power_law(
      df,
      data_name='misfitting',
      do_adapt=True,
      flop_vals=DEFAULT_FLOP_VALUES,
      seed=42,
      keep_bs_lr_keys=False,
      seq_len_value=2048,
):
    if 'misfitting' in data_name:
        df = adapt_df_for_isoflop(df, data_name=data_name, seq_len_value=seq_len_value, do_adapt=do_adapt)
        config = NAME_TO_CONFIG_DICT['misfitting']
    else:
        df = adapt_df_for_isoflop(df, data_name=data_name, lr_key='lr', do_adapt=do_adapt)
        config = NAME_TO_CONFIG_DICT['rsld']
    isoflop_args = ISOFLOP_ARGS[config[-2:]]
    show_df = df
    np.random.seed(seed)

    if len(show_df) == 0:
        raise RuntimeError("chosen config has no data")

    data, optimal_pairs, _, _ = interp_flop(
        show_df,
        flop_vals=flop_vals, **isoflop_args,
        keep_bs_lr_keys=keep_bs_lr_keys, min_df_size=1,
    )

    fit_results = fit_compute_optimal_power_laws(optimal_pairs, data)
    return fit_results

def get_rsld_data(config_name, ckpt=False):
    """
    Get raw data from Porian, et al
    """
    df = pd.read_pickle(f'{LOCAL_DIR}/data/rsld/experiment_results.pickle.xz', compression='xz')
    df = add_columns_to_rsld_df(df)
    # select only the rows for a particular config
    config = NAME_TO_CONFIG_DICT[config_name]
    dataset, hparams, warmup, decay, param_count, val = config
    isoflop_args = ISOFLOP_ARGS[config[-2:]]
    df = df.query(f"dataset=='{dataset}' and hparams=='{hparams}' and warmup=='{warmup}' and decay=='{decay}'").copy()

    # get C, D, loss at last checkpoint
    flops_field_name = isoflop_args['flop_per_token_key']
    loss_key = isoflop_args['loss_key']
    Ds, current_steps, portion_steps_elapsed = [], [], []
    for ind, row in df.iterrows():
        df.loc[ind, 'total_steps'] = row[loss_key].index[-1]
        current_steps.append(row[loss_key].index.astype(float).to_list())
        portion_steps_elapsed.append((row[loss_key].index.astype(float)/row[loss_key].index[-1]).to_list())
        Ds.append(np.array((row[loss_key].index.astype(float) * row['seq_len'] * row['bs']).to_list()))
        row[loss_key].index = row[loss_key].index.astype(float) * row['seq_len'] * row['bs']
    df.loc[:, 'D'] = Ds
    df.loc[:, 'current_steps'] = current_steps
    df.loc[:, 'portion_steps_elapsed'] = portion_steps_elapsed
    if ckpt:
          df.loc[:,'N'] = df[isoflop_args['n_key']]
          df.loc[:, loss_key] = df[loss_key].values
          df = df.explode([loss_key, 'D', 'current_steps', 'portion_steps_elapsed'])
          df.loc[:, 'C'] = df["D"] * df[flops_field_name]
          df.loc[:, 'loss'] = df[loss_key]
          df = df.astype({'D': 'float64', 'C': 'float64', 'loss': 'float64', 'N': int})

    else:
        for ind, row in df.iterrows():
            df.loc[ind, f'last_{loss_key}_C'] = row[loss_key].index[-1] * row[flops_field_name]
            df.loc[ind, f'last_{loss_key}_D'] = row[loss_key].index[-1]
            df.loc[ind, f'last_{loss_key}'] = row[loss_key].iloc[-1]
            df.loc[ind, 'current_steps'] = row['current_steps'][-1]
            df.loc[ind, 'portion_steps_elapsed'] = row['portion_steps_elapsed'][-1]
        df.loc[:,'N'] = df[isoflop_args['n_key']]
        df.loc[:,'C'] = df[f'last_{loss_key}_C']
        df.loc[:,'D'] = df[f'last_{loss_key}_D']
        df.loc[:,'loss'] = df[f'last_{loss_key}']
        df = df.astype({'D': 'float64', 'C': 'float64', 'loss': 'float64', 'N': int})

    df['N_no_emb'] = precise_param_count_open_lm(df.width, df.depth, vocab_size=0)
    df['C_no_emb'] = df['N_no_emb'] * 6 * df['D']
    df['C_6ND'] = 6 * df['N'] * df['D']
    return df

def get_rsld_data_interp(config_name):
    """
    Get data from Porian, et al, with interpolated values, for, e.g., IsoFLOP analysis
    """
    df = pd.read_pickle(f'{LOCAL_DIR}/data/rsld/experiment_results.pickle.xz', compression='xz')
    df = add_columns_to_rsld_df(df)
    config = NAME_TO_CONFIG_DICT[config_name]
    dataset, hparams, warmup, decay, param_count, val = config
    show_df = df.query(f"dataset=='{dataset}' and hparams=='{hparams}' and warmup=='{warmup}' and decay=='{decay}'").copy()
    return show_df

def get_random_indices(
    indices,
    num_sets=4000,
    seed=42,
):
    """
    randomly select subset of models to include
    """
    np.random.seed(seed)
    random_indices = [np.random.choice(indices, size=len(indices), replace=True) for _ in range(num_sets)]
    return random_indices

def filter_data(N, D, losses, indices, nr_of_models_excluded=0, pct_of_models_excluded=0):
    """
    maybe remove highest loss models
    """
    sorted_losses = sorted(losses)
    if nr_of_models_excluded:
        indices = [i for i in range(len(N)) if losses[i] < sorted_losses[-nr_of_models_excluded]]
    elif pct_of_models_excluded:
        indices = [i for i in range(len(N)) if losses[i] < sorted_losses[int(-pct_of_models_excluded)]]
    return indices

def get_data(data_name, rsld_config_name="rsld", n_field_name = "N", d_field_name = "D", loss_field_name="loss", c_field_name="C", data_selections=None, default_data_selections=None):
    """
    load scaling law data from csv
    """
    if data_name == "misfitting_old":
        csv_file = f"{LOCAL_DIR}/data/data.csv"
        training_df = get_local_data(csv_file=csv_file, n_field_name=n_field_name, d_field_name=d_field_name, loss_field_name=loss_field_name, c_field_name=c_field_name,
                                     col_names=None, data_selections=data_selections, default_data_selections=default_data_selections)
    elif data_name == "misfitting_new_ckpt":
        csv_file = f"{LOCAL_DIR}/data/scaling_results.csv"
        training_df = get_local_data(csv_file=csv_file, n_field_name=n_field_name, d_field_name=d_field_name, loss_field_name=loss_field_name, c_field_name=c_field_name,
                                     col_names=None, data_selections=data_selections, default_data_selections=default_data_selections)
    elif data_name == "misfitting_new_final":
        csv_file = f"{LOCAL_DIR}/data/scaling_results_last_ckpt_only.csv"
        training_df = get_local_data(csv_file=csv_file, n_field_name=n_field_name, d_field_name=d_field_name, loss_field_name=loss_field_name, c_field_name=c_field_name,
                                     col_names=None, data_selections=data_selections, default_data_selections=default_data_selections)
    elif data_name == "epoch_ai":
        csv_file = f"{LOCAL_DIR}/data/epoch_ai.csv"
        training_df = get_local_data(csv_file=csv_file, col_names=None, )
    elif data_name == "rsld_final":
        training_df = get_rsld_data(config_name=rsld_config_name)
    elif data_name == "rsld_ckpt":
        training_df = get_rsld_data(config_name=rsld_config_name, ckpt=True)
    elif data_name == "rsld_isoflop":
        training_df = get_rsld_data_interp(config_name=rsld_config_name)
    else:
        raise RuntimeError(f"type of data {data_name} to use not recognized")
    training_df = maybe_filter_data(training_df, n_field_name=n_field_name, d_field_name=d_field_name, loss_field_name=loss_field_name, c_field_name=c_field_name,
                                     data_selections=data_selections, default_data_selections=default_data_selections)

    N = training_df[n_field_name].values if n_field_name in training_df.columns else None
    D = training_df[d_field_name].values if d_field_name in training_df.columns else None
    losses = training_df[loss_field_name].values if loss_field_name in training_df.columns else None
    indices = list(range(len(N))) if N is not None else None

    return N, D, losses, indices, training_df

"""## Power law fitting"""

# partially adapted from https://github.com/epoch-research/analyzing-chinchilla

import autograd.numpy as np
from autograd.scipy.stats import norm
from scipy.optimize import minimize
from scipy.special import erf


# Define the log-sum-exp function
def log_sum_exp_chinch(a, b, e, alpha, beta, N, D):
    return np.log(np.exp(a - alpha * np.log(N)) + np.exp(b - beta * np.log(D)) + np.exp(e))

# not a log_sum_exp, but keeping the name
def log_sum_exp_kaplan(a, b, e, alpha, beta, N, D):
    # return beta * np.log((N/np.exp(a))**(alpha) + D/np.exp(b))
    return beta * np.log(np.exp(alpha*(np.log(N) - a)) + np.exp(np.log(D) - b))

# Define the Huber loss function
def custom_huber_loss(y_true, y_pred, delta=1e-3):
    # Calculate the difference
    diff = y_true - y_pred
    # Calculate the condition for Huber loss
    cond = np.abs(diff) <= delta
    # Apply Huber loss formula
    loss = np.where(cond, 0.5 * diff**2, delta * (np.abs(diff) - 0.5 * delta))
    return np.sum(loss)

def huber_normalizing_factor(delta=1e-3):
    return np.sqrt(2*np.pi) * (1 - 2*norm.sf(delta)) + 2 * np.exp(-0.5*delta**2)/delta

def huber_logpdf(x, delta=1e-3, loc=0, scale=1):
    x = (x-loc)/scale

    cond = np.abs(x) <= delta
    loss = np.where(cond, 0.5 * x**2, delta * (np.abs(x) - 0.5 * delta))
    return -loss - np.log(huber_normalizing_factor(delta=delta)) - np.log(scale)

def huber_pdf(x, delta=1e-3, loc=0, scale=1):
    return np.exp(huber_logpdf(x, delta=delta, loc=loc, scale=scale))

# Define the objective function to be minimized
def objective(params, N, D, losses, form='chinchilla', tie_alpha_beta=False):
    log_sum_exp_fn = FORMS[form]
    a, b, e, alpha, beta, sigma = params
    if tie_alpha_beta:
        beta = alpha
    predictions = log_sum_exp_fn(a, b, e, alpha, beta, N, D)
    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))
    # return custom_huber_loss(np.log(losses), predictions, delta=1e-3)

def scale_objective(params, N, D, losses, form='chinchilla', tie_alpha_beta=False):
    log_sum_exp_fn = FORMS[form]
    a, b, e, alpha, beta, sigma = params
    if tie_alpha_beta:
        beta = alpha
    predictions = log_sum_exp_fn(a, b, e, alpha, beta, N, D)
    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))

def log_huber_loss_objective(params, N, D, losses, form='chinchilla', delta=1e-3, tie_alpha_beta=False):
    log_sum_exp_fn = FORMS[form]
    a, b, e, alpha, beta = params
    if tie_alpha_beta:
        beta = alpha
    predictions = log_sum_exp_fn(a, b, e, alpha, beta, N, D)
    return custom_huber_loss(np.log(losses), predictions, delta=delta)

def log_mae_objective(params, N, D, losses, form='chinchilla', delta=1e-3, tie_alpha_beta=False):
    log_sum_exp_fn = FORMS[form]
    a, b, e, alpha, beta = params
    if tie_alpha_beta:
        beta = alpha
    predictions = log_sum_exp_fn(a, b, e, alpha, beta, N, D)
    return np.sum(np.abs(np.log(losses) - predictions))

def log_mse_objective(params, N, D, losses, form='chinchilla', delta=1e-3, tie_alpha_beta=False):
    log_sum_exp_fn = FORMS[form]
    a, b, e, alpha, beta = params
    if tie_alpha_beta:
        beta = alpha
    predictions = log_sum_exp_fn(a, b, e, alpha, beta, N, D)
    return np.sum((np.log(losses) - predictions)**2)

def huber_loss_objective(params, N, D, losses, form='chinchilla', delta=1e-3, tie_alpha_beta=False):
    log_sum_exp_fn = FORMS[form]
    a, b, e, alpha, beta = params
    if tie_alpha_beta:
        beta = alpha
    predictions = np.exp(log_sum_exp_fn(a, b, e, alpha, beta, N, D))
    return custom_huber_loss(losses, predictions, delta=delta)

# Define the Huber loss function on residuals
def huber_loss(residuals, delta=1e-3):
    # Calculate the difference
    diff = residuals
    # Calculate the condition for Huber loss
    cond = np.abs(diff) <= delta
    # Apply Huber loss formula
    loss = np.where(cond, 0.5 * diff**2, delta * (np.abs(diff) - 0.5 * delta))
    return loss

FORMS = {
    'chinchilla': log_sum_exp_chinch,
    'kaplan': log_sum_exp_kaplan,
}


import heapq

OBJECTIVES = {
    'log_huber': log_huber_loss_objective,
    'huber': huber_loss_objective,
    'log_mae': log_mae_objective,
    'log_mse': log_mse_objective,
    # 'constant': constant_term_objective,
    # 'scale': scale_objective,
    'scaled_log_huber': objective,
}


# transform parameters back from log space
def untransform_params(param_array):
    if len(np.shape(param_array)) == 2:
      return np.hstack((np.exp(param_array[:, :3]), param_array[:, 3:]))
    else:
      return np.hstack((np.exp(param_array[:3]), param_array[3:]))


class PQItem(object):
    def __init__(self, loss, params):
        self.loss = loss
        self.params = params

    def __lt__(self, other):
        return self.loss > other.loss # reversed because we want to retain lower loss params

def fit_perf_power_law(
    N,
    D,
    model_losses,
    indices,
    init_grid=None,
    keep_best_k_from_init_grid=-1,
    form="chinchilla", # chinchilla, kaplan
    method='BFGS',
    obj_name='log_huber',
    tie_alpha_beta=False,
    use_grad=False,
    tol=None,
    add_sigma=False,
    max_opt_inits=-1, #no max by default
    dense_init=False,
):
    """
    From hoffman, et al:
    We use the LBFGS algorithm to find local minima of the objective above, started on a grid
    of initialisation given by:
    𝛼 ∈ {0., 0.5, . . . , 2.},
    𝛽 ∈ {0., 0.5, . . . , 2.},
    𝑒 ∈ {−1., −.5, . . . , 1.},
    𝑎 ∈ {0, 5, . . . , 25}, and
    𝑏 ∈ {0, 5, . . . , 25}
    """

    best_loss = np.inf
    best_params = None
    pq = []
    param_list = []
    i = 0
    if init_grid is None:
        if dense_init:
            a_vals = np.arange(0, 25, 1)
            b_vals = np.arange(0, 25, 1)
            e_vals = np.arange(-1, 1, 0.1)
            alpha_vals = np.arange(0, 2, 0.1)
            beta_vals = np.arange(0, 2, 0.1)
        else:
            a_vals = np.arange(0, 25, 5)
            b_vals = np.arange(0, 25, 5)
            e_vals = np.arange(-1, 1, 0.5)
            alpha_vals = np.arange(0, 2, 0.5)
            beta_vals = np.arange(0, 2, 0.5)
    else:
        a_vals, b_vals, e_vals, alpha_vals, beta_vals = init_grid
    if tie_alpha_beta:
        beta_vals = np.array([0])
    if form == 'kaplan':
        e_vals = np.array([0])

    grid = np.array(np.meshgrid(
        a_vals, b_vals, e_vals, alpha_vals, beta_vals
    )).T.reshape(-1, 5)
    np.random.shuffle(grid)

    obj = partial(OBJECTIVES[obj_name], form=form, tie_alpha_beta=tie_alpha_beta)

    if keep_best_k_from_init_grid > 0:
        init_pq = []
        for init_params in grid:
            init_loss = obj(init_params, N[indices], D[indices], model_losses[indices])#, form=form)
            if len(init_pq) < keep_best_k_from_init_grid:
                heapq.heappush(init_pq, PQItem(init_loss, init_params))
            elif init_loss < init_pq[0].loss:
                heapq.heappushpop(init_pq, PQItem(init_loss, init_params))
        grid = [pq_item.params for pq_item in heapq.nlargest(keep_best_k_from_init_grid, init_pq)]

    results_dict = {}
    for init_params in grid:
        if method == 'grid':
            params = init_params
            loss = obj(init_params, N, D, model_losses[indices])
            success = True
        else:
            if add_sigma or obj_name in ['scaled_log_huber']:
                init_params = init_params + [0]
            if method == 'nonlinear_least_squares':
                result = scipy.optimize.least_squares(obj, init_params, args=(N[indices], D[indices], model_losses[indices]), )
            else:
                result = minimize(obj, init_params, args=(N[indices], D[indices], model_losses[indices]), tol=tol, method=method, jac=grad(obj) if use_grad else None)

            # set beta value to alpha
            if tie_alpha_beta:
                result.x[4] = result.x[3]
            params, loss, success = result.x, result.fun, result.success

        results_dict[tuple(init_params)] = {'params': params, 'loss': loss}
        param_list.append(params)

        # update best params so far
        if success and loss < best_loss:
            best_loss = loss
            best_params = params

        # add all best 100 results to priority queue
        if len(pq) < 100:
            heapq.heappush(pq, PQItem(loss, params))
        elif loss < pq[0].loss:
            heapq.heappushpop(pq, PQItem(loss, params))

        i += 1
        if i == max_opt_inits:
            break

    largest = heapq.nlargest(100, pq)

    if best_params is not None:
        best_params_untransformed = list(untransform_params(best_params))
        A, B, E, alpha, beta = best_params_untransformed
        print(f"Best fit parameters: A={A}, B={B}, E={E}, alpha={alpha}, beta={beta}")
        print(f"Best loss: {best_loss}")

        param_list = np.array(param_list)
        cov_matrix = np.cov(np.transpose(param_list))
        param_list_untransformed = untransform_params(param_list)
        cov_matrix_untransformed = np.cov(np.transpose(param_list_untransformed))
        standard_errors = np.sqrt(np.diag(cov_matrix[:5, :5]))
        standard_errors_untransformed = np.sqrt(np.diag(cov_matrix_untransformed[:5, :5]))

        parameter_labels = ["A", "B", "E", "alpha", "beta"]
        print("Parameter estimates and their standard errors")
        for index, label in enumerate(parameter_labels):
            print("%s: %.5f (%.5f)" % (label, best_params_untransformed[index], standard_errors_untransformed[index]))

    else:
        print("Optimization failed to converge.")

    return {'pq': pq, 'loss': best_loss, 'params':best_params, 'form': form}

def scaling_law_chinch(Ns, Ds, params=CHINCHILLA_PARAMS_LOG):
    """
    return predicted loss values based on fit params for Chinchilla form
    scaling law
    """
    A, B, E, alpha, beta = untransform_params(params)
    Ls = []
    for N, D in zip(Ns, Ds):
        L = E + (A / (N ** alpha)) + (B / (D ** beta))
        Ls.append(L)
    return Ls

def scaling_law_kaplan(Ns, Ds, params=KAPLAN_PARAMS_LOG):
    """
    return predicted loss values based on fit params for Kaplan form
    scaling law
    """
    A, B, _, alpha, beta = untransform_params(params)
    Ls = []
    for N, D in zip(Ns, Ds):
        L = ((N / A) ** (alpha) + D / B) ** beta
        Ls.append(L)
    return Ls

def opt_N_D_chinchilla(params, Cs):
    """
    Find optimal N and D values for a specific set of C values.
    Closed form taken from Hoffmann, et. al.
    """
    A, B, E, opt_alpha, opt_beta = untransform_params(params)

    opt_a =  opt_beta / (opt_alpha+opt_beta)
    opt_b =  opt_alpha / (opt_alpha+opt_beta)
    G = ((opt_alpha * A) / (opt_beta * B)) ** (1 / (opt_alpha + opt_beta))
    opt_Ns, opt_Ds = [], []
    for C in Cs:
        opt_Ns.append(G * (C / 6) ** opt_a)
        opt_Ds.append((1 / G) * (C / 6) ** opt_b)

    return opt_Ns, opt_Ds

def opt_N_D_kaplan(params, Cs, max_epsilon=1e-6):
    """
    Estimate optimal N and D values for a specific set of C values.
    No closed form solution found in Kaplan. Relying on trisecting values until
    sufficiently close to optimal
    """
    opt_Ns, opt_Ds = [], []
    for C in Cs:
        min = {"N": C/6,  "D": 1, "L": scaling_law_kaplan([C/6], [1], params=params)[0]}
        max = {"N": 1, "D": C/6, "L": scaling_law_kaplan([1], [C/6], params=params)[0]}
        l1, l2 = np.log(min["N"]), np.log(max["N"])
        while l2 - l1 > max_epsilon:
            mid_n = np.exp(np.array([l1*2/3 + l2*1/3, l1*1/3 + l2*2/3]))
            mid_d = (C / 6) / mid_n
            mid_l = scaling_law_kaplan(mid_n, mid_d, params=params)
            argmin = np.argmin([min["L"]] + mid_l + [max["L"]])
            if argmin in [0, 1]:
                max = {"N": mid_n[2], "D": mid_d[2], "L": mid_l[2]}
            else:
                min = {"N": mid_n[1], "D": mid_d[1], "L": mid_l[1]}
            l1, l2 = np.log(min["N"]), np.log(max["N"])

        opt_Ns.append(min["N"])
        opt_Ds.append(min["D"])

    return opt_Ns, opt_Ds

def opt_N_D_kaplan_original(Cs):
    # Kaplan's C is in PF-days = 8.64e19 FLOPs
    Cs_k = [C / 8.64e19 for C in Cs]
    Ns = [1.3e9 * (C ** 0.73) for C in Cs_k]
    Ds = [C / (6 * N) for (N, C) in zip(Ns, Cs)]
    return Ns, Ds

def opt_N_D_llama_original(Cs):
    Ns = [0.29 * (C ** 0.53) for C in Cs]
    Ds = [C / (6 * N) for (N, C) in zip(Ns, Cs)]
    return Ns, Ds
# shown in Figure 3. Extrapolation of the resulting scaling law to 3.8 × 1025 FLOPs suggests training a 402B
# parameter model on 16.55T tokens.

chinchilla_compute = (1.4*10**12)*(70*10**9)*6

labels_dict = {
    "C": "Training compute (FLOP)",
    "N": "Power law optimal parameters",
    "D": "Power law optimal data (tokens)",
    "D/N": "Tokens per parameters ratio",
}

LLMS_MARKERS = ["*",".", "P", "d", "^", "X"]
LLMS = {
    "Llama 3 405B (reported)": {"N": 405e9, "D": 15.6e12},
    "Llama 2 70B (reported)": {"N": 70e9, "D": 2e12},
    "Chinchilla (reported)": {"N": 70e9, "D": 1.4e12},
    "GPT-2 (est. on reports)": {"N": 1.5e9, "D": 100e9},
}
LLM_SORTED_KEYS = sorted(LLMS.keys())
{v.update({"C": 6 * v["N"] * v["D"], "D/N": v["D"] / v["N"], "marker": LLMS_MARKERS[i]}) for i, v in enumerate(LLMS.values())}

LLMS_POINTS = [
    "Llama 3 405B (reported)",
    # "Llama 2 70B",
    "Chinchilla (reported)",
    "GPT-2 (est. on reports)",
]
LLMS_POINTS_IDS = sorted([LLM_SORTED_KEYS.index(m) for m in LLMS_POINTS])

DEFAULT_Cs = [LLMS[m]["C"] for m in LLM_SORTED_KEYS] + [1e18, 1e19, 1e20, 1e21, 1e22, 1e23, 1e24, 1e25, 1e26]

def log_format(val, pos):
    """Format the tick labels on logarithmic scale."""
    val_str = '{:g}'.format(val)
    if float(val_str) >= 1.0:
        # If the value is a whole number, return it as an integer.
        return str(int(val))
    else:
        # Otherwise, return the string as is (useful for fractional values).
        return val_str

def format_number(val):
    if val > 0.95e12:
        return '{:g}'.format(float('{:.1g}'.format(val/1e12))) + "T"
    elif val > 0.95e9:
        return '{:g}'.format(float('{:.1g}'.format(val/1e9))) + "B"
    elif val > 0.95e6:
        return '{:g}'.format(float('{:.1g}'.format(val/1e6))) + "M"
    elif val > 0.95e3:
        return '{:g}'.format(float('{:.1g}'.format(val/1e3))) + "k"
    else:
        return '{:g}'.format(float('{:.1g}'.format(val)))

def array_to_list(array):
    if isinstance(array, np.ndarray):
        return array_to_list(array.tolist())
    elif isinstance(array, list):
        return [array_to_list(item) for item in array]
    elif isinstance(array, tuple):
        return tuple(array_to_list(item) for item in array)
    elif isinstance (array, dict):
        return {key: array_to_list(value) for key, value in array.items()}
    else:
        return array

def plot_lines(
    results_to_plot,
    x_axis="C",
    y_axis="N",
    compute_vals=DEFAULT_Cs,
    model_annotations_ids=LLMS_POINTS_IDS,
    figure_folder="",
    save_as=None,
    format='pdf',
    included_reference_lines=['chinchilla', 'kaplan'],
    add_reference_points=False,
    highlight_low=True,
):
    # compute_vals
    if add_reference_points:
        [plt.axvline(LLMS[LLM_SORTED_KEYS[j]][x_axis], color='0.75', linestyle=':') for j in model_annotations_ids]
    results_to_plot = {k:results_to_plot[k] for k in results_to_plot if results_to_plot[k] is not None}
    all_losses = [results_to_plot[k].get("loss", np.inf) for k in sorted(list(results_to_plot.keys()))]
    all_losses = [np.mean(item) if hasattr(item, "__len__") else item for item in all_losses]
    lowest_loss_l = sorted(list(results_to_plot.keys()))[np.argmin(all_losses)]
    l_dict = copy(results_to_plot)
    for l, result in l_dict.items():
        if 'chinchilla' in result['form']:
            opt_Ns, opt_Ds = opt_N_D_chinchilla(result['params'], compute_vals)
            Ls = scaling_law_chinch(opt_Ns, opt_Ds, result['params'])
        elif 'kaplan' in result['form']:
            opt_Ns, opt_Ds = opt_N_D_kaplan(result['params'], compute_vals)
            Ls = scaling_law_kaplan(opt_Ns, opt_Ds, result['params'])
        elif "isoflop" in result['form']:
            key_coef, key_exponent = f'n_coef', f'n_exponent'
            d = result['params']
            opt_Ns = np.array(d[key_coef]) * compute_vals ** np.array(d[key_exponent])
            opt_Ds = (compute_vals / opt_Ns) / 6
            Ls = None
        result.update({
            "plot_info": {"C": compute_vals, "N": opt_Ns, "D": opt_Ds, "L": Ls, "D/N": [d/n for n, d in zip(opt_Ns, opt_Ds)],}
        })
        result = array_to_list(result)

    if save_as:
        os.makedirs(figure_folder, exist_ok=True)
        for k in l_dict:
            if 'pq' in l_dict[k]:
                del l_dict[k]['pq']
        l_dict = array_to_list(l_dict)
        with open(f"{figure_folder}/{save_as}.json", "w") as f:
            json.dump(l_dict, f)

    line_width = 0.8
    ref_mins, ref_maxes = {}, {}
    for i, (label, result_dict) in enumerate(l_dict.items()):
        d = result_dict["plot_info"]
        if x_axis in d and y_axis in d:
            j, k = np.argmin(d[x_axis]), np.argmax(d[x_axis])
            plt.plot(
                [d[x_axis][k], d[x_axis][j]], [d[y_axis][k], d[y_axis][j]], label=label,
                linestyle="-" if (highlight_low and label == lowest_loss_l) else "-.",
                linewidth=line_width*2 if (highlight_low and label == lowest_loss_l) else line_width,
                color=get_color(i))
            # show Y axis values for bottom- and top-most lines at reference LLM X values
            if add_reference_points:
                for j, (x, y) in enumerate(zip(d[x_axis][:len(LLMS)], d[y_axis][:len(LLMS)])):
                    if j not in model_annotations_ids:
                        continue
                    if ref_mins.get(x) is None or ref_mins.get(x)[0] > y:
                        ref_mins[x] = (y, get_color(i), f"N: {format_number(d['N'][j])}\nD: {format_number(d['D'][j])}")
                    if ref_maxes.get(x) is None or ref_maxes.get(x)[0] < y:
                        ref_maxes[x] = (y, get_color(i), f"N: {format_number(d['N'][j])}\nD: {format_number(d['D'][j])}")

    if 'chinchilla' in included_reference_lines:
        opt_Ns, opt_Ds = opt_N_D_chinchilla(CHINCHILLA_PARAMS_LOG, compute_vals)
        Ls = scaling_law_chinch(opt_Ns, opt_Ds, CHINCHILLA_PARAMS_LOG )
        chinchilla_dict = {"C": compute_vals, "N": opt_Ns, "D": opt_Ds, "L": Ls, "D/N": [d/n for n, d in zip(opt_Ns, opt_Ds)]}
        j, k = np.argmin(chinchilla_dict[x_axis]), np.argmax(chinchilla_dict[x_axis])
        plt.plot(
            [chinchilla_dict[x_axis][j], chinchilla_dict[x_axis][k]],
            [chinchilla_dict[y_axis][j], chinchilla_dict[y_axis][k]],
            label="Hoffmann (reported)", color='0.4', linestyle=':')

    if 'kaplan' in included_reference_lines:
        opt_Ns, opt_Ds = opt_N_D_kaplan_original(compute_vals)
        Ls = scaling_law_kaplan(opt_Ns, opt_Ds, KAPLAN_PARAMS_LOG )
        kaplan_dict = {"C": compute_vals, "N": opt_Ns, "D": opt_Ds, "L": Ls, "D/N": [d/n for n, d in zip(opt_Ns, opt_Ds)]}
        j, k = np.argmin(kaplan_dict[x_axis]), np.argmax(kaplan_dict[x_axis])
        plt.plot(
            [kaplan_dict[x_axis][j], kaplan_dict[x_axis][k]],
            [kaplan_dict[y_axis][j], kaplan_dict[y_axis][k]],
            label="Kaplan (reported)", color='0.2', linestyle=':', linewidth=0.8)

    if 'besiroglu' in included_reference_lines:
        opt_Ns, opt_Ds = opt_N_D_chinchilla(BESIROGLU_PARAMS_LOG, compute_vals)
        Ls = scaling_law_chinch(opt_Ns, opt_Ds, BESIROGLU_PARAMS_LOG )
        besiroglu_dict = {"C": compute_vals, "N": opt_Ns, "D": opt_Ds, "L": Ls, "D/N": [d/n for n, d in zip(opt_Ns, opt_Ds)]}
        j, k = np.argmin(besiroglu_dict[x_axis]), np.argmax(besiroglu_dict[x_axis])
        plt.plot(
            [besiroglu_dict[x_axis][j], besiroglu_dict[x_axis][k]],
            [besiroglu_dict[y_axis][j], besiroglu_dict[y_axis][k]],
            label="Besiroglu (reported)", color='0.5', linestyle=':', linewidth=0.5)

    for i in LLMS_POINTS_IDS:
        model = LLM_SORTED_KEYS[i]
        plt.plot(LLMS[model][x_axis], LLMS[model][y_axis], 'o', color='0.2', marker=LLMS[model]["marker"], markersize=7, label=f"{model}", alpha=0.4)  # 'o' is the marker style for a filled circle

    if add_reference_points:
        [plt.annotate(label, (x, y),
             xytext=(2, -25),
                      textcoords='offset points',
                      color=c,
        ) for x, (y, c, label) in ref_mins.items()]
        [plt.plot(x, y,'o', color=c,markersize=5,
        ) for x, (y, c, label) in ref_mins.items()]
        [plt.annotate(label, (x, y),
             xytext=(-25, 8),
                      textcoords='offset points',
                      color=c,
        ) for x, (y, c, label) in ref_maxes.items()]
        [plt.plot(x, y,'o', color=c,markersize=5,
        ) for x, (y, c, label) in ref_maxes.items()]

    plt.xscale("log")
    plt.yscale("log")

    plt.xlim([min(compute_vals), max(compute_vals)])
    plt.subplots_adjust(bottom=0.15)  # Adjust bottom margin to make space for the x-axis

    plt.xlabel(labels_dict[x_axis])
    plt.ylabel(labels_dict[y_axis])
    plt.legend(loc='upper left', prop={'size': 9},
              )
    plt.grid()
    if save_as:
        plt.savefig(f"{figure_folder}/{save_as}.{format}", format=format)
    plt.show()

def plot_from_json(file_name):
    with open(file_name, 'r') as f:
        data = json.load(f)
    plot_lines(data)

MAX_MODEL_SIZE = "1b"

DEFAULT_DATA_ARGS = {
    "epoch_ai":{
        "data_name": "epoch_ai",
        "loss_field_name": "loss",
        "default_data_selections": {},
    },
    "rsld_final":{
        "data_name":"rsld_final",
        "loss_field_name":"loss",
        "default_data_selections": {},
    },
    "rsld_ckpt":{
        "data_name":"rsld_ckpt",
        "loss_field_name":"loss",
        "default_data_selections": {},
    },
    "rsld_isoflop":{
        "data_name":"rsld_isoflop",
        "loss_field_name":"loss",
        "default_data_selections": {},
    },
    "misfitting_old":{
        "data_name":"misfitting_old",
        "loss_field_name": 'C4 Eval Loss',
        "default_data_selections": {"lr": ("opt", None), "N": ("<", MAX_MODEL_SIZE)},
    },
    "misfitting_new_ckpt":{
        "data_name":"misfitting_new_ckpt",
        "loss_field_name": 'C4 Eval Loss',
        "default_data_selections": {"lr": ("opt", None), "N": ("<", MAX_MODEL_SIZE)},
    },
    "misfitting_new_final":{
        "data_name":"misfitting_new_final",
        "loss_field_name": 'C4 Eval Loss',
        "default_data_selections": {"lr": ("opt", None), "N": ("<", MAX_MODEL_SIZE)},
    },
}

DATA_ARGS = {}

for dt in [
    "epoch_ai", "rsld_final", "rsld_ckpt", "rsld_isoflop",
    "misfitting_new_ckpt", "misfitting_new_final"
]:
  DATA_ARGS.update({
      f"{dt}_wiki":{
          "data_name": dt,
          "loss_field_name": 'Wiki Eval Loss',
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_noemb_n":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "n_field_name": "N_no_emb",
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_noemb_c":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "c_field_name": "Approx_C_no_emb" if dt == "misfitting_old" else "C_no_emb",
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_noemb_nc":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "c_field_name": "Approx_C_no_emb" if dt == "misfitting_old" else "C_no_emb",
          "n_field_name": "N_no_emb",
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_approx_c":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "c_field_name": "Approx_C" if dt == "misfitting_old" else "C_6ND",
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_fixed_lr_1e-3":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"lr": ("=", "1e-3")},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
        f"{dt}_fixed_lr_2e-3":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"lr": ("=", "2e-3")},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_fixed_lr_4e-3":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"lr": ("=", "4e-3")},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_max_model_100m":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"N": ("<", "100m")},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_max_model_400m":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"N": ("<", "400m")},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_max_model_1b":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"N": ("<", "1b")},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_max_model_None":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"N": (None, None)},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_low_ratio":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"ratio": ("<", "18")},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_high_ratio":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"ratio": (">", "22")},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_no_filtering":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": None,
      },
      f"{dt}_ckpt_10pct":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"ckpt": (">", "0.1")},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_ckpt_20pct":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"ckpt": (">", "0.2")},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_ckpt_50pct":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"ckpt": (">", "0.5")},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_opt_lr":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"lr": ("opt", None)},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}_all_lrs":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": {"lr": (None, None)},
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
      f"{dt}":{
          "data_name": dt,
          "loss_field_name": DEFAULT_DATA_ARGS.get(dt, {}).get("loss_field_name"),
          "data_selections": [],
          "default_data_selections": DEFAULT_DATA_ARGS.get(dt, {}).get("default_data_selections"),
      },
  })

from datetime import datetime

# Get the current date and time
now = datetime.now()
# Format the date and time as a string
date_time_string = now.strftime("%Y-%m-%d-%H:%M:%S")
figure_folder = f"{LOCAL_DIR}/figures/{MAX_MODEL_SIZE}/{date_time_string}"

results_dict = {}

grps_to_plot = [
    "form",
    "opt",
    "init",
    "loss",
    "counting",
    "filter",
    "lr",
    "dn_ratio",
]

datasets_to_plot = [
    "misfitting_new_final",
    "misfitting_new_ckpt",
    "epoch_ai",
    "rsld_final",
    "rsld_ckpt",
  ]


fast_debug = False

for dataset in datasets_to_plot:
    dataset_dict=DATA_ARGS[dataset]
    data_name=dataset_dict['data_name']
    x_axis="C"
    y_axis="N"

    results_dict[data_name] = {}
    N, D, losses, indices, data_df = get_data(**dataset_dict)
    if fast_debug:
        grp = "debug"
        results_dict[data_name][grp] = {}
        results_dict[data_name][grp]['Hoffmann Form'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber")
        plot_lines(results_dict[data_name][grp], x_axis=x_axis, y_axis=y_axis, figure_folder=figure_folder, save_as=f"analysis_{grp}_{data_name}_{x_axis}_vs_{y_axis}_full", add_reference_points=True, highlight_low=False)

    chinchilla_result = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber")

    grp = 'form'
    if grp in grps_to_plot:
      results_dict[data_name][grp] = {}
      results_dict[data_name][grp]['Hoffmann Form'] = chinchilla_result
      if data_name == "rsld_ckpt":
          _, _, _, _, isoflop_data = get_data("rsld_isoflop", loss_field_name="loss")
          isoflop_results = fit_isoflop_power_law(isoflop_data, data_name=data_name, do_adapt=False)
          for k in isoflop_results:
              if "bootstrap" in k:
                  continue
              results_dict[data_name][grp][f'Original IsoFLOP ({k})'] = {'params': isoflop_results[k], 'form': f'original isoflop {k}'}
      if 'misfitting' in data_name or data_name == 'rsld_ckpt':
          isoflop_results = fit_isoflop_power_law(data_df, data_name=data_name, do_adapt=True)
          for k in isoflop_results:
              if "bootstrap" in k:
                  continue
              results_dict[data_name][grp][f'IsoFLOP ({k})'] = {'params': isoflop_results[k], 'form': f'isoflop {k}'}
      # results_dict[data_name][grp]['Kaplan Form'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber", form='kaplan')
      results_dict[data_name][grp]['Hoffmann w/ alpha=beta'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber", form='chinchilla', tie_alpha_beta=True)
      if 'misfitting' in data_name or data_name == 'rsld_ckpt':
          plot_keys = ['Hoffmann Form',  'Hoffmann w/ alpha=beta', 'IsoFLOP (weighted)']
          plot_lines({k:results_dict[data_name][grp][k] for k in plot_keys}, x_axis=x_axis, y_axis=y_axis, figure_folder=figure_folder, save_as=f"analysis_{grp}_{data_name}_{x_axis}_vs_{y_axis}", add_reference_points=True, highlight_low=False)
          plot_lines(results_dict[data_name][grp], x_axis=x_axis, y_axis=y_axis, figure_folder=figure_folder, save_as=f"analysis_{grp}_{data_name}_{x_axis}_vs_{y_axis}_full", add_reference_points=True, highlight_low=False)

      else:
          plot_keys = ['Hoffmann Form',  'Hoffmann w/ alpha=beta']
          plot_lines({k:results_dict[data_name][grp][k] for k in plot_keys}, x_axis=x_axis, y_axis=y_axis, figure_folder=figure_folder, save_as=f"analysis_{grp}_{data_name}_{x_axis}_vs_{y_axis}", add_reference_points=True, highlight_low=False)

    grp = 'opt'
    if grp in grps_to_plot:
      # plot_keys = ['LBFGS', 'LBFGS (high tol)', 'LBFGS (med tol)', 'LBFGS (low tol)', 'LBFGS (gradient)', 'BFGS', 'BFGS (high tol)', 'BFGS (low tol)', 'BFGS (gradient)', 'Nonlinear Least Squares', 'Grid Search']
      plot_keys = ['LBFGS', 'LBFGS (high tol)', 'LBFGS (gradient)', 'BFGS',  'Nonlinear Least Squares', 'Grid Search']
      results_dict[data_name][grp] = {}
      results_dict[data_name][grp]['LBFGS'] = chinchilla_result if 'LBFGS' in plot_keys else None
      results_dict[data_name][grp]['LBFGS (high tol)'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber", tol=1e-4) if 'LBFGS (high tol)' in plot_keys else None
      results_dict[data_name][grp]['LBFGS (med tol)'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber", tol=1e-6) if 'LBFGS (med tol)' in plot_keys else None
      results_dict[data_name][grp]['BFGS'] = fit_perf_power_law(N, D, losses, indices, method="BFGS", obj_name="log_huber") if 'BFGS' in plot_keys else None
      results_dict[data_name][grp]['Nonlinear Least Squares'] = fit_perf_power_law(N, D, losses, indices, method="nonlinear_least_squares") if 'Nonlinear Least Squares' in plot_keys else None
      results_dict[data_name][grp]['Grid Search'] = fit_perf_power_law(N, D, losses, indices, method="grid", dense_init=True) if 'Grid Search' in plot_keys else None

      results_dict[data_name][grp]['LBFGS (low tol)'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber", tol=1e-8) if 'LBFGS (low tol)' in plot_keys else None
      results_dict[data_name][grp]['LBFGS (gradient)'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber", use_grad=True) if 'LBFGS (gradient)' in plot_keys else None
      results_dict[data_name][grp]['BFGS (gradient)'] = fit_perf_power_law(N, D, losses, indices, method="BFGS", obj_name="log_huber", use_grad=True) if 'BFGS (gradient)' in plot_keys else None
      results_dict[data_name][grp]['BFGS (high tol)'] = fit_perf_power_law(N, D, losses, indices, method="BFGS", obj_name="log_huber", tol=1e-4) if 'BFGS (high tol)' in plot_keys else None
      results_dict[data_name][grp]['BFGS (low tol)'] = fit_perf_power_law(N, D, losses, indices, method="BFGS", obj_name="log_huber", tol=1e-8) if 'BFGS (low tol)' in plot_keys else None
      plot_lines(results_dict[data_name][grp], x_axis=x_axis, y_axis=y_axis, figure_folder=figure_folder, save_as=f"analysis_{grp}_{data_name}_{x_axis}_vs_{y_axis}", add_reference_points=True, highlight_low=False)

    grp = 'init'
    if grp in grps_to_plot:
      plot_keys = ['Optimize Full Grid', 'Search Grid, Optimize 1', 'Search Grid, Optimize 1000', 'Optimize 100 Random', 'Initialize from Chinchilla']
      results_dict[data_name][grp] = {}
      results_dict[data_name][grp]['Optimize Full Grid'] = chinchilla_result if 'Optimize Full Grid' in plot_keys else None
      results_dict[data_name][grp]['Search Grid, Optimize 1'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber", keep_best_k_from_init_grid=1) if 'Search Grid, Optimize 1' in plot_keys else None
      results_dict[data_name][grp]['Search Grid, Optimize 1000'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber", keep_best_k_from_init_grid=1000) if 'Search Grid, Optimize 1000' in plot_keys else None
      results_dict[data_name][grp]['Optimize 100 Random'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber", max_opt_inits=100) if 'Optimize 100 Random' in plot_keys else None
      results_dict[data_name][grp]['Initialize from Chinchilla'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber", init_grid=[[v] for v in CHINCHILLA_PARAMS_LOG]) if 'Initialize from Chinchilla' in plot_keys else None
      # results_dict[data_name][grp]['Optimize Denser Grid'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_huber", dense_init=True)
      plot_lines(results_dict[data_name][grp], x_axis=x_axis, y_axis=y_axis, figure_folder=figure_folder, save_as=f"analysis_{grp}_{data_name}_{x_axis}_vs_{y_axis}", add_reference_points=True)

    grp = 'loss'
    if grp in grps_to_plot:
      plot_keys = ['Log-Huber', 'Huber', 'MSE', 'MAE']
      results_dict[data_name][grp] = {}
      results_dict[data_name][grp]['Log-Huber'] = chinchilla_result if 'Log-Huber' in plot_keys else None
      results_dict[data_name][grp]['Huber'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="huber") if 'Huber' in plot_keys else None
      results_dict[data_name][grp]['MSE'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_mse") if 'MSE' in plot_keys else None
      results_dict[data_name][grp]['MAE'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="log_mae") if 'MAE' in plot_keys else None
      # results_dict[data_name]['scale'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="scale")
      # results_dict[data_name]['base'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="base")
      # results_dict[data_name]['constant'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="constant")
      # results_dict['Scaled Log-Huber'] = fit_perf_power_law(N, D, losses, indices, method="L-BFGS-B", obj_name="scaled_log_huber")
      plot_lines({k:results_dict[data_name][grp][k] for k in plot_keys}, x_axis=x_axis, y_axis=y_axis, figure_folder=figure_folder, save_as=f"analysis_{grp}_{data_name}_{x_axis}_vs_{y_axis}", add_reference_points=True, highlight_low=False)

    if data_name in ["misfitting_new_final", "misfitting_new_ckpt", "rsld_final", "rsld_ckpt"]:
        grp = "counting"
        if grp in grps_to_plot:
          plot_keys = ["N, C w/ embed", "N w/o embed", "C w/o embed", "N, C w/o embed", "C=6ND approximation"]
          results_dict[data_name][grp] = {}
          results_dict[data_name][grp]["N, C w/ embed"] = chinchilla_result
          for dkey, sub_dataset in [("N w/o embed",f"{data_name}_noemb_n"), ("C w/o embed", f"{data_name}_noemb_c"), ("N, C w/o embed", f"{data_name}_noemb_nc"), ("C=6ND approximation", f"{data_name}_approx_c")]:
              dataset_dict=DATA_ARGS[sub_dataset]
              _N, _D, _losses, _indices, _data_df = get_data(**dataset_dict)
              results_dict[data_name][grp][dkey] = fit_perf_power_law(_N, _D, _losses, _indices, method="L-BFGS-B", obj_name="log_huber") if dkey in plot_keys else None
          plot_lines(results_dict[data_name][grp], x_axis=x_axis, y_axis=y_axis, figure_folder=figure_folder, save_as=f"analysis_{grp}_{data_name}_{x_axis}_vs_{y_axis}", add_reference_points=True)

    if data_name in ["misfitting_new_ckpt", "rsld_ckpt", ]:
        grp = "filter"
        if grp in grps_to_plot:
          plot_keys = ["Final checkpoint only", "All checkpoints", "10%+ training done", "20%+ training done", "50%+ training done"]
          results_dict[data_name][grp] = {}
          results_dict[data_name][grp]["All checkpoints"] = chinchilla_result
          for dkey, sub_dataset in [
              ("10%+ training done", f"{data_name}_ckpt_10pct"),
              ("20%+ training done", f"{data_name}_ckpt_20pct"),
              ("50%+ training done", f"{data_name}_ckpt_50pct"),
              ("Final checkpoint only", "misfitting_new_final"),
          ]:
              dataset_dict=DATA_ARGS[sub_dataset]
              _N, _D, _losses, _indices, _data_df = get_data(**dataset_dict)
              results_dict[data_name][grp][dkey] = fit_perf_power_law(_N, _D, _losses, _indices, method="L-BFGS-B", obj_name="log_huber") if dkey in plot_keys else None
          plot_lines({k:results_dict[data_name][grp][k] for k in plot_keys}, x_axis=x_axis, y_axis=y_axis, figure_folder=figure_folder, save_as=f"analysis_{grp}_{data_name}_{x_axis}_vs_{y_axis}", add_reference_points=True)

    if data_name in ["misfitting_new_final", "misfitting_new_ckpt", "epoch_ai",]:
        grp = "lr"
        if grp in grps_to_plot:
          plot_keys = ["Sweep Learning Rate (LR)","Fixed LR = 1e-3", "Fixed LR = 2e-3", "Fixed LR = 4e-3", "Use all LRs"]
          results_dict[data_name][grp] = {}
          results_dict[data_name][grp]["Sweep Learning Rate (LR)"] = chinchilla_result
          for dkey, sub_dataset in [
              ("Fixed LR = 1e-3", f"{data_name}_fixed_lr_1e-3"),
              ("Fixed LR = 2e-3", f"{data_name}_fixed_lr_2e-3"),
              ("Fixed LR = 4e-3", f"{data_name}_fixed_lr_4e-3"),
              ("Use all LRs", f"{data_name}_all_lrs"),
          ]:
              dataset_dict=DATA_ARGS[sub_dataset]
              _N, _D, _losses, _indices, _data_df = get_data(**dataset_dict)
              results_dict[data_name][grp][dkey] = fit_perf_power_law(_N, _D, _losses, _indices, method="L-BFGS-B", obj_name="log_huber") if dkey in plot_keys else None
          plot_lines(results_dict[data_name][grp], x_axis=x_axis, y_axis=y_axis, figure_folder=figure_folder, save_as=f"analysis_{grp}_{data_name}_{x_axis}_vs_{y_axis}", add_reference_points=True)

    grp = "dn_ratio"
    if grp in grps_to_plot:
        plot_keys = ["All models", "N <= 100M", "N <= 400M", "D/N <= 18", "D/N >= 22",]
        results_dict[data_name][grp] = {}
        for dkey, sub_dataset in [
            ("All models", f"{data_name}_max_model_None"),
            ("N <= 100M", f"{data_name}_max_model_100m"),
            ("N <= 400M", f"{data_name}_max_model_400m"),
            ("D/N <= 18", f"{data_name}_low_ratio"),
            ("D/N >= 22", f"{data_name}_high_ratio"),
        ]:
            dataset_dict=DATA_ARGS[sub_dataset]
            _N, _D, _losses, _indices, _data_df = get_data(**dataset_dict)
            results_dict[data_name][grp][dkey] = fit_perf_power_law(_N, _D, _losses, _indices, method="L-BFGS-B", obj_name="log_huber") if dkey in plot_keys else None
        plot_lines(results_dict[data_name][grp], x_axis=x_axis, y_axis=y_axis, figure_folder=figure_folder, save_as=f"analysis_{grp}_{data_name}_{x_axis}_vs_{y_axis}", add_reference_points=True)

