{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "lang = \"py\"\n",
    "run_lang = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "entity, project = \"loubnabnl\", \"scaling_laws\"  # set to your entity and project \n",
    "runs = api.runs(entity + \"/\" + project) \n",
    "\n",
    "summary_list, config_list, name_list = [], [], []\n",
    "for run in runs: \n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files \n",
    "    summary_list.append(run.summary._json_dict)\n",
    "\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config_list.append(\n",
    "        {k: v for k,v in run.config.items()\n",
    "         if not k.startswith('_')})\n",
    "\n",
    "    # .name is the human-readable name of the run.\n",
    "    name_list.append(run.name)\n",
    "\n",
    "orig_runs_df = pd.DataFrame({\n",
    "    \"summary\": summary_list,\n",
    "    \"config\": config_list,\n",
    "    \"name\": name_list\n",
    "    })\n",
    "\n",
    "def expand_dict_to_columns(df, col):\n",
    "    return pd.concat([df.drop([col], axis=1),  pd.json_normalize(df[col])], axis=1)\n",
    "\n",
    "run_df = expand_dict_to_columns(orig_runs_df, \"summary\")\n",
    "run_df = expand_dict_to_columns(run_df, \"config\")\n",
    "\n",
    "keep_cols = [c for c in run_df if \"lm-loss-validation/TEST_\" not in c]\n",
    "run_df = run_df[keep_cols]\n",
    "\n",
    "df_run = run_df[run_df[\"_step\"] >= .99 * run_df[\"train_iters\"]]\n",
    "df_run.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = load_dataset(\"nuprl/pass_k_with_MultiPL-E\", split=\"train\", use_auth_token=True, revision=\"bigcode_scaling_laws\")\n",
    "\n",
    "scores = scores.map(lambda x: {\"idx\": int(x[\"Experiment\"].split(\"idx_\")[1].split(\"-\")[0])})\n",
    "scores = scores.map(lambda x: {\"lang\": x[\"Experiment\"].split(\"humaneval-\")[1].split(\"-\")[0]})\n",
    "\n",
    "scores = scores.filter(lambda x: x[\"k\"]==100)\n",
    "scores = scores.filter(lambda x: x[\"lang\"]==lang)\n",
    "\n",
    "df_scores = scores.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PROJECT_DIR = \"/fsx-onellm/margaretli/env_srcs/xlf/xlformers_n/scaling/data\"\n",
    "\n",
    "col_names = ['C', 'D', 'N', 'lr', 'Avg Train Loss', 'Max Train Loss', 'C4 Eval PPL', 'Wiki Eval PPL', 'C4 Eval Loss', 'Wiki Eval Loss']\n",
    "# col_names = ['C', 'N', 'loss']\n",
    "\n",
    "def read_data(csv_file, loss_name='C4 Eval Loss', col_names=col_names):\n",
    "    mins_only = []\n",
    "    df = pd.read_csv(csv_file,)\n",
    "    df.dropna(subset=[loss_name], inplace=True, use_cols=col_names)\n",
    "\n",
    "    if 'lr' in df.columns:\n",
    "        df = df.loc[(df['lr'] >= 0)]\n",
    "    if 'D' not in df.columns:\n",
    "        df['D'] = df['C'] / (df['N'] * 6)\n",
    "\n",
    "    n_vals = df['N'].unique()\n",
    "    d_vals = sorted(df['D'].unique())\n",
    "    for n in n_vals:\n",
    "        for d in d_vals:\n",
    "            cd_df = df[(df['N'] == n) & (df['D'] == d)]\n",
    "            if cd_df.empty:\n",
    "                continue\n",
    "            min_index = cd_df[loss_name].idxmin()\n",
    "            mins_only.append(cd_df.loc[min_index])\n",
    "\n",
    "    mins_only_df = pd.DataFrame(mins_only)\n",
    "    print(mins_only_df)\n",
    "    return mins_only_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def scaling_law(N, D, params):\n",
    "    a, b, e, alpha, beta = params\n",
    "    A = np.exp(a)\n",
    "    B = np.exp(b)\n",
    "    E = np.exp(e)\n",
    "    \n",
    "    L = E + (A / (N**alpha)) + (B /(D**beta))\n",
    "    \n",
    "    return L\n",
    "\n",
    "def opt_N_D(C, G, opt_a, opt_b):\n",
    "    opt_N = G*(C/6)**opt_a\n",
    "    opt_D = (1/G)*(C/6)**opt_b\n",
    "    return opt_N, opt_D\n",
    "\n",
    "def loss(inp, params, loss_type='huber'):\n",
    "    a, b, e, alpha, beta = params[0], params[1], params[2], params[3], params[4]\n",
    "    pre_lse = torch.stack([a - alpha*torch.log(inp[:, 0]), b - beta*torch.log(inp[:, 1]), e.expand((inp.shape[0]))])\n",
    "    post_lse = torch.logsumexp(pre_lse, dim=0)\n",
    "    if loss_type == 'huber':\n",
    "        # huber_loss = torch.nn.functional.huber_loss(post_lse, torch.log(inp[:, 2]), delta=1e-3, reduction='none')\n",
    "        loss = torch.nn.functional.huber_loss(post_lse, torch.log(inp[:, 2]), delta=10, reduction='none')\n",
    "    elif loss_type == 'mse':\n",
    "        loss = torch.nn.functional.mse_loss(post_lse, torch.log(inp[:, 2]), reduction='none')\n",
    "    elif loss_type == 'l1':\n",
    "        loss = torch.nn.functional.l1_loss(post_lse, torch.log(inp[:, 2]), reduction='none')\n",
    "    elif loss_type == 'smooth_l1':\n",
    "        loss = torch.nn.functional.smooth_l1_loss(post_lse, torch.log(inp[:, 2]), reduction='none')\n",
    "    elif loss_type == 'kl':\n",
    "        loss = torch.nn.functional.kl_div(post_lse, torch.log(inp[:, 2]), reduction='none')\n",
    "    elif loss_type == 'cosine':\n",
    "        loss = torch.nn.functional.cosine_similarity(post_lse, torch.log(inp[:, 2]), reduction='none')\n",
    "    elif loss_type == 'poisson':\n",
    "        loss = torch.nn.functional.poisson_nll_loss(post_lse, torch.log(inp[:, 2]), reduction='none')\n",
    "    elif loss_type == 'nll':\n",
    "        loss = torch.nn.functional.nll_loss(post_lse, torch.log(inp[:, 2]), reduction='none')\n",
    "    \n",
    "    return loss.sum()\n",
    "\n",
    "def minimize_loss(inp, init_params=[6, 6, -1, 0.28, 0.32], steps=500, algorithm=\"lbfgs\", lr=1e-2):\n",
    "    params = torch.nn.Parameter(data=torch.Tensor(init_params))\n",
    "    \n",
    "    if algorithm == \"lbfgs\":\n",
    "        opt = torch.optim.LBFGS([params],\n",
    "                    lr=lr,\n",
    "                    history_size=10, \n",
    "                    max_iter=20, \n",
    "                    line_search_fn=\"strong_wolfe\")\n",
    "    elif algorithm == \"adam\":\n",
    "        opt = torch.optim.Adam([params], lr=lr)\n",
    "    elif algorithm == \"sgd\":    \n",
    "        opt = torch.optim.SGD([params], lr=lr)\n",
    "    elif algorithm == \"adagrad\":\n",
    "        opt = torch.optim.Adagrad([params], lr=lr)\n",
    "    elif algorithm == \"adadelta\":\n",
    "        opt = torch.optim.Adadelta([params], lr=lr)\n",
    "    elif algorithm == \"rmsprop\":\n",
    "        opt = torch.optim.RMSprop([params], lr=lr)\n",
    "    elif algorithm == \"rprop\":\n",
    "        opt = torch.optim.Rprop([params], lr=lr)\n",
    "    elif algorithm == \"adamw\":\n",
    "        opt = torch.optim.AdamW([params], lr=lr)\n",
    "    elif algorithm == \"sparseadam\":\n",
    "        opt = torch.optim.SparseAdam([params], lr=lr)\n",
    "    elif algorithm == \"adamax\":\n",
    "        opt = torch.optim.Adamax([params], lr=lr)\n",
    "    elif algorithm == \"asgd\":\n",
    "        opt = torch.optim.ASGD([params], lr=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid algorithm\")\n",
    "\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        l = loss(inp, params)\n",
    "        l.backward()\n",
    "        return l\n",
    "\n",
    "    for i in range(steps):\n",
    "        l = opt.step(closure)\n",
    "    return l, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Params  total steps      lr  Avg Train Loss  Max Train Loss  C4 Eval PPL  \\\n",
      "4       NaN          NaN  0.0040           4.957           5.185      146.194   \n",
      "12      NaN          NaN  0.0040           4.717           5.044      119.259   \n",
      "17      NaN          NaN  0.0040           4.453           4.659       89.148   \n",
      "21      NaN          NaN  0.0040           3.999           4.218       61.665   \n",
      "25      NaN          NaN  0.0040           3.736           4.058       46.984   \n",
      "..      ...          ...     ...             ...             ...          ...   \n",
      "268     NaN          NaN  0.0020           2.899           3.182       21.522   \n",
      "272     NaN          NaN  0.0020           2.864           3.263       20.629   \n",
      "276     NaN          NaN  0.0020           2.815           3.157       19.575   \n",
      "286     NaN          NaN  0.0010           2.725           3.060       17.837   \n",
      "295     NaN          NaN  0.0008           2.487           2.915       14.389   \n",
      "\n",
      "     Wiki Eval PPL  C4 Eval Loss  Wiki Eval Loss         Est C  ...  \\\n",
      "4          216.300         4.985           5.377  4.470000e+16  ...   \n",
      "12         166.031         4.781           5.112  5.590000e+16  ...   \n",
      "17         115.380         4.490           4.748  8.050000e+16  ...   \n",
      "21          70.939         4.122           4.262  1.120000e+17  ...   \n",
      "25          47.592         3.850           3.863  1.680000e+17  ...   \n",
      "..             ...           ...             ...           ...  ...   \n",
      "268         16.467         3.069           2.801  2.640000e+18  ...   \n",
      "272         15.535         3.027           2.743  3.170000e+18  ...   \n",
      "276         14.377         2.974           2.666  4.220000e+18  ...   \n",
      "286         12.987         2.881           2.564  7.560000e+18  ...   \n",
      "295          9.577         2.666           2.259  1.510000e+19  ...   \n",
      "\n",
      "     FFN FLOPs per layer-token  Body FLOPs  logits FLOPS  FLOPs / update  \\\n",
      "4                          NaN         NaN           NaN             NaN   \n",
      "12                         NaN         NaN           NaN             NaN   \n",
      "17                         NaN         NaN           NaN             NaN   \n",
      "21                         NaN         NaN           NaN             NaN   \n",
      "25                         NaN         NaN           NaN             NaN   \n",
      "..                         ...         ...           ...             ...   \n",
      "268                        NaN         NaN           NaN             NaN   \n",
      "272                        NaN         NaN           NaN             NaN   \n",
      "276                        NaN         NaN           NaN             NaN   \n",
      "286                        NaN         NaN           NaN             NaN   \n",
      "295                        NaN         NaN           NaN             NaN   \n",
      "\n",
      "     FLOPS / update w/ rema  % FLOPS Proj  % FLOPS FFN  % FLOPS logit  \\\n",
      "4                       NaN           NaN          NaN            NaN   \n",
      "12                      NaN           NaN          NaN            NaN   \n",
      "17                      NaN           NaN          NaN            NaN   \n",
      "21                      NaN           NaN          NaN            NaN   \n",
      "25                      NaN           NaN          NaN            NaN   \n",
      "..                      ...           ...          ...            ...   \n",
      "268                     NaN           NaN          NaN            NaN   \n",
      "272                     NaN           NaN          NaN            NaN   \n",
      "276                     NaN           NaN          NaN            NaN   \n",
      "286                     NaN           NaN          NaN            NaN   \n",
      "295                     NaN           NaN          NaN            NaN   \n",
      "\n",
      "     % FLOPS attn  updates per second  \n",
      "4             NaN                 NaN  \n",
      "12            NaN                 NaN  \n",
      "17            NaN                 NaN  \n",
      "21            NaN                 NaN  \n",
      "25            NaN                 NaN  \n",
      "..            ...                 ...  \n",
      "268           NaN                 NaN  \n",
      "272           NaN                 NaN  \n",
      "276           NaN                 NaN  \n",
      "286           NaN                 NaN  \n",
      "295           NaN                 NaN  \n",
      "\n",
      "[64 rows x 51 columns]\n",
      "Algorithm:  lbfgs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:34<00:00, 34.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0234, grad_fn=<SumBackward0>)\n",
      "[5.011801   9.981039   0.73496526 0.32206574 0.48285556]\n",
      "Algorithm:  adam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0129, grad_fn=<SumBackward0>)\n",
      "[ 3.445105  14.621374   0.7848881  0.2189644  0.7264511]\n",
      "Algorithm:  adagrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0230, grad_fn=<SumBackward0>)\n",
      "[ 4.892044   10.048826    0.7287354   0.3132959   0.48643562]\n",
      "Algorithm:  rmsprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0541, grad_fn=<SumBackward0>)\n",
      "[ 7.169803   12.82391     0.9486355   0.48422384  0.6281711 ]\n",
      "Algorithm:  rprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0116, grad_fn=<SumBackward0>)\n",
      "[ 4.262972   14.428007    0.8462173   0.27863997  0.7154747 ]\n",
      "Algorithm:  adamw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0299, grad_fn=<SumBackward0>)\n",
      "[ 2.2191088   8.97714    -0.42929128  0.09013393  0.429927  ]\n",
      "Algorithm:  adamax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0113, grad_fn=<SumBackward0>)\n",
      "[ 3.2709134  14.612422    0.76042837  0.20365712  0.7250983 ]\n",
      "Best Algorithm:  adamax\n",
      "Best Params:  [ 3.2709134  14.612422    0.76042837  0.20365712  0.7250983 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_name = \"C4 Eval Loss\"\n",
    "csv_file = \"/fsx-onellm/margaretli/env_srcs/xlf/xlformers_n/scaling/data/dense.csv\"\n",
    "fit_df = read_data(csv_file=csv_file, loss_name=loss_name)\n",
    "# loss_name = \"loss\"\n",
    "# csv_file = \"/fsx-onellm/margaretli/env_srcs/xlf/xlformers_n/scaling/data/epoch_ai.csv\"\n",
    "# fit_df = read_data(csv_file=csv_file, loss_name=loss_name, col_names=None)\n",
    "\n",
    "project_dir=DEFAULT_PROJECT_DIR\n",
    "\n",
    "# fit_df = fit_df[fit_df['N'] < 100000000]\n",
    "\n",
    "inp = torch.Tensor([[N, D, L] for N, D, L in \n",
    "                    zip(fit_df[\"N\"], fit_df[\"D\"], fit_df[loss_name])])\n",
    "inp.require_grad = True\n",
    "steps = 20000\n",
    "lr = 1e-2\n",
    "\n",
    "all_algorithms = [\"lbfgs\", \"adam\", \"adagrad\", \"rmsprop\", \"rprop\", \"adamw\", \"adamax\"]\n",
    "# all_algorithms = [\"lbfgs\", \"adam\", \"sgd\", \"adagrad\", \"adadelta\", \"rmsprop\", \"rprop\", \"adamw\", \"adamax\", \"asgd\"]\n",
    "all_best_params = {}\n",
    "\n",
    "min_all_loss = 1e10\n",
    "best_algorithm = ''\n",
    "\n",
    "for algorithm in all_algorithms:\n",
    "    print(\"Algorithm: \", algorithm)\n",
    "\n",
    "    min_loss = 1e10\n",
    "    # for a in tqdm(np.linspace(0, 10, 5)):\n",
    "    #     for b in np.linspace(0, 10, 5):\n",
    "    #         for e in np.linspace(-1, 2, 4):\n",
    "    #             for alpha in np.linspace(0, 1, 4):\n",
    "    #                 for beta in np.linspace(0, 1, 4):\n",
    "    #                     l, params = minimize_loss(inp, [a, b, e, alpha, beta], algorithm=algorithm, steps=steps, lr=lr)\n",
    "    #                     if l < min_loss:\n",
    "    #                         min_loss = l\n",
    "    #                         best_params = params.detach().numpy()\n",
    "\n",
    "    for a in tqdm([5]):\n",
    "        for b in [10]:\n",
    "            for e in [0.5]:\n",
    "                for alpha in [0.5]:\n",
    "                    for beta in [0.5]:\n",
    "                        l, params = minimize_loss(inp, [a, b, e, alpha, beta], algorithm=algorithm, steps=steps, lr=lr)\n",
    "                        if l < min_loss:\n",
    "                            min_loss = l\n",
    "                            best_params = params.detach().numpy()\n",
    "\n",
    "\n",
    "    print(min_loss)\n",
    "    print(best_params)\n",
    "    all_best_params[algorithm] = best_params\n",
    "    if min_loss < min_all_loss:\n",
    "        min_all_loss = min_loss\n",
    "        best_algorithm = algorithm\n",
    "    \n",
    "print(\"Best Algorithm: \", best_algorithm)\n",
    "print(\"Best Params: \", all_best_params[best_algorithm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm:  lbfgs\n",
      "Scaling:          compute parameters (B) tokens (B)    ratio\n",
      "0  1.250000e+18           0.01      20.61  2039.66\n",
      "1  5.010000e+18           0.02      38.04  1733.26\n",
      "2  1.980000e+19           0.05      69.78  1475.32\n",
      "3  1.000000e+21           0.42     394.01   931.48\n",
      "4  1.000000e+23           5.54    3007.90   542.85\n",
      "Algorithm:  adam\n",
      "Scaling:          compute parameters (B) tokens (B)   ratio\n",
      "0  1.250000e+18           0.02      13.61  888.80\n",
      "1  5.010000e+18           0.04      22.69  616.45\n",
      "2  1.980000e+19           0.09      37.63  429.14\n",
      "3  1.000000e+21           1.04     159.50  152.64\n",
      "4  1.000000e+23          19.17     869.36   45.35\n",
      "Algorithm:  adagrad\n",
      "Scaling:          compute parameters (B) tokens (B)    ratio\n",
      "0  1.250000e+18           0.01      20.51  2020.00\n",
      "1  5.010000e+18           0.02      37.29  1665.11\n",
      "2  1.980000e+19           0.05      67.37  1375.25\n",
      "3  1.000000e+21           0.46     364.41   796.76\n",
      "4  1.000000e+23           6.30    2644.94   419.74\n",
      "Algorithm:  rmsprop\n",
      "Scaling:          compute parameters (B) tokens (B)    ratio\n",
      "0  1.250000e+18           0.01      15.15  1102.08\n",
      "1  5.010000e+18           0.03      29.24  1023.96\n",
      "2  1.980000e+19           0.06      56.05   952.08\n",
      "3  1.000000e+21           0.46     359.05   773.50\n",
      "4  1.000000e+23           5.24    3178.28   606.09\n",
      "Algorithm:  rprop\n",
      "Scaling:          compute parameters (B) tokens (B)    ratio\n",
      "0  1.250000e+18           0.01      15.19  1107.92\n",
      "1  5.010000e+18           0.03      27.46   902.97\n",
      "2  1.980000e+19           0.07      49.33   737.46\n",
      "3  1.000000e+21           0.63     262.61   413.78\n",
      "4  1.000000e+23           8.91    1870.56   209.94\n",
      "Algorithm:  adamw\n",
      "Scaling:          compute parameters (B) tokens (B)        ratio\n",
      "0  1.250000e+18           0.00     919.36   4057033.50\n",
      "1  5.010000e+18           0.00    2149.57   5533711.46\n",
      "2  1.980000e+19           0.00    4982.95   7524202.57\n",
      "3  1.000000e+21           0.00   54900.67  18084527.12\n",
      "4  1.000000e+23           0.02  918684.68  50638970.22\n",
      "Algorithm:  adamax\n",
      "Scaling:          compute parameters (B) tokens (B)   ratio\n",
      "0  1.250000e+18           0.01      14.02  943.27\n",
      "1  5.010000e+18           0.04      23.11  639.64\n",
      "2  1.980000e+19           0.09      37.91  435.45\n",
      "3  1.000000e+21           1.07     155.63  145.32\n",
      "4  1.000000e+23          20.40     817.12   40.06\n"
     ]
    }
   ],
   "source": [
    "# all_best_params = {\n",
    "#     \"lbfgs\": [6.291975, 12.452576, 0.90624857, 0.45291403, 0.60690635],\n",
    "#     \"adam\": [10.057474, 11.168409, 0.8845671, 0.7009503, 0.5374865],\n",
    "#     \"sgd\": [-44.88375, -8.306413, 41.089928, 867.93774, 228.67094],\n",
    "#     \"adagrad\": [7.6266613, 10.241391, 0.82940316, 0.556825, 0.48704857],\n",
    "#     \"adadelta\": [0.31501442, 10.162561, 0.13301994, 0.01270814, 0.47626096],\n",
    "#     \"rmsprop\": [ 0.3898864, 10.235114, 0.95406806, 0.61034465, 0.4942382],\n",
    "#     \"rprop\": [5.42916, 10.889063, 0.83016324, 0.39976725, 0.5223741],\n",
    "#     \"adamw\": [1.6131105, 6.284966, 0.02174136, 0.13462491, 0.2669011],\n",
    "#     \"adamax\": [9.979901, 10.904844, 0.870543, 0.69669473, 0.52316064],\n",
    "#     \"asgd\": [-1.2160281e-01, -1.3072298e+01, 4.0904160e+01, 2.1169136e+00, 3.7313159e+02],\n",
    "# }\n",
    "\n",
    "for algorithm, best_params in all_best_params.items():\n",
    "    opt_alpha = best_params[-2]\n",
    "    opt_beta = best_params[-1]\n",
    "\n",
    "    opt_a =  opt_beta / (opt_alpha+opt_beta)\n",
    "    opt_b =  opt_alpha / (opt_alpha+opt_beta)\n",
    "\n",
    "    A = np.exp(best_params[0])\n",
    "    B = np.exp(best_params[1])\n",
    "    G = ((opt_alpha*A)/(opt_beta*B))**(1/(opt_alpha+opt_beta))\n",
    "\n",
    "    scaling = []\n",
    "\n",
    "    for C in [1.25E+18, 5.01E+18, 1.98E+19, 1E21, 1E23]:\n",
    "        N, D = opt_N_D(C, G, opt_a, opt_b)\n",
    "        scaling.append(\n",
    "            {\"compute\": f\"{C:e}\",\n",
    "            \"parameters (B)\": f\"{N/1e9:.2f}\",\n",
    "            \"tokens (B)\": f\"{D/1e9:.2f}\",\n",
    "            \"ratio\": f\"{D/N:.2f}\",\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(\"Algorithm: \", algorithm)\n",
    "    print(\"Scaling: \", pd.DataFrame(scaling))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compute</th>\n",
       "      <th>parameters (B)</th>\n",
       "      <th>tokens (B)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.920000e+19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>18.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.210000e+20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>20.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.230000e+22</td>\n",
       "      <td>77.25</td>\n",
       "      <td>26.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.760000e+23</td>\n",
       "      <td>2883.82</td>\n",
       "      <td>33.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.850000e+24</td>\n",
       "      <td>17234.12</td>\n",
       "      <td>37.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.900000e+24</td>\n",
       "      <td>41917.26</td>\n",
       "      <td>39.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.430000e+25</td>\n",
       "      <td>134974.20</td>\n",
       "      <td>42.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.270000e+26</td>\n",
       "      <td>462657.10</td>\n",
       "      <td>45.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.300000e+28</td>\n",
       "      <td>36053325.81</td>\n",
       "      <td>60.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        compute parameters (B) tokens (B)\n",
       "0  1.920000e+19           0.18      18.13\n",
       "1  1.210000e+20           1.00      20.21\n",
       "2  1.230000e+22          77.25      26.54\n",
       "3  5.760000e+23        2883.82      33.29\n",
       "4  3.850000e+24       17234.12      37.23\n",
       "5  9.900000e+24       41917.26      39.36\n",
       "6  3.430000e+25      134974.20      42.35\n",
       "7  1.270000e+26      462657.10      45.75\n",
       "8  1.300000e+28    36053325.81      60.10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaling = []\n",
    "\n",
    "for C in [1.92e19, 1.21e20, 1.23e22, 5.76e23, 3.85e24, 9.90e24, 3.43e25, 1.27e26, 1.30e28]:\n",
    "    N, D = opt_N_D(C, G, opt_a, opt_b)\n",
    "    scaling.append(\n",
    "        {\"compute\": f\"{C:e}\",\n",
    "         \"parameters (B)\": f\"{N/1e9:.2f}\",\n",
    "         \"tokens (B)\": f\"{D/1e9:.2f}\",\n",
    "        }\n",
    "    )\n",
    "     \n",
    "pd.DataFrame(scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2020082572609923"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scaling_law(1e10, 100e9, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'N'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3696810/819571701.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# color_map={\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#            \"3e+18\": \"orange\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#            \"6e+18\": \"black\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;31m#            \"3e+18\": \"brown\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m#            \"1e+19\": \"green\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#            \"3e+19\": \"purple\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#            \"6e+19\": \"red\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/fsx-onellm/margaretli/miniforge3/envs/mlexp/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_format_argument_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallow_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/fsx-onellm/margaretli/miniforge3/envs/mlexp/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6909\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6911\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6912\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6914\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6915\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/fsx-onellm/margaretli/miniforge3/envs/mlexp/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m             )\n\u001b[1;32m   1849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'N'"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "color_map={\n",
    "           \"3e+18\": \"orange\",\n",
    "           \"6e+18\": \"black\",\n",
    "           \"3e+18\": \"brown\",\n",
    "           \"1e+19\": \"green\",\n",
    "           \"3e+19\": \"purple\",\n",
    "           \"6e+19\": \"red\",\n",
    "           \"1e+20\": \"blue\",\n",
    "           \"3e+20\": \"pink\",\n",
    "           \"6e+20\": \"gold\",\n",
    "           \"1e+21\": \"silver\",\n",
    "          }\n",
    "fit_df.head(4)\n",
    "\n",
    "# fit_df['C'] = fit_df['C'].astype(str)\n",
    "fig = px.scatter(fit_df, x='N', y=loss_name, color='C', \n",
    "                 log_x=True, color_discrete_map=color_map)\n",
    "\n",
    "for compute in color_map.keys():\n",
    "    tmp_df = fit_df[fit_df['C'] == compute]\n",
    "    df_d = list() \n",
    "    for _, row in tmp_df.iterrows():\n",
    "        pred = scaling_law(row['N'], row['D'], best_params)\n",
    "        df_d.append({'prediction': pred, 'N': row['N'], 'D': row['D']})\n",
    "    fig2 = px.line(pd.DataFrame(df_d).sort_values('N'), \n",
    "                   x='N', y='prediction', log_x=True)\n",
    "    fig2.update_traces(line_color=color_map[compute], line_width=2)\n",
    "    fig = go.Figure(data=fig.data + fig2.data)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
