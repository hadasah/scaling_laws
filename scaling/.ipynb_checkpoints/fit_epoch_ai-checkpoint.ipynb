{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import CLD_CONTINUED\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.optimize import basinhopping\n",
    "from sklearn.metrics._plot.confusion_matrix import unique_labels\n",
    "import scipy.interpolate\n",
    "\n",
    "from scipy.optimize import OptimizeWarning\n",
    "np.seterr(over='ignore')\n",
    "np.seterr(invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=OptimizeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, hparams, warmup, decay, param_count, val = config\n",
    "# from https://github.com/formll/resolving-scaling-law-discrepancies\n",
    "\n",
    "FIGURE1_CONFIGS = [\n",
    "    ('rw', 'base', 'long', 'kaplan', 'kaplan', 'train'),\n",
    "    ('rw', 'base', 'long', 'kaplan', 'standard', 'val'),\n",
    "    ('rw', 'base', 'short', 'kaplan', 'standard', 'val'),\n",
    "    ('rw', 'base', 'short', 'chinchilla', 'standard', 'val'),\n",
    "    ('rw', 'tuned', 'short', 'const', 'standard', 'val')\n",
    "]\n",
    "\n",
    "CONFIG_DICT_LABEL = {\n",
    "    ('rw', 'base', 'long', 'kaplan', 'kaplan', 'train'): 'Reproducing Kaplan et al.',\n",
    "    ('rw', 'base', 'long', 'kaplan', 'standard', 'val'): 'Counting last layer FLOPs',\n",
    "    ('rw', 'base', 'short', 'kaplan', 'standard', 'val'): 'Correcting warmup',\n",
    "    ('rw', 'base', 'short', 'chinchilla', 'standard', 'val'): 'Cosine decay', # original Chinchilla?\n",
    "    ('rw', 'tuned', 'short', 'const', 'standard', 'val'): 'Optimizer tuning (no decay)',\n",
    "    ('rw', 'tuned', 'short', 'const', 'standard', 'train'): 'Optimizer tuning (no decay) - train',\n",
    "    ('rw', 'tuned', 'long', 'const', 'kaplan', 'train'): '', # kaplan tuned\n",
    "    ('rw', 'base', 'long', 'kaplan', 'attention', 'train'): 'Counting last layer\\nand attention FLOPs',\n",
    "    ('rw', 'base', 'short', 'kaplan', 'attention', 'train'): 'Correcting warmup',\n",
    "    ('rw', 'base', 'short', 'chinchilla', 'attention', 'train'): 'Cosine decay',\n",
    "    ('rw', 'tuned', 'short', 'const', 'attention', 'train'): 'Optimizer tuning (no decay)',\n",
    "}\n",
    "\n",
    "ISOFLOP_ARGS = {\n",
    "    ('kaplan', 'train'): dict(loss_key='train/loss_smoothed', flop_per_token_key='flops_per_token_no_att_no_embed', n_key='params_no_embed'),\n",
    "    ('standard', 'val'):  dict(loss_key='val/loss', flop_per_token_key='flops_per_token', n_key='params'),\n",
    "    ('standard', 'train'): dict(loss_key='train/loss_smoothed', flop_per_token_key='flops_per_token', n_key='params'),\n",
    "    ('attention', 'train'): dict(loss_key='train/loss_smoothed', flop_per_token_key='flops_per_token_att', n_key='eff_params_att'),\n",
    "}\n",
    "\n",
    "# LABEL_TO_CONFIG_DICT = {\n",
    "#     n: c for c, n in CONFIG_DICT_LABEL.items()\n",
    "# }\n",
    "\n",
    "def fetch_flop(df, flop, loss_key='train/loss_smoothed', warmup_remove_factor=1e-12, n_key='params', \n",
    "               seq_len=2048, bs_key='bs', keep_bs_lr_keys=False,\n",
    "               flop_per_token_key='flops_per_token', flop_tolerance=0.1):\n",
    "    out = []\n",
    "    for _, row in df.iterrows():\n",
    "        if len(row[loss_key]) == 0:\n",
    "            continue\n",
    "        loss_vals = row[loss_key].dropna().groupby(level=0).mean().sort_index()\n",
    "        step_vals = loss_vals.index\n",
    "        mask = step_vals >= ((warmup_remove_factor * row.warmup_tokens) / row.bs / row.seq_len)\n",
    "        loss_vals = loss_vals[mask]\n",
    "        loss_vals.index = loss_vals.index.astype(float) * seq_len * row[bs_key] * row[flop_per_token_key]\n",
    "        flop_vals = loss_vals.index\n",
    "        \n",
    "        if len(loss_vals) == 0:\n",
    "            continue        \n",
    "        flop_ind = loss_vals.index.searchsorted(flop)\n",
    "        if flop_ind > 0:\n",
    "            flop_ind += -1 + np.abs(np.log(flop_vals[flop_ind-1:flop_ind+1]/flop)).argmin()\n",
    "        rel_err = np.exp(np.abs(np.log(flop_vals[flop_ind]/flop))) - 1\n",
    "        if rel_err > flop_tolerance:\n",
    "            continue\n",
    "\n",
    "        if len(flop_vals) > 1:\n",
    "            flop_slice = flop_vals[max(0,flop_ind-5):flop_ind+5]\n",
    "            loss_slide = loss_vals.iloc[max(0,flop_ind-5):flop_ind+5]\n",
    "            loss_interp = np.exp(np.interp(np.log(flop), np.log(flop_slice), np.log(loss_slide)))\n",
    "            out.append(dict(n=row[n_key], t=flop / row[flop_per_token_key], loss=loss_interp))\n",
    "        else:\n",
    "            out.append(dict(n=row[n_key], t=loss_vals.index[flop_ind] / row[flop_per_token_key], loss=loss_vals.iloc[flop_ind]))\n",
    "        if keep_bs_lr_keys:\n",
    "            out[-1].update({k: row[k] for k in [bs_key, 'lr']})\n",
    "\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "# def power_law_fit(df, x, y, weighted=False):\n",
    "#     if isinstance(y, (list, tuple)):\n",
    "#         out = {}\n",
    "#         for yy in y:\n",
    "#             if 'loss' not in yy:\n",
    "#                 out.update(power_law_fit(df, x, yy, weighted=weighted))\n",
    "#             else:\n",
    "#                 df = df.copy()\n",
    "#                 out.update(fit_loss_with_saturation(df, weighted=weighted))\n",
    "#         return out\n",
    "#     else:\n",
    "#         X_data = np.log(df.dropna()[x].values).reshape(-1, 1)\n",
    "#         y_data = np.log(df.dropna()[y].values)\n",
    "#         std_key = f'{y}_star_std'\n",
    "#         if weighted and std_key in df.columns:\n",
    "#             y_data_std = df.dropna()[std_key].values\n",
    "#             w = 1 / y_data_std ** 2\n",
    "#         else:\n",
    "#             w = None\n",
    "\n",
    "#         clf = LinearRegression().fit(X_data, y_data, sample_weight=w)\n",
    "#         return {f'{y}_exponent': clf.coef_.item(),\n",
    "#                 f'{y}_coef': np.exp(clf.intercept_),\n",
    "#                 f'{y}_r2': clf.score(X_data, y_data)}\n",
    "\n",
    "\n",
    "# def fit_compute_optimal_power_laws(optimal_pairs, bootstrap_data, bootstrap_num=None, bootstrap_num_loss=200, fit_loss=True):\n",
    "#     keys_to_fit = ['n', 't', 'multiplier']\n",
    "#     if fit_loss:\n",
    "#         keys_to_fit.append('loss')\n",
    "#     out = {'basic': power_law_fit(optimal_pairs.reset_index(), 'flops', keys_to_fit),\n",
    "#            'weighted': power_law_fit(optimal_pairs.reset_index(), 'flops', keys_to_fit, weighted=True)}\n",
    "#     bootstrap_samples = bootstrap_data.dropna().set_index('flops')[\n",
    "#         ['n_stars', 't_stars', 'multiplier_stars', 'loss_stars', 'n_star_std', 't_star_std', 'loss_star_std']].rename(\n",
    "#         columns=lambda x: x.replace('_stars', ''))\n",
    "#     if bootstrap_num is None:\n",
    "#         bootstrap_num = bootstrap_samples[['n', 't', 'multiplier']].applymap(len).min().min()\n",
    "\n",
    "#     for name, is_weighted in dict(bootstrap=False, bootstrap_weighted=True).items():\n",
    "#         bs_smaples_arr = [\n",
    "#             power_law_fit(bootstrap_samples.applymap(lambda x: maybe_get_item(x, i)).reset_index(),\n",
    "#             'flops', ['loss'], weighted=is_weighted)\n",
    "#             for i in range(bootstrap_num_loss)\n",
    "#             ] if fit_loss else []\n",
    "#         bs_smaples_arr.extend([power_law_fit(\n",
    "#             bootstrap_samples.applymap(lambda x: maybe_get_item(x, i)).reset_index(),\n",
    "#             'flops', ['n', 't', 'multiplier'], weighted=is_weighted)\n",
    "#             for i in range(bootstrap_num)])\n",
    "#         out[name] = bs_smaples_arr\n",
    "#     bootstrap_medians = bootstrap_samples.applymap(np.median)\n",
    "#     out.update({\n",
    "#         'bs_median': power_law_fit(bootstrap_medians.reset_index(), 'flops', keys_to_fit),\n",
    "#         'bs_median_weighted': power_law_fit(bootstrap_medians.reset_index(), 'flops', keys_to_fit, weighted=True)})\n",
    "#     return out\n",
    "\n",
    "\n",
    "def get_noise_for_loss(loss, bootstrap_iters, noise_low=0.005, noise_high=0.1, l_threshold_high=6, l_threshold_low=3):\n",
    "    basic_noise = np.random.normal(0, 1, (bootstrap_iters, len(loss) // bootstrap_iters))\n",
    "    noise_adjusted_losses = np.zeros((bootstrap_iters, len(loss) // bootstrap_iters))\n",
    "\n",
    "    for i in range(len(loss) // bootstrap_iters):\n",
    "        if np.log(loss[i]) >= l_threshold_high:\n",
    "            log_noise = np.log(noise_high)\n",
    "        elif np.log(loss[i]) <= l_threshold_low:\n",
    "            log_noise = np.log(noise_low)\n",
    "        else:\n",
    "            log_noise = np.interp(np.log(loss[i]), [np.log(l_threshold_low), np.log(l_threshold_high)], [np.log(noise_low), np.log(noise_high)])\n",
    "        noise_factor = np.exp(log_noise)\n",
    "        noise_adjusted_losses[:, i] = loss[i] + noise_factor * basic_noise[:, i]\n",
    "        \n",
    "    return noise_adjusted_losses.flatten()\n",
    "\n",
    "\n",
    "def vectorized_interp_with_seed_noise(df, n_interp_, bootstrap_iters, seed_noise=None,\n",
    "                                      min_std_factor=0.33, tok_or_n='n'):\n",
    "    if seed_noise is None:\n",
    "        seed_noise = {}\n",
    "    interp_num = len(n_interp_)\n",
    "    stacked_df = pd.concat([df] * bootstrap_iters).reset_index(drop=True)\n",
    "    stacked_df['loss'] = get_noise_for_loss(stacked_df.loss, bootstrap_iters=bootstrap_iters, **seed_noise)\n",
    "\n",
    "    batch_ids = np.repeat(np.arange(bootstrap_iters), len(df))\n",
    "    stacked_df['batch_id'] = batch_ids\n",
    "    stacked_df.sort_values(by=['batch_id', tok_or_n], inplace=True)\n",
    "\n",
    "    def batch_interp(batch):\n",
    "        interp = scipy.interpolate.Akima1DInterpolator(np.log(batch[tok_or_n]), np.log(batch['loss']))\n",
    "        return np.exp(interp(np.log(n_interp_)))\n",
    "\n",
    "    interpolated_values = stacked_df.groupby('batch_id').apply(batch_interp)\n",
    "\n",
    "    # Find the index of the minimum interpolated loss value per batch\n",
    "    min_indices = interpolated_values.apply(np.argmin)\n",
    "    results = [n_interp_[idx] if idx != 0 and idx != interp_num - 1 else None for idx in min_indices]\n",
    "    valid_results_loss = [interpolated_values[i][idx] for i, idx in enumerate(min_indices) if idx != 0 and idx != interp_num - 1]\n",
    "    # Filter None values and calculate statistics\n",
    "    valid_results = [result for result in results if result is not None]\n",
    "    if len(valid_results) < bootstrap_iters // 2:\n",
    "        return None, 0, None, None, None\n",
    "    else:\n",
    "        n_star_std_ = np.std(np.log(valid_results))\n",
    "        min_std = min_std_factor * np.log(n_interp_[1] / n_interp_[0])  # this assumes a roughly uniform grid\n",
    "        n_star_std_ = max(n_star_std_, min_std) * (bootstrap_iters / len(valid_results))\n",
    "        loss_star_std_ = np.std(np.log(valid_results_loss))\n",
    "        min_std_loss = min_std_factor * min([np.log(df.loss.iloc[i+1] / df.loss.iloc[i]) for i in range(len(df) - 1)])\n",
    "        loss_star_std_ = max(loss_star_std_, min_std_loss) * (bootstrap_iters / len(valid_results_loss))\n",
    "        return n_star_std_, None, valid_results, valid_results_loss, loss_star_std_\n",
    "\n",
    "\n",
    "def interpolation(df_, interp_num, bootstrap_iters, seed_noise, min_std_factor, interp_num_multiplier, std_method, col):\n",
    "    interp_ = np.geomspace(df_[col].min(), df_[col].max(), interp_num)\n",
    "    df_ = df_.sort_values(col)\n",
    "    interpolator = scipy.interpolate.Akima1DInterpolator(np.log(df_[col]), np.log(df_.loss))\n",
    "    loss_interp_ = np.exp(interpolator(np.log(interp_)))\n",
    "    star_ind_ = loss_interp_.argmin()\n",
    "\n",
    "    if std_method == 'add_seed_noise':\n",
    "        star_std_, _, noised_stars_, noised_loss, loss_star_std = vectorized_interp_with_seed_noise(\n",
    "            df_, interp_, bootstrap_iters, seed_noise, min_std_factor * interp_num_multiplier, tok_or_n=col)\n",
    "    else:\n",
    "        star_std_ = None\n",
    "        noised_stars_ = []\n",
    "\n",
    "    return star_ind_, star_std_, noised_stars_, interp_, loss_interp_, noised_loss, loss_star_std\n",
    "\n",
    "\n",
    "def interp_flop(big_df, loss_key, flop_vals=[8e16, 3e17, 6e17, 3e18, 6e18, 1e19], groupby_action='min',\n",
    "                warmup_remove_factor=1e-12,\n",
    "                interp_num_multiplier=25,\n",
    "                n_key='params', n_star_std_method='add_seed_noise', t_star_std_method='add_seed_noise',\n",
    "                bootstrap_iters=1000,\n",
    "                min_std_factor=0.33,\n",
    "                seed_noise=None, flop_tolerance=0.1,\n",
    "                flop_per_token_key='flops_per_token',\n",
    "                bs_median_as_obs=True,\n",
    "                keep_bs_lr_keys=False,\n",
    "                ):\n",
    "    out = []\n",
    "    optimal_pairs = []\n",
    "    max_loss, min_loss = 0, 1e12\n",
    "\n",
    "    for c in flop_vals:\n",
    "        df_ = fetch_flop(big_df, c, loss_key=loss_key, \n",
    "                         warmup_remove_factor=warmup_remove_factor, n_key=n_key, \n",
    "                         flop_per_token_key=flop_per_token_key,\n",
    "                         flop_tolerance=flop_tolerance, keep_bs_lr_keys=keep_bs_lr_keys)\n",
    "\n",
    "        if len(df_) < 3:\n",
    "            out.append(dict(n_interp=None, loss_interp=None, t_interp=None, \n",
    "                            loss_interp_tok=None, opt_ind=None, opt_tok_ind=None, flops=c))\n",
    "            continue\n",
    "        if 'bs' in df_.columns and 'lr' in df_.columns:\n",
    "            df_sweep_opt_eta = df_.groupby(['n','bs']).apply(minimize_with_interp).drop(['bs', 'n'], axis=1).reset_index()\n",
    "            df_sweep_opt_eta_and_bs = df_sweep_opt_eta.groupby(['n']).apply(lambda x: minimize_with_interp(x, x_key='bs')).drop('n', axis=1).reset_index()\n",
    "            df_ = df_sweep_opt_eta_and_bs[['n']]\n",
    "            df_['t'] \n",
    "            print(df_.iloc[0].loss)\n",
    "        elif groupby_action == 'min':\n",
    "            df_ = df_.loc[df_.groupby(['n']).loss.idxmin()]\n",
    "        elif groupby_action == 'mean':\n",
    "            df_ = df_.groupby('n').mean()\n",
    "        else:\n",
    "            raise ValueError(f'Unknown groupby_action {groupby_action}')\n",
    "        df_ = df_.reset_index()\n",
    "\n",
    "        interp_num = (len(df_) - 1) * interp_num_multiplier\n",
    "\n",
    "        max_loss, min_loss = max(max_loss, df_.loss.max()), min(min_loss, df_.loss.min())\n",
    "        \n",
    "        n_star_ind_, n_star_std_, noised_n_stars_, n_interp_, loss_interp_, noised_loss, loss_star_std = interpolation(\n",
    "            df_, interp_num, bootstrap_iters, seed_noise, min_std_factor, interp_num_multiplier, n_star_std_method, 'n')\n",
    "\n",
    "        t_star_ind_, t_star_std_, noised_t_stars_, t_interp_, loss_interp_tok_, noised_loss, _ = interpolation(\n",
    "            df_, interp_num, bootstrap_iters, seed_noise, min_std_factor, interp_num_multiplier, t_star_std_method, 't')\n",
    "        \n",
    "        if n_star_ind_ != 0 and n_star_ind_ != interp_num -1 and noised_n_stars_ is not None:\n",
    "            optimal_pairs.append(\n",
    "                dict(flops=c, n=n_interp_[n_star_ind_], t=t_interp_[t_star_ind_], multiplier=c / 6 / (n_interp_[n_star_ind_]**2),\n",
    "                     loss=loss_interp_.min(), loss_t=loss_interp_tok_.min(),\n",
    "                     n_vals=df_.n.values, t_vals=df_.t.values, loss_vals=df_.loss\n",
    "                    )\n",
    "            )\n",
    "        else:\n",
    "            optimal_pairs.append(\n",
    "                dict(flops=c, n=None, t=None, loss=None, loss_t=None,\n",
    "                        n_vals=df_.n.values, t_vals=df_.t.values, loss_vals=df_.loss\n",
    "                    )\n",
    "            )\n",
    "        out.append(\n",
    "            dict(n_interp=n_interp_, loss_interp=loss_interp_, \n",
    "                 t_interp=t_interp_, loss_interp_tok=loss_interp_tok_, \n",
    "                 opt_ind=n_star_ind_, opt_tok_ind=t_star_ind_, flops=c, \n",
    "                 orig_n=df_.n, orig_t=df_.t, orig_loss=df_.loss)\n",
    "            )\n",
    "        if n_star_std_method == 'add_seed_noise':\n",
    "            out[-1]['n_star_std'] = n_star_std_\n",
    "            out[-1]['n_stars'] = noised_n_stars_\n",
    "            out[-1]['multiplier_stars'] = (c / (6 * np.array(noised_n_stars_)**2)) if noised_n_stars_ is not None else None\n",
    "            optimal_pairs[-1]['n_star_std'] = n_star_std_ \n",
    "            \n",
    "            out[-1]['multiplier_star_std'] = 2 * n_star_std_ if n_star_std_ is not None else None\n",
    "            optimal_pairs[-1]['multiplier_star_std'] = 2 * n_star_std_ if n_star_std_ is not None else None\n",
    "\n",
    "            out[-1]['t_star_std'] = t_star_std_\n",
    "            out[-1]['t_stars'] = noised_t_stars_\n",
    "            optimal_pairs[-1]['t_star_std'] = t_star_std_\n",
    "\n",
    "            out[-1]['loss_stars'] = noised_loss\n",
    "            out[-1]['loss_star_std'] = loss_star_std \n",
    "            optimal_pairs[-1]['loss_star_std'] = loss_star_std\n",
    "\n",
    "    out_df = pd.DataFrame(out)\n",
    "    optimal_pairs_df = pd.DataFrame(optimal_pairs)\n",
    "\n",
    "    if bs_median_as_obs:\n",
    "        for ind, row in optimal_pairs_df.iterrows():\n",
    "            if row['n'] is None or np.isnan(row['n']):\n",
    "                continue\n",
    "            flop = row['flops']\n",
    "            data_row = out_df.set_index('flops').loc[flop]\n",
    "            for key in ['n', 't', 'multiplier', 'loss']:\n",
    "                optimal_pairs_df.at[ind, key] = np.median(data_row[key + '_stars']) if data_row[key + '_stars'] is not None else row[key]\n",
    "\n",
    "    return out_df, optimal_pairs_df, max_loss, min_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_rsld_data(config_number=0):\n",
    "    big_df = pd.read_pickle('/fsx-onellm/margaretli/code/scaling_tomer/resolving-scaling-law-discrepancies/data/experiment_results.pickle.xz', compression='xz')\n",
    "    big_df = process_big_df(big_df.copy())\n",
    "    config = FIGURE1_CONFIGS[config_number]\n",
    "    dataset, hparams, warmup, decay, param_count, val = config\n",
    "    df = big_df.query(f\"dataset=='{dataset}' and hparams=='{hparams}' and warmup=='{warmup}' and decay=='{decay}'\")\n",
    "    \n",
    "    for loss_key in ['val/loss', 'train/loss']:\n",
    "        for ind, row in df.iterrows():\n",
    "            row[f\"{loss_key}\"].index = row[f\"{loss_key}\"].index.astype(float) * row['seq_len'] * row['bs']\n",
    "            df.loc[ind, f'last_{loss_key}_C'] = row[loss_key].index[-1] * row['flops_per_token']\n",
    "            df.loc[ind, f'last_{loss_key}_D'] = row[loss_key].index[-1] \n",
    "            df.loc[ind, f'last_{loss_key}'] = row[loss_key].iloc[-1]\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rsld_data_interp(config_number=0,\n",
    "                          flop_vals=None,\n",
    "                          seed=42, seed_noise_args=None, \n",
    "                          keep_bs_lr_keys=False\n",
    "                          ):\n",
    "    big_df = pd.read_pickle('/fsx-onellm/margaretli/code/scaling_tomer/resolving-scaling-law-discrepancies/data/experiment_results.pickle.xz', compression='xz')\n",
    "    big_df = process_big_df(big_df.copy())\n",
    "    config = FIGURE1_CONFIGS[config_number]\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if flop_vals is None:\n",
    "        flop_vals = FLOP_VALS\n",
    "    if seed_noise_args is None:\n",
    "        seed_noise_args = SEED_ARGS\n",
    "    df = big_df.copy()\n",
    "    out = []\n",
    "    dataset, hparams, warmup, decay, param_count, val = config\n",
    "    show_df = df.query(f\"dataset=='{dataset}' and hparams=='{hparams}' and warmup=='{warmup}' and decay=='{decay}'\")\n",
    "\n",
    "    if len(show_df) == 0:\n",
    "        continue\n",
    "    data, optimal_pairs, max_loss, min_loss = interp_flop(\n",
    "        show_df, seed_noise = seed_noise_args[config], \n",
    "        flop_vals=flop_vals, **ISOFLOP_ARGS[config[-2:]],\n",
    "        keep_bs_lr_keys=keep_bs_lr_keys,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # if len(show_df) == 0:\n",
    "    #     continue\n",
    "    \n",
    "    # data, optimal_pairs, max_loss, min_loss = interp_flop(\n",
    "    #     show_df, seed_noise = seed_noise_args[config], \n",
    "    #     flop_vals=flop_vals, **ISOFLOP_ARGS[config[-2:]],\n",
    "    #     keep_bs_lr_keys=keep_bs_lr_keys,\n",
    "    # )\n",
    "\n",
    "    # fit_results = fit_compute_optimal_power_laws(optimal_pairs, data, fit_loss=True)\n",
    "\n",
    "#         out.append(dict(dataset=dataset, hparams=hparams, warmup=warmup, decay=decay, param_count=param_count, val=val, \n",
    "#                         optimal_pairs=optimal_pairs, fit_results=fit_results,\n",
    "#                         data=data, max_loss=max_loss, min_loss=min_loss,))\n",
    "# return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scaling_law_chinch(N, D, params):\n",
    "    if len(params) == 6:\n",
    "        params = params[:-1]\n",
    "    a, b, e, alpha, beta = params\n",
    "        \n",
    "    A = np.exp(a)\n",
    "    B = np.exp(b)\n",
    "    E = np.exp(e)\n",
    "    \n",
    "    L = E + (A / (N**alpha)) + (B /(D**beta))\n",
    "    \n",
    "    return L\n",
    "\n",
    "def scaling_law_kaplan(N, D, params):\n",
    "    if len(params) == 6:\n",
    "        params = params[:-1]\n",
    "    a, b, e, alpha, beta = params\n",
    "        \n",
    "    A = np.exp(a)\n",
    "    B = np.exp(b)\n",
    "    E = np.exp(e)\n",
    "    \n",
    "    L = ((A/N)**(alpha/beta) + B/D)**beta\n",
    "    \n",
    "    return L\n",
    "\n",
    "\n",
    "# def scaling_law_kaplan_chinch(N, D, params):\n",
    "#     if len(params) == 6:\n",
    "#         params = params[:-1]\n",
    "#     a, b, e, alpha, beta = params\n",
    "        \n",
    "#     A = np.exp(a)\n",
    "#     B = np.exp(b)\n",
    "#     E = np.exp(e)\n",
    "    \n",
    "#     L = E + ((A/N)**(alpha/beta) + B/D)**beta\n",
    "    \n",
    "#     return L\n",
    "\n",
    "\n",
    "def opt_N_D(C, G, opt_a, opt_b):\n",
    "    opt_N = G*(C/6)**opt_a\n",
    "    opt_D = (1/G)*(C/6)**opt_b\n",
    "    return opt_N, opt_D\n",
    "\n",
    "def print_opts(best_params):\n",
    "    if len(best_params) == 6:\n",
    "        best_params = best_params[:-1]\n",
    "    opt_alpha = best_params[-2]\n",
    "    opt_beta = best_params[-1]\n",
    "\n",
    "    opt_a =  opt_beta / (opt_alpha+opt_beta)\n",
    "    opt_b =  opt_alpha / (opt_alpha+opt_beta)\n",
    "\n",
    "    A = np.exp(best_params[0])\n",
    "    B = np.exp(best_params[1])\n",
    "    G = ((opt_alpha*A)/(opt_beta*B))**(1/(opt_alpha+opt_beta))\n",
    "\n",
    "    scaling = []\n",
    "\n",
    "    for C in [1.25E+18, 5.01E+18, 1.98E+19, 1E21, 1E23]:\n",
    "        N, D = opt_N_D(C, G, opt_a, opt_b)\n",
    "        scaling.append(\n",
    "            {\"compute\": f\"{C:e}\",\n",
    "            \"parameters (B)\": f\"{N/1e9:.2f}\",\n",
    "            \"tokens (B)\": f\"{D/1e9:.2f}\",\n",
    "            \"ratio\": f\"{D/N:.2f}\",\n",
    "            \"predicted loss (Chinchilla)\": f\"{scaling_law_chinch(N, D, best_params):.2f}\"\n",
    "            \"predicted loss (Kaplan)\": f\"{scaling_law_kaplan(N, D, best_params):.2f}\"\n",
    "            }\n",
    "        )\n",
    "    print(\"Scaling: \\n\", pd.DataFrame(scaling))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/formll/resolving-scaling-law-discrepancies\n",
    "def precise_flops_per_token_chinchilla(width, depth):\n",
    "    seq_len = 2048\n",
    "    vocab_size = 50432\n",
    "    num_heads = 4\n",
    "    width = width.astype(float)\n",
    "    depth = depth.astype(float)\n",
    "\n",
    "    embeddings = 2 * seq_len * width\n",
    "\n",
    "    attention = 2 * 3 * seq_len * (width ** 2)\n",
    "    kq_logits = 2 * seq_len * seq_len * width\n",
    "    softmax = 3 * num_heads * seq_len * seq_len\n",
    "    softmax_q_red = 2 * seq_len * seq_len * width\n",
    "    final_linear = 2 * seq_len * (width ** 2)\n",
    "    attention += kq_logits + softmax + softmax_q_red + final_linear\n",
    "\n",
    "    ffw_size = 4 * width # check this, in the paper it is 4 * width\n",
    "    dense_block = 4 * seq_len * width * ffw_size\n",
    "    final_logits = 2 * seq_len * width * vocab_size\n",
    "    forward_pass = embeddings + depth * attention + depth * dense_block + final_logits\n",
    "    backward_pass = 2 * forward_pass\n",
    "    return (forward_pass + backward_pass) / seq_len\n",
    "    \n",
    "def precise_param_count_open_lm(width, depth, vocab_size=50432):\n",
    "    d_ff = 256 * (((2 * 4 * width / 3).astype(int) + 256 - 1) // 256)\n",
    "    return (4 * width + 3 * d_ff) * width * depth + vocab_size * width\n",
    "    \n",
    "def apply_smoothing_filter(df, filter_func, compensate_for_logging_delay=True, key='train/loss', **filter_args):\n",
    "    out = []\n",
    "    for _, row in df.iterrows():\n",
    "        if len(row[key]) == 0:\n",
    "            out.append(None)\n",
    "            continue\n",
    "        filtered = filter_func(row[key].dropna(), **filter_args)\n",
    "        if compensate_for_logging_delay:\n",
    "            filtered.index = filtered.index - np.diff(filtered.index, prepend=0)/2\n",
    "        out.append(filtered)\n",
    "    return out\n",
    "\n",
    "def proportional_sliding_window_filter(x, p=0.05):\n",
    "    # assert that the index of x has constant increments?\n",
    "    x_cumsum = x.cumsum().values\n",
    "    x_cumsum_pad = np.concatenate([[0], x_cumsum])\n",
    "    inds = np.arange(len(x))\n",
    "    inds_up = np.minimum(inds + np.floor(p * inds).astype(int), len(x)-1)\n",
    "    inds_down = np.maximum(0, inds - np.floor(p * inds).astype(int))\n",
    "    inds_new = (inds_up + inds_down)/2\n",
    "    index_new = np.interp(inds_new, inds, x.index)\n",
    "    try:\n",
    "        x_series = pd.Series((x_cumsum[inds_up] - x_cumsum_pad[inds_down]) / (inds_up - inds_down+1), \n",
    "                     index=index_new, name=x.name + '_smoothed')\n",
    "    except:\n",
    "        print(x)\n",
    "        x_series = pd.Series((x_cumsum[inds_up] - x_cumsum_pad[inds_down]) / (inds_up - inds_down+1), \n",
    "                     index=index_new, name=\"\" + '_smoothed')\n",
    "    return x_series\n",
    "    \n",
    "def process_big_df(big_df):\n",
    "    big_df = big_df.copy()\n",
    "\n",
    "    # Counting parameters\n",
    "    big_df['params_active'] = (12 * (big_df.width**2) * big_df.depth + big_df.vocab_size * big_df.width).astype(float)\n",
    "    big_df['params_active_precise'] = precise_param_count_open_lm(big_df.width, big_df.depth)\n",
    "    big_df['params_no_embed'] = precise_param_count_open_lm(big_df.width, big_df.depth, vocab_size=0)\n",
    "    big_df['params_all'] = 12 * (big_df.width**2) * big_df.depth + (big_df.seq_len + 2 * big_df.vocab_size) * big_df.width\n",
    "    \n",
    "    # Counting FLOPs\n",
    "    big_df['flops_per_token_att_no_embed'] = 6 * big_df['params_no_embed'] + 6 * big_df.seq_len * big_df.width * big_df.depth\n",
    "    big_df['flops_per_token_att'] = 6 * big_df['params_active_precise']  + 6 * big_df.seq_len * big_df.width * big_df.depth\n",
    "    big_df['flops_per_token_cc'] = precise_flops_per_token_chinchilla(big_df['width'], big_df['depth'])\n",
    "    big_df['flops_per_token_no_att'] = 6 * big_df['params_active_precise']\n",
    "    big_df['flops_per_token_no_att_no_embed'] = 6 * big_df['params_no_embed']\n",
    "    big_df['flops_per_token'] = big_df['flops_per_token_no_att']\n",
    "\n",
    "    big_df['params'] = big_df['flops_per_token'] / 6 \n",
    "    big_df['eff_params_att'] = big_df['flops_per_token_att'] / 6\n",
    "\n",
    "\n",
    "    big_df['train/loss_smoothed'] = apply_smoothing_filter(big_df, proportional_sliding_window_filter, compensate_for_logging_delay=True, key='train/loss')\n",
    "    for k in big_df: \n",
    "        if k.startswith('train/') and k.endswith('_loss'):\n",
    "            big_df[k + '_smoothed'] = apply_smoothing_filter(big_df, proportional_sliding_window_filter, compensate_for_logging_delay=False, key=k)\n",
    "    return big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PROJECT_DIR = \"/fsx-onellm/margaretli/env_srcs/xlf/xlformers_n/scaling/data\"\n",
    "\n",
    "col_names = ['C', 'D', 'N', 'lr', 'Avg Train Loss', 'Max Train Loss', 'C4 Eval PPL', 'Wiki Eval PPL', 'C4 Eval Loss', 'Wiki Eval Loss']\n",
    "\n",
    "def read_local_data(csv_file, loss_name='C4 Eval Loss', col_names=col_names):\n",
    "    mins_only = []\n",
    "    df = pd.read_csv(csv_file, usecols=col_names,)\n",
    "    df.dropna(subset=[loss_name], inplace=True)\n",
    "\n",
    "    if 'lr' in df.columns:\n",
    "        df = df.loc[(df['lr'] >= 0)]\n",
    "    if 'D' not in df.columns:\n",
    "        df['D'] = df['C'] / (df['N'] * 6)\n",
    "        \n",
    "    n_vals = df['N'].unique()\n",
    "    d_vals = sorted(df['D'].unique())\n",
    "    for n in n_vals:\n",
    "        for d in d_vals:\n",
    "            cd_df = df[(df['N'] == n) & (df['D'] == d)]\n",
    "            if cd_df.empty:\n",
    "                continue\n",
    "            min_index = cd_df[loss_name].idxmin()\n",
    "            # print(min_index)\n",
    "            # print(cd_df)\n",
    "            # print(cd_df.loc[min_index])\n",
    "            mins_only.append(cd_df.loc[min_index])\n",
    "\n",
    "    mins_only_df = pd.DataFrame(mins_only)\n",
    "\n",
    "    print(mins_only_df)\n",
    "    # df.rename(columns={})\n",
    "    return mins_only_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(use_data, rsld_config_number=0):\n",
    "    local_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    \n",
    "    n_name = \"N\"\n",
    "    d_name = \"D\"\n",
    "    \n",
    "    if use_data == \"ours\":\n",
    "        loss_name = \"C4 Eval Loss\"\n",
    "        csv_file = f\"{local_dir}/data/data.csv\"\n",
    "        training_df = read_local_data(csv_file=csv_file, loss_name=loss_name, col_names=None)\n",
    "    elif use_data == \"epoch_ai\":\n",
    "        loss_name = \"loss\"\n",
    "        csv_file = f\"{local_dir}/data/epoch_ai.csv\"\n",
    "        training_df = read_local_data(csv_file=csv_file, loss_name=loss_name, col_names=None)\n",
    "    elif use_data == \"rsld\":\n",
    "        n_name = \"params_no_embed\"\n",
    "        d_name = \"last_val/loss_D\"\n",
    "        loss_name = \"last_val/loss\"\n",
    "        training_df = get_rsld_data(config_number=0)\n",
    "    elif use_data == \"rsld_isoflop\":\n",
    "        n_name = \"params_no_embed\"\n",
    "        d_name = \"last_val/loss_D\"\n",
    "        loss_name = \"last_val/loss\"\n",
    "        training_df = get_rsld_data_isoflop(config_number=0)\n",
    "    else\n",
    "        raise Error(\"type of data to use not recognized\")\n",
    "\n",
    "    N = training_df[n_name].values\n",
    "    D = training_df[d_name].values\n",
    "    losses = training_df[loss_name].values\n",
    "    bootstraps = 4000\n",
    "    nr_of_models_excluded = 0\n",
    "\n",
    "    sorted_losses = sorted(losses)\n",
    "    if nr_of_models_excluded == 0:\n",
    "        indices = list(range(len(N)))\n",
    "    else:\n",
    "        sorted_losses = sorted(losses)\n",
    "        indices = [i for i in range(len(N)) if losses[i] < sorted_losses[-nr_of_models_excluded]]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    random_indices = [np.random.choice(indices, size=len(indices), replace=True) for _ in range(bootstraps)]\n",
    "\n",
    "    return N, D, losses, indices, random_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd.scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import erf\n",
    "\n",
    "true_params = np.array([np.log(406.4), np.log(410.7), np.log(1.69), 0.34, 0.28])\n",
    "\n",
    "# Define the log-sum-exp function\n",
    "def log_sum_exp_chinch(a, b, e, alpha, beta, N, D):\n",
    "    return np.log(np.exp(a - alpha * np.log(N)) + np.exp(b - beta * np.log(D)) + np.exp(e))\n",
    "\n",
    "def log_sum_exp_kaplan(a, b, e, alpha, beta, N, D):\n",
    "    return np.log(np.exp((alpha / beta) * (a - np.log(N))) + np.exp(b - np.log(D))) * beta\n",
    "\n",
    "## TODO @margaretli\n",
    "def log_sum_exp_kaplan_chinch(a, b, e, alpha, beta, N, D):\n",
    "    return np.log(np.exp((alpha / beta) * (a - np.log(N))) + np.exp(b - np.log(D)) + np.exp(e)) * beta\n",
    "\n",
    "# Define the Huber loss function\n",
    "def custom_huber_loss(y_true, y_pred, delta=1e-3):\n",
    "    # Calculate the difference\n",
    "    diff = y_true - y_pred\n",
    "    # Calculate the condition for Huber loss\n",
    "    cond = np.abs(diff) <= delta\n",
    "    # Apply Huber loss formula\n",
    "    loss = np.where(cond, 0.5 * diff**2, delta * (np.abs(diff) - 0.5 * delta))\n",
    "    return np.sum(loss)\n",
    "\n",
    "def huber_normalizing_factor(delta=1e-3):\n",
    "    return np.sqrt(2*np.pi) * (1 - 2*norm.sf(delta)) + 2 * np.exp(-0.5*delta**2)/delta\n",
    "\n",
    "def huber_logpdf(x, delta=1e-3, loc=0, scale=1):\n",
    "    x = (x-loc)/scale\n",
    "\n",
    "    cond = np.abs(x) <= delta\n",
    "    loss = np.where(cond, 0.5 * x**2, delta * (np.abs(x) - 0.5 * delta))\n",
    "    return -loss - np.log(huber_normalizing_factor(delta=delta)) - np.log(scale)\n",
    "\n",
    "def huber_pdf(x, delta=1e-3, loc=0, scale=1):\n",
    "    return np.exp(huber_logpdf(x, delta=delta, loc=loc, scale=scale))\n",
    "\n",
    "# Define the objective function to be minimized\n",
    "def objective(params, N, D, losses, log_sum_exp_fn=log_sum_exp_chinch):\n",
    "    a, b, e, alpha, beta, sigma = params\n",
    "    predictions = log_sum_exp_fn(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "    # return custom_huber_loss(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "def scale_objective(sigma, params, N, D, losses, log_sum_exp_fn=log_sum_exp_chinch):\n",
    "    a, b, e, alpha, beta = params\n",
    "    predictions = log_sum_exp_fn(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "    # return custom_huber_loss(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "def constant_term_objective(params, a, b, alpha, beta, N, D, losses, log_sum_exp_fn=log_sum_exp_chinch):\n",
    "    e, sigma = params\n",
    "    predictions = log_sum_exp_fn(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "\n",
    "def huber_loss_objective(params, N, D, losses, log_sum_exp_fn=log_sum_exp_chinch):\n",
    "    a, b, e, alpha, beta = params\n",
    "    predictions = log_sum_exp_fn(a, b, e, alpha, beta, N, D)\n",
    "    return custom_huber_loss(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "# Define the parameter untransform\n",
    "def untransform_params(param_array):\n",
    "    if len(np.shape(param_array)) == 2:\n",
    "      return np.hstack((np.exp(param_array[:, :3]), param_array[:, 3:]))\n",
    "    else:\n",
    "      return np.hstack((np.exp(param_array[:3]), param_array[3:]))\n",
    "\n",
    "# Define the Huber loss function on residuals\n",
    "def huber_loss(residuals, delta=1e-3):\n",
    "    # Calculate the difference\n",
    "    diff = residuals\n",
    "    # Calculate the condition for Huber loss\n",
    "    cond = np.abs(diff) <= delta\n",
    "    # Apply Huber loss formula\n",
    "    loss = np.where(cond, 0.5 * diff**2, delta * (np.abs(diff) - 0.5 * delta))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import heapq\n",
    "\n",
    "class PQItem(object):\n",
    "    def __init__(self, loss, params):\n",
    "        self.loss = loss\n",
    "        self.params = params\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.loss > other.loss # reversed because we want to retain lower loss params\n",
    "\n",
    "\n",
    "def fit_from_scratch(N, D, losses, indices, obj=huber_loss_objective, method='BFGS', use_grad=True, add_sigma=False):\n",
    "    # Set up the grid for initial parameter values\n",
    "    # alpha_vals = np.arange(0, 2.5, 0.5)\n",
    "    # beta_vals = np.arange(0, 2.5, 0.5)\n",
    "    # e_vals = np.arange(-1, 1.5, 0.5)\n",
    "    # a_vals = np.arange(0, 30, 5)\n",
    "    # b_vals = np.arange(0, 30, 5)\n",
    "\n",
    "    a_vals = np.arange(0, 30, 1) \n",
    "    b_vals = np.arange(0, 30, 1) \n",
    "    e_vals = np.arange(-1, 1.5, 0.1) \n",
    "    alpha_vals = np.arange(0, 2.5, 0.1)\n",
    "    beta_vals = np.arange(0, 2.5, 0.1)\n",
    "\n",
    "    # Perform the optimization using L-BFGS over the grid of initial values\n",
    "    best_loss = np.inf\n",
    "    best_params = None\n",
    "    pq = []\n",
    "    i = 0\n",
    "\n",
    "    grid = np.array(np.meshgrid(\n",
    "        a_vals, b_vals, e_vals, alpha_vals, beta_vals\n",
    "    )).T.reshape(-1, 5)\n",
    "    # pbar = tqdm(total=len(grid))\n",
    "    i = 0\n",
    "    np.random.shuffle(grid)\n",
    "    \n",
    "    from itertools import product\n",
    "    results_dict = {}\n",
    "    for init_params in grid:\n",
    "    # for alpha, beta, e, a, b in product(alpha_vals, beta_vals, e_vals, a_vals, b_vals):\n",
    "        # init_params = [a, b, e, alpha, beta]\n",
    "        if add_sigma:\n",
    "            init_params = init_params + [0]\n",
    "        result = minimize(obj, init_params, args=(N[indices], D[indices], losses[indices]), method=method, jac=grad(obj) if use_grad else None)\n",
    "        results_dict[tuple(init_params)] = {'params': result.x, 'loss': result.fun}\n",
    "        if result.success and result.fun < best_loss:\n",
    "            best_loss = result.fun\n",
    "            best_params = result.x\n",
    "\n",
    "        if len(pq) < 100:\n",
    "            heapq.heappush(pq, PQItem(result.fun, result.x))\n",
    "        elif result.fun < pq[0].loss:\n",
    "            heapq.heappushpop(pq, PQItem(result.fun, result.x))\n",
    "\n",
    "        # pbar.update(1)\n",
    "        # if pbar.n % 1000 == 0:\n",
    "        #     print(\"best loss: {} : best params : {}\".format(best_loss, best_params))\n",
    "        # if pbar.n == 100_000:\n",
    "        #     break\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(\"best loss: {} : best params : {}\".format(best_loss, best_params))\n",
    "        if i == 10_000:\n",
    "            break\n",
    "\n",
    "    largest = heapq.nlargest(100, pq)\n",
    "    for item in largest:\n",
    "        print(round(item.loss, 6), [round(n, 2) for n in item.params])\n",
    "\n",
    "    # Transform the fitted parameters a, b, e to A, B, E\n",
    "    if best_params is not None:\n",
    "        A = np.exp(best_params[0])\n",
    "        B = np.exp(best_params[1])\n",
    "        E = np.exp(best_params[2])\n",
    "        alpha = best_params[3]\n",
    "        beta = best_params[4]\n",
    "        print(f\"Best fit parameters: A={A}, B={B}, E={E}, alpha={alpha}, beta={beta}\")\n",
    "        print_opts(best_params)\n",
    "        print(f\"Best loss: {best_loss}\")\n",
    "    else:\n",
    "        print(\"Optimization failed to converge.\")\n",
    "\n",
    "def fit_from_chinchilla_random(N, D, losses, random_indices, obj=huber_loss_objective, method='BFGS', use_grad=True, add_sigma=False):\n",
    "    # Set up the grid for initial parameter values\n",
    "    param_list = []\n",
    "\n",
    "    for num, indices in enumerate(random_indices):\n",
    "    # Perform the optimization using BFGS\n",
    "        best_loss = np.inf\n",
    "        best_params = None\n",
    "\n",
    "        init_params = list(true_params)\n",
    "            \n",
    "        if add_sigma:\n",
    "            init_params = init_params + [0]\n",
    "\n",
    "        result = minimize(obj, init_params, args=(N[indices], D[indices], losses[indices]), \\\n",
    "                            jac=grad(obj) if use_grad else None, method=method)\n",
    "\n",
    "        best_loss = result.fun\n",
    "        best_params = result.x\n",
    "        #print(f\"New best loss: {best_loss}\")\n",
    "        #print(f\"Best params: {best_params}\")\n",
    "\n",
    "        if num % 1000 == 999:\n",
    "            print(\"Bootstrap step %d completed\" % (num+1))\n",
    "\n",
    "        param_list.append(result.x)\n",
    "\n",
    "    param_list = np.array(param_list)\n",
    "    cov_matrix = np.cov(np.transpose(param_list))\n",
    "\n",
    "    if best_params is not None:\n",
    "        A = np.exp(best_params[0])\n",
    "        B = np.exp(best_params[1])\n",
    "        E = np.exp(best_params[2])\n",
    "        alpha = best_params[3]\n",
    "        beta = best_params[4]\n",
    "        print(f\"Best fit parameters: A={A}, B={B}, E={E}, alpha={alpha}, beta={beta}\")\n",
    "        print_opts(best_params)\n",
    "        print(f\"Best loss: {best_loss}\")\n",
    "    else:\n",
    "        print(\"Optimization failed to converge.\")\n",
    "\n",
    "def fit_from_init(N, D, losses, indices, obj=huber_loss_objective, method='BFGS', use_grad=True, add_sigma=False, init_params=None):\n",
    "\n",
    "    if add_sigma:\n",
    "        init_params = init_params + [0]\n",
    "        \n",
    "    result = minimize(obj, init_params, args=(N[indices], D[indices], losses[indices]), method=method, jac=grad(obj) if use_grad else None)\n",
    "\n",
    "    print(result)\n",
    "    print(result.x)\n",
    "    best_loss = result.fun\n",
    "\n",
    "    estimated_params = result.x[:5]\n",
    "    # best_params = untransform_params(estimated_params)\n",
    "    best_params = estimated_params\n",
    "    if best_params is not None:\n",
    "        A = np.exp(best_params[0])\n",
    "        B = np.exp(best_params[1])\n",
    "        E = np.exp(best_params[2])\n",
    "        alpha = best_params[3]\n",
    "        beta = best_params[4]\n",
    "        print(f\"Best fit parameters: A={A}, B={B}, E={E}, alpha={alpha}, beta={beta}\")\n",
    "        print_opts(best_params)\n",
    "        print(f\"Best loss: {best_loss}\")\n",
    "    else:\n",
    "        print(\"Optimization failed to converge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import heapq\n",
    "\n",
    "class PQItem(object):\n",
    "    def __init__(self, loss, params):\n",
    "        self.loss = loss\n",
    "        self.params = params\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.loss > other.loss # reversed because we want to retain lower loss params\n",
    "\n",
    "# custom_huber_loss(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "def fit_init_only_with_grid_search(\n",
    "        N, D, data_losses,\n",
    "        obj_func=log_sum_exp_chinch, loss_fn=(lambda x,y : np.sum((x - y)**2)),\n",
    "        a=None, b=None, e=None, alpha=None, beta=None\n",
    "    ):\n",
    "    # a_vals = np.arange(0, 30, 0.3) if a is None else a\n",
    "    # b_vals = np.arange(0, 30, 0.3) if b is None else b\n",
    "    # e_vals = np.arange(-1, 1.5, 0.025) if e is None else e\n",
    "    # alpha_vals = np.arange(0, 2.5, 0.025) if alpha is None else alpha\n",
    "    # beta_vals = np.arange(0, 2.5, 0.025) if beta is None else beta\n",
    "\n",
    "    a_vals = np.arange(0, 30, 1) if a is None else a\n",
    "    b_vals = np.arange(0, 30, 1) if b is None else b\n",
    "    e_vals = np.arange(-1, 1.5, 0.1) if e is None else e\n",
    "    alpha_vals = np.arange(0, 2.5, 0.1) if alpha is None else alpha\n",
    "    beta_vals = np.arange(0, 2.5, 0.1) if beta is None else beta\n",
    "\n",
    "    best_params = None\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    grid = np.array(np.meshgrid(\n",
    "        a_vals, b_vals, e_vals, alpha_vals, beta_vals\n",
    "    )).T.reshape(-1, 5)\n",
    "    # pbar = tqdm(total=len(grid))\n",
    "    i = 0\n",
    "    np.random.shuffle(grid)\n",
    "\n",
    "    pq = []\n",
    "\n",
    "    for params in grid:\n",
    "        pred_losses = np.exp(obj_func(*params, N, D))\n",
    "        loss = loss_fn(pred_losses, data_losses)\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_params = params\n",
    "\n",
    "        if len(pq) < 100:\n",
    "            heapq.heappush(pq, PQItem(loss, params))\n",
    "        elif loss < pq[0].loss:\n",
    "            heapq.heappushpop(pq, PQItem(loss, params))\n",
    "\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(\"best loss: {} : best params : {}\".format(best_loss, best_params))\n",
    "            # pass\n",
    "        if i == 1_000_000:\n",
    "            break\n",
    "\n",
    "    largest = heapq.nlargest(100, pq)\n",
    "    for item in largest:\n",
    "        print(round(item.loss, 4), [round(n, 2) for n in item.params])\n",
    "\n",
    "    return pq, best_loss, best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             x          y    color             N             C hex_color  \\\n",
      "0    154.03592  140.63364  #faebdd  6.795600e+09  9.993853e+18   #faebdd   \n",
      "1    151.60375  175.51709  #f8d1b8  2.979521e+09  9.227541e+18   #f8d1b8   \n",
      "6    223.29648  175.51709  #931c5b  2.979521e+09  9.691017e+19   #931c5b   \n",
      "2    153.29233  180.65761  #f47a54  2.638631e+09  9.753047e+18   #f47a54   \n",
      "7    223.74805  180.65761  #8c1d5b  2.638631e+09  9.835628e+19   #8c1d5b   \n",
      "..         ...        ...      ...           ...           ...       ...   \n",
      "226  254.57913  244.27495  #781f59  5.865994e+08  2.703956e+20   #781f59   \n",
      "227  256.05204  257.94760  #811e5a  4.246106e+08  2.837799e+20   #811e5a   \n",
      "234  234.10686  252.01338  #871e5b  4.885467e+08  1.381549e+20   #871e5b   \n",
      "236  248.18209  239.03398  #7a1f59  6.639580e+08  2.192159e+20   #7a1f59   \n",
      "244  372.54603  140.63355  #34193d  6.795615e+09  1.295602e+22   #34193d   \n",
      "\n",
      "         loss             D  \n",
      "0    5.005582  2.451060e+08  \n",
      "1    4.665232  5.161647e+08  \n",
      "6    2.628285  5.420903e+09  \n",
      "2    3.765563  6.160421e+08  \n",
      "7    2.585322  6.212583e+09  \n",
      "..        ...           ...  \n",
      "226  2.464301  7.682573e+10  \n",
      "227  2.516535  1.113883e+11  \n",
      "234  2.554521  4.713125e+10  \n",
      "236  2.475405  5.502754e+10  \n",
      "244  2.077394  3.177545e+11  \n",
      "\n",
      "[245 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "N, D, losses, indices, random_indices = get_data(\"epoch_ai\")\n",
    "# N, D, losses, indices, random_indices = get_data(\"rsld\", rsld_config_number=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.73169920e+09 1.93173914e+10 6.97250611e+09 2.86271734e+10\n",
      " 9.37112371e+09 1.22935050e+10 6.16615117e+09 6.13731533e+09\n",
      " 6.08226509e+09 6.44349952e+09 5.66021325e+09 1.14677514e+10\n",
      " 4.33533747e+09 7.29913754e+09 1.69413181e+10 2.16635802e+10]\n"
     ]
    }
   ],
   "source": [
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 0.7975132762123402 : best params : [ 6.  15.   1.1  0.4  1.9]\n",
      "best loss: 0.7975132762123402 : best params : [ 6.  15.   1.1  0.4  1.9]\n",
      "best loss: 0.7660174186969138 : best params : [ 7.  18.   1.2  0.5  2.1]\n",
      "best loss: 0.7660174186969138 : best params : [ 7.  18.   1.2  0.5  2.1]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.5926721001750532 : best params : [ 3.  11.   1.1  0.2  1.6]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.4206883708868552 : best params : [ 2.  15.   0.6  0.1  0.7]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.40835412043283636 : best params : [3.  7.  0.6 0.2 0.3]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "best loss: 0.3953235081284995 : best params : [ 2.  10.   0.7  0.1  0.5]\n",
      "0.3953 [2.0, 10.0, 0.7, 0.1, 0.5]\n",
      "0.4084 [3.0, 7.0, 0.6, 0.2, 0.3]\n",
      "0.4207 [2.0, 15.0, 0.6, 0.1, 0.7]\n",
      "0.4521 [3.0, 25.0, 1.0, 0.2, 1.2]\n",
      "0.5036 [3.0, 3.0, -0.0, 0.2, 0.1]\n",
      "0.506 [2.0, 16.0, 0.8, 0.1, 0.8]\n",
      "0.5602 [2.0, 1.0, 0.8, 0.1, 0.2]\n",
      "0.5656 [3.0, 3.0, -0.3, 0.2, 0.1]\n",
      "0.574 [4.0, 28.0, 1.1, 0.3, 1.3]\n",
      "0.5761 [2.0, 15.0, 0.7, 0.1, 0.7]\n",
      "0.5765 [2.0, 20.0, 0.8, 0.1, 1.1]\n",
      "0.5927 [3.0, 19.0, 1.1, 0.2, 2.4]\n",
      "0.5927 [3.0, 6.0, 1.1, 0.2, 2.2]\n",
      "0.5927 [3.0, 7.0, 1.1, 0.2, 2.2]\n",
      "0.5927 [3.0, 12.0, 1.1, 0.2, 2.3]\n",
      "0.5927 [3.0, 3.0, 1.1, 0.2, 1.9]\n",
      "0.5927 [3.0, 6.0, 1.1, 0.2, 2.3]\n",
      "0.5927 [3.0, 7.0, 1.1, 0.2, 2.0]\n",
      "0.5927 [3.0, 5.0, 1.1, 0.2, 2.3]\n",
      "0.5927 [3.0, 11.0, 1.1, 0.2, 2.3]\n",
      "0.5927 [3.0, 11.0, 1.1, 0.2, 2.0]\n",
      "0.5927 [3.0, 7.0, 1.1, 0.2, 1.8]\n",
      "0.5927 [3.0, 20.0, 1.1, 0.2, 2.3]\n",
      "0.5927 [3.0, 15.0, 1.1, 0.2, 2.0]\n",
      "0.5927 [3.0, 11.0, 1.1, 0.2, 1.8]\n",
      "0.5927 [3.0, 25.0, 1.1, 0.2, 2.4]\n",
      "0.5927 [3.0, 0.0, 1.1, 0.2, 1.3]\n",
      "0.5927 [3.0, 25.0, 1.1, 0.2, 2.3]\n",
      "0.5927 [3.0, 11.0, 1.1, 0.2, 1.6]\n",
      "0.5927 [3.0, 21.0, 1.1, 0.2, 2.0]\n",
      "0.5927 [3.0, 15.0, 1.1, 0.2, 1.7]\n",
      "0.5927 [3.0, 16.0, 1.1, 0.2, 1.6]\n",
      "0.5927 [3.0, 16.0, 1.1, 0.2, 1.5]\n",
      "0.5927 [3.0, 21.0, 1.1, 0.2, 1.7]\n",
      "0.5927 [3.0, 15.0, 1.1, 0.2, 1.4]\n",
      "0.5927 [3.0, 4.0, 1.1, 0.2, 0.9]\n",
      "0.5927 [3.0, 7.0, 1.1, 0.2, 1.0]\n",
      "0.5927 [3.0, 21.0, 1.1, 0.2, 1.5]\n",
      "0.5927 [3.0, 28.0, 1.1, 0.2, 1.7]\n",
      "0.5927 [3.0, 14.0, 1.1, 0.2, 1.1]\n",
      "0.5927 [3.0, 0.0, 1.1, 0.2, 0.5]\n",
      "0.5933 [3.0, 29.0, 1.1, 0.2, 1.6]\n",
      "0.5936 [2.0, 20.0, 0.8, 0.1, 1.2]\n",
      "0.5939 [3.0, 25.0, 1.1, 0.2, 1.4]\n",
      "0.5954 [2.0, 25.0, 0.8, 0.1, 1.5]\n",
      "0.5956 [2.0, 22.0, 0.8, 0.1, 1.4]\n",
      "0.5957 [2.0, 8.0, 0.8, 0.1, 0.8]\n",
      "0.5957 [2.0, 28.0, 0.8, 0.1, 1.7]\n",
      "0.5957 [2.0, 3.0, 0.8, 0.1, 0.6]\n",
      "0.5957 [2.0, 14.0, 0.8, 0.1, 1.1]\n",
      "0.5958 [2.0, 8.0, 0.8, 0.1, 0.9]\n",
      "0.5958 [2.0, 20.0, 0.8, 0.1, 1.5]\n",
      "0.5958 [2.0, 22.0, 0.8, 0.1, 1.6]\n",
      "0.5958 [2.0, 10.0, 0.8, 0.1, 1.1]\n",
      "0.5958 [2.0, 3.0, 0.8, 0.1, 0.8]\n",
      "0.5958 [2.0, 16.0, 0.8, 0.1, 1.4]\n",
      "0.5958 [2.0, 2.0, 0.8, 0.1, 0.8]\n",
      "0.5958 [2.0, 12.0, 0.8, 0.1, 1.3]\n",
      "0.5958 [2.0, 27.0, 0.8, 0.1, 2.0]\n",
      "0.5958 [2.0, 13.0, 0.8, 0.1, 1.5]\n",
      "0.5958 [2.0, 5.0, 0.8, 0.1, 1.2]\n",
      "0.5958 [2.0, 16.0, 0.8, 0.1, 1.7]\n",
      "0.5958 [2.0, 7.0, 0.8, 0.1, 1.3]\n",
      "0.5958 [2.0, 0.0, 0.8, 0.1, 1.0]\n",
      "0.5958 [2.0, 20.0, 0.8, 0.1, 1.9]\n",
      "0.5958 [2.0, 22.0, 0.8, 0.1, 2.0]\n",
      "0.5958 [2.0, 24.0, 0.8, 0.1, 2.1]\n",
      "0.5958 [2.0, 5.0, 0.8, 0.1, 1.3]\n",
      "0.5958 [2.0, 5.0, 0.8, 0.1, 1.4]\n",
      "0.5958 [2.0, 27.0, 0.8, 0.1, 2.4]\n",
      "0.5958 [2.0, 22.0, 0.8, 0.1, 2.2]\n",
      "0.5958 [2.0, 4.0, 0.8, 0.1, 1.4]\n",
      "0.5958 [2.0, 3.0, 0.8, 0.1, 1.4]\n",
      "0.5958 [2.0, 14.0, 0.8, 0.1, 1.9]\n",
      "0.5958 [2.0, 12.0, 0.8, 0.1, 1.9]\n",
      "0.5958 [2.0, 3.0, 0.8, 0.1, 1.5]\n",
      "0.5958 [2.0, 18.0, 0.8, 0.1, 2.3]\n",
      "0.5958 [2.0, 20.0, 0.8, 0.1, 2.4]\n",
      "0.5958 [2.0, 17.0, 0.8, 0.1, 2.3]\n",
      "0.5958 [2.0, 16.0, 0.8, 0.1, 2.3]\n",
      "0.5958 [2.0, 14.0, 0.8, 0.1, 2.4]\n",
      "0.5958 [2.0, 2.0, 0.8, 0.1, 1.9]\n",
      "0.5958 [2.0, 1.0, 0.8, 0.1, 1.8]\n",
      "0.5958 [2.0, 10.0, 0.8, 0.1, 2.2]\n",
      "0.5958 [2.0, 13.0, 0.8, 0.1, 2.4]\n",
      "0.5958 [2.0, 0.0, 0.8, 0.1, 2.2]\n",
      "0.5958 [2.0, 6.0, 0.8, 0.1, 1.9]\n",
      "0.5975 [3.0, 24.0, 1.1, 0.2, 1.3]\n",
      "0.6066 [3.0, 18.0, 1.1, 0.2, 1.0]\n",
      "0.6158 [7.0, 12.0, 1.1, 0.5, 0.6]\n",
      "0.63 [3.0, 29.0, 1.0, 0.2, 1.4]\n",
      "0.6368 [3.0, 19.0, 1.1, 0.2, 1.0]\n",
      "0.6369 [2.0, 5.0, 0.7, 0.1, 0.3]\n",
      "0.6413 [2.0, 7.0, -0.0, 0.1, 0.3]\n",
      "0.6524 [7.0, 1.0, 1.1, 0.5, 0.1]\n",
      "0.6645 [4.0, 7.0, 0.7, 0.3, 0.3]\n",
      "0.6795 [4.0, 3.0, 0.1, 0.3, 0.1]\n",
      "0.6883 [3.0, 15.0, 1.1, 0.2, 0.8]\n",
      "0.7595 [3.0, 5.0, 0.2, 0.2, 0.2]\n",
      "0.7627 [4.0, 23.0, 1.1, 0.3, 1.1]\n"
     ]
    }
   ],
   "source": [
    "pq, best_loss, best_params = fit_init_only_with_grid_search(N, D, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 3.787945545510745e-05 : best params : [ 2.55680649 15.01163762  0.57347052  0.11964638  0.74169355]\n",
      "best loss: 3.787945545510745e-05 : best params : [ 2.55680649 15.01163762  0.57347052  0.11964638  0.74169355]\n",
      "best loss: 3.786205707473094e-05 : best params : [ 2.58213038 13.97855658  0.58821181  0.12227579  0.69480374]\n",
      "best loss: 3.784929045224317e-05 : best params : [ 2.60025232 13.96955753  0.60120201  0.12415799  0.69483638]\n",
      "best loss: 3.784929045224317e-05 : best params : [ 2.60025232 13.96955753  0.60120201  0.12415799  0.69483638]\n",
      "best loss: 3.784929045224317e-05 : best params : [ 2.60025232 13.96955753  0.60120201  0.12415799  0.69483638]\n",
      "best loss: 3.784929045224317e-05 : best params : [ 2.60025232 13.96955753  0.60120201  0.12415799  0.69483638]\n",
      "best loss: 3.784929045224317e-05 : best params : [ 2.60025232 13.96955753  0.60120201  0.12415799  0.69483638]\n",
      "best loss: 3.784929045224317e-05 : best params : [ 2.60025232 13.96955753  0.60120201  0.12415799  0.69483638]\n",
      "best loss: 3.784929045224317e-05 : best params : [ 2.60025232 13.96955753  0.60120201  0.12415799  0.69483638]\n",
      "3.8e-05 [2.6, 13.97, 0.6, 0.12, 0.69]\n",
      "3.8e-05 [2.6, 14.02, 0.6, 0.12, 0.7]\n",
      "3.8e-05 [2.6, 13.72, 0.6, 0.12, 0.68]\n",
      "3.8e-05 [2.61, 13.98, 0.61, 0.13, 0.7]\n",
      "3.8e-05 [2.58, 13.98, 0.59, 0.12, 0.69]\n",
      "3.8e-05 [2.59, 14.91, 0.6, 0.12, 0.74]\n",
      "3.8e-05 [2.62, 14.06, 0.61, 0.13, 0.7]\n",
      "3.8e-05 [2.62, 13.6, 0.61, 0.13, 0.68]\n",
      "3.8e-05 [2.6, 14.97, 0.6, 0.12, 0.74]\n",
      "3.8e-05 [2.56, 15.01, 0.57, 0.12, 0.74]\n",
      "3.8e-05 [2.61, 15.01, 0.61, 0.13, 0.74]\n",
      "3.8e-05 [2.62, 13.01, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.62, 13.01, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.61, 13.01, 0.6, 0.13, 0.65]\n",
      "3.8e-05 [2.62, 15.0, 0.62, 0.13, 0.74]\n",
      "3.8e-05 [2.62, 15.03, 0.62, 0.13, 0.74]\n",
      "3.8e-05 [2.6, 15.77, 0.61, 0.12, 0.78]\n",
      "3.8e-05 [2.58, 13.11, 0.58, 0.12, 0.65]\n",
      "3.8e-05 [2.63, 12.29, 0.61, 0.13, 0.62]\n",
      "3.8e-05 [2.54, 15.97, 0.56, 0.12, 0.78]\n",
      "3.8e-05 [2.6, 16.01, 0.61, 0.12, 0.79]\n",
      "3.8e-05 [2.62, 15.97, 0.62, 0.13, 0.79]\n",
      "3.8e-05 [2.62, 16.0, 0.63, 0.13, 0.79]\n",
      "3.8e-05 [2.63, 16.0, 0.63, 0.13, 0.79]\n",
      "3.8e-05 [2.63, 16.01, 0.63, 0.13, 0.79]\n",
      "3.8e-05 [2.63, 12.01, 0.61, 0.13, 0.61]\n",
      "3.8e-05 [2.63, 11.98, 0.61, 0.13, 0.6]\n",
      "3.8e-05 [2.63, 16.23, 0.63, 0.13, 0.8]\n",
      "3.8e-05 [2.63, 11.84, 0.61, 0.13, 0.6]\n",
      "3.8e-05 [2.63, 16.52, 0.63, 0.13, 0.81]\n",
      "3.8e-05 [2.53, 16.97, 0.56, 0.12, 0.83]\n",
      "3.8e-05 [2.62, 16.88, 0.63, 0.13, 0.83]\n",
      "3.8e-05 [2.61, 16.97, 0.62, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 16.98, 0.63, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 16.96, 0.64, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 17.0, 0.63, 0.13, 0.83]\n",
      "3.8e-05 [2.64, 16.97, 0.64, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 17.02, 0.63, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 17.02, 0.64, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 17.11, 0.63, 0.13, 0.84]\n",
      "3.8e-05 [2.64, 11.03, 0.62, 0.13, 0.56]\n",
      "3.8e-05 [2.64, 11.02, 0.61, 0.13, 0.56]\n",
      "3.8e-05 [2.64, 11.02, 0.61, 0.13, 0.56]\n",
      "3.8e-05 [2.64, 11.01, 0.61, 0.13, 0.56]\n",
      "3.8e-05 [2.64, 11.01, 0.61, 0.13, 0.56]\n",
      "3.8e-05 [2.64, 11.02, 0.61, 0.13, 0.56]\n",
      "3.8e-05 [2.64, 11.0, 0.61, 0.13, 0.56]\n",
      "3.8e-05 [2.64, 10.99, 0.61, 0.13, 0.56]\n",
      "3.8e-05 [2.65, 10.73, 0.61, 0.13, 0.55]\n",
      "3.8e-05 [2.63, 17.71, 0.64, 0.13, 0.87]\n",
      "3.8e-05 [2.63, 17.75, 0.64, 0.13, 0.87]\n",
      "3.8e-05 [2.63, 17.84, 0.64, 0.13, 0.87]\n",
      "3.8e-05 [2.64, 17.97, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.64, 17.97, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.63, 18.01, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.64, 18.01, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.64, 18.02, 0.65, 0.13, 0.88]\n",
      "3.8e-05 [2.64, 18.07, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.65, 10.13, 0.62, 0.13, 0.52]\n",
      "3.8e-05 [2.65, 10.03, 0.62, 0.13, 0.51]\n",
      "3.8e-05 [2.65, 10.01, 0.61, 0.13, 0.51]\n",
      "3.8e-05 [2.66, 9.98, 0.62, 0.13, 0.51]\n",
      "3.8e-05 [2.66, 9.87, 0.62, 0.13, 0.51]\n",
      "3.8e-05 [2.66, 9.76, 0.62, 0.13, 0.5]\n",
      "3.8e-05 [2.64, 18.4, 0.65, 0.13, 0.9]\n",
      "3.8e-05 [2.64, 18.65, 0.65, 0.13, 0.91]\n",
      "3.9e-05 [2.67, 9.03, 0.62, 0.13, 0.47]\n",
      "3.9e-05 [2.64, 18.83, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.67, 9.02, 0.62, 0.13, 0.47]\n",
      "3.9e-05 [2.64, 18.93, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 18.96, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 18.94, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 18.96, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 18.98, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 19.01, 0.65, 0.13, 0.93]\n",
      "3.9e-05 [2.64, 19.01, 0.65, 0.13, 0.93]\n",
      "3.9e-05 [2.64, 19.01, 0.64, 0.13, 0.93]\n",
      "3.9e-05 [2.42, 16.02, 0.44, 0.1, 0.78]\n",
      "3.9e-05 [2.69, 8.07, 0.62, 0.13, 0.42]\n",
      "3.9e-05 [2.69, 7.99, 0.62, 0.13, 0.42]\n",
      "3.9e-05 [2.64, 19.52, 0.65, 0.13, 0.95]\n",
      "3.9e-05 [2.64, 19.83, 0.65, 0.13, 0.96]\n",
      "3.9e-05 [2.64, 19.84, 0.65, 0.13, 0.96]\n",
      "3.9e-05 [2.64, 19.96, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.96, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.97, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.97, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.97, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.96, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 20.01, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 20.01, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 20.01, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 20.02, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 20.02, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 20.03, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.66, 19.95, 0.66, 0.13, 0.97]\n",
      "3.9e-05 [2.69, 7.06, 0.61, 0.13, 0.37]\n",
      "3.9e-05 [2.69, 7.02, 0.61, 0.13, 0.37]\n",
      "3.9e-05 [2.69, 7.02, 0.61, 0.13, 0.37]\n",
      "3.9e-05 [2.69, 7.02, 0.61, 0.13, 0.37]\n",
      "Best fit parameters: A=13.467135677968107, B=1166545.6811785959, E=1.8243103236198983, alpha=0.1241579899551615, beta=0.6948363842294596\n",
      "Scaling: \n",
      "         compute parameters (B) tokens (B)  ratio predicted loss\n",
      "0  1.250000e+18           0.06       3.69  65.53           3.56\n",
      "1  5.010000e+18           0.18       4.56  24.90           3.32\n",
      "2  1.980000e+19           0.59       5.62   9.56           3.12\n",
      "3  1.000000e+21          16.37      10.18   0.62           2.68\n",
      "4  1.000000e+23         814.64      20.46   0.03           2.35\n",
      "Best loss: 3.784929045224317e-05\n"
     ]
    }
   ],
   "source": [
    "fit_from_scratch(N, D, losses, indices, obj=huber_loss_objective, method='L-BFGS-B', use_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 3.9903721089100134e-05\n",
      "        x: [ 2.687e+00  5.409e+00  5.737e-01  1.329e-01  2.900e-01]\n",
      "      nit: 90\n",
      "      jac: [ 6.143e-06  2.748e-07  7.069e-06 -1.003e-04 -2.327e-05]\n",
      "     nfev: 858\n",
      "     njev: 143\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "[2.68682667 5.40857773 0.57373204 0.13289562 0.28997104]\n",
      "Best fit parameters: A=14.685001525287666, B=223.31374881887865, E=1.7748786232868297, alpha=0.1328956236915945, beta=0.2899710402320098\n",
      "Scaling: \n",
      "         compute parameters (B) tokens (B) ratio predicted loss\n",
      "0  1.250000e+18           0.19       1.10  5.76           3.47\n",
      "1  5.010000e+18           0.49       1.69  3.44           3.27\n",
      "2  1.980000e+19           1.26       2.61  2.06           3.10\n",
      "3  1.000000e+21          18.62       8.95  0.48           2.70\n",
      "4  1.000000e+23         438.04      38.05  0.09           2.38\n",
      "Best loss: 3.9903721089100134e-05\n"
     ]
    }
   ],
   "source": [
    "# np.array([np.log(406.4), np.log(410.7), np.log(1.69), 0.34, 0.28])\n",
    "# array([6.0073379 , 6.01786302, 0.52472853, 0.34      , 0.28      ])\n",
    "init_params = [6.0073379 , 6.01786302, 0.52472853, 0.34, 0.28]\n",
    "# fit_from_init(obj=huber_loss_objective, method='L-BFGS-B', use_grad=False, init_params=list(true_params))\n",
    "fit_from_init(N, D, losses, indices, obj=huber_loss_objective, method='L-BFGS-B', use_grad=False, init_params=init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0021415626088716454"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huber_loss_objective(true_params, N, D, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 3.789252564543738e-05 : best params : [ 2.6334663  14.6751309   0.62756515  0.12769534  0.72791943]\n",
      "best loss: 3.788544711694731e-05 : best params : [ 2.62356022 13.08068485  0.61289141  0.12651906  0.65439012]\n",
      "best loss: 3.785837821408434e-05 : best params : [ 2.61180955 14.01415087  0.60943915  0.12535571  0.69711318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 3.785837821408434e-05 : best params : [ 2.61180955 14.01415087  0.60943915  0.12535571  0.69711318]\n",
      "best loss: 3.785837821408434e-05 : best params : [ 2.61180955 14.01415087  0.60943915  0.12535571  0.69711318]\n",
      "best loss: 3.785837821408434e-05 : best params : [ 2.61180955 14.01415087  0.60943915  0.12535571  0.69711318]\n",
      "best loss: 3.785837821408434e-05 : best params : [ 2.61180955 14.01415087  0.60943915  0.12535571  0.69711318]\n",
      "best loss: 3.785262501411214e-05 : best params : [ 2.60607582 14.01419296  0.60550674  0.12475928  0.69704812]\n",
      "best loss: 3.785262501411214e-05 : best params : [ 2.60607582 14.01419296  0.60550674  0.12475928  0.69704812]\n",
      "best loss: 3.785262501411214e-05 : best params : [ 2.60607582 14.01419296  0.60550674  0.12475928  0.69704812]\n",
      "3.8e-05 [2.61, 14.01, 0.61, 0.12, 0.7]\n",
      "3.8e-05 [2.59, 14.62, 0.6, 0.12, 0.72]\n",
      "3.8e-05 [2.61, 14.01, 0.61, 0.13, 0.7]\n",
      "3.8e-05 [2.57, 15.01, 0.58, 0.12, 0.74]\n",
      "3.8e-05 [2.62, 13.64, 0.61, 0.13, 0.68]\n",
      "3.8e-05 [2.61, 13.34, 0.61, 0.13, 0.67]\n",
      "3.8e-05 [2.62, 13.08, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.62, 13.02, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.62, 13.02, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.63, 14.68, 0.63, 0.13, 0.73]\n",
      "3.8e-05 [2.62, 15.02, 0.62, 0.13, 0.74]\n",
      "3.8e-05 [2.62, 15.02, 0.62, 0.13, 0.74]\n",
      "3.8e-05 [2.63, 12.73, 0.61, 0.13, 0.64]\n",
      "3.8e-05 [2.61, 15.95, 0.61, 0.12, 0.79]\n",
      "3.8e-05 [2.66, 13.99, 0.64, 0.13, 0.7]\n",
      "3.8e-05 [2.63, 15.97, 0.63, 0.13, 0.79]\n",
      "3.8e-05 [2.63, 16.03, 0.63, 0.13, 0.79]\n",
      "3.8e-05 [2.63, 16.07, 0.63, 0.13, 0.79]\n",
      "3.8e-05 [2.63, 11.99, 0.61, 0.13, 0.6]\n",
      "3.8e-05 [2.63, 11.98, 0.61, 0.13, 0.6]\n",
      "3.8e-05 [2.63, 11.91, 0.61, 0.13, 0.6]\n",
      "3.8e-05 [2.63, 11.8, 0.61, 0.13, 0.6]\n",
      "3.8e-05 [2.64, 11.69, 0.61, 0.13, 0.59]\n",
      "3.8e-05 [2.64, 11.6, 0.61, 0.13, 0.59]\n",
      "3.8e-05 [2.53, 17.01, 0.56, 0.12, 0.83]\n",
      "3.8e-05 [2.63, 16.96, 0.63, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 16.97, 0.64, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 16.99, 0.63, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 16.97, 0.64, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 17.01, 0.63, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 17.01, 0.63, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 17.02, 0.64, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 17.08, 0.63, 0.13, 0.84]\n",
      "3.8e-05 [2.64, 16.99, 0.64, 0.13, 0.83]\n",
      "3.8e-05 [2.64, 11.14, 0.62, 0.13, 0.57]\n",
      "3.8e-05 [2.64, 11.02, 0.61, 0.13, 0.56]\n",
      "3.8e-05 [2.65, 11.02, 0.62, 0.13, 0.56]\n",
      "3.8e-05 [2.63, 17.38, 0.64, 0.13, 0.85]\n",
      "3.8e-05 [2.63, 17.97, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.63, 17.98, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.64, 17.97, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.64, 17.97, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.63, 17.99, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.64, 18.02, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.65, 10.02, 0.62, 0.13, 0.51]\n",
      "3.8e-05 [2.65, 10.02, 0.62, 0.13, 0.51]\n",
      "3.8e-05 [2.65, 9.98, 0.61, 0.13, 0.51]\n",
      "3.8e-05 [2.66, 9.92, 0.62, 0.13, 0.51]\n",
      "3.8e-05 [2.66, 9.75, 0.62, 0.13, 0.5]\n",
      "3.8e-05 [2.64, 18.43, 0.65, 0.13, 0.9]\n",
      "3.9e-05 [2.64, 18.83, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.67, 9.02, 0.62, 0.13, 0.47]\n",
      "3.9e-05 [2.67, 8.9, 0.62, 0.13, 0.46]\n",
      "3.9e-05 [2.66, 18.69, 0.66, 0.13, 0.91]\n",
      "3.9e-05 [2.64, 18.95, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 18.96, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 18.97, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 18.97, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 18.98, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 18.99, 0.65, 0.13, 0.93]\n",
      "3.9e-05 [2.64, 19.0, 0.65, 0.13, 0.93]\n",
      "3.9e-05 [2.59, 18.96, 0.61, 0.12, 0.92]\n",
      "3.9e-05 [2.68, 8.49, 0.62, 0.13, 0.44]\n",
      "3.9e-05 [2.68, 8.32, 0.62, 0.13, 0.43]\n",
      "3.9e-05 [2.64, 19.32, 0.65, 0.13, 0.94]\n",
      "3.9e-05 [2.54, 11.01, 0.54, 0.12, 0.56]\n",
      "3.9e-05 [2.55, 18.96, 0.58, 0.12, 0.92]\n",
      "3.9e-05 [2.68, 8.05, 0.62, 0.13, 0.42]\n",
      "3.9e-05 [2.69, 8.04, 0.62, 0.13, 0.42]\n",
      "3.9e-05 [2.69, 8.01, 0.62, 0.13, 0.42]\n",
      "3.9e-05 [2.69, 8.0, 0.62, 0.13, 0.42]\n",
      "3.9e-05 [2.64, 19.66, 0.65, 0.13, 0.96]\n",
      "3.9e-05 [2.64, 19.67, 0.65, 0.13, 0.96]\n",
      "3.9e-05 [2.64, 19.93, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.93, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.96, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.97, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.97, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.97, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.97, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.97, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 19.99, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 20.0, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 20.02, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 20.03, 0.65, 0.13, 0.97]\n",
      "3.9e-05 [2.64, 20.08, 0.65, 0.13, 0.98]\n",
      "3.9e-05 [2.69, 7.02, 0.61, 0.13, 0.37]\n",
      "3.9e-05 [2.69, 7.01, 0.61, 0.13, 0.37]\n",
      "3.9e-05 [2.69, 6.99, 0.61, 0.13, 0.37]\n",
      "3.9e-05 [2.64, 20.31, 0.65, 0.13, 0.99]\n",
      "3.9e-05 [2.64, 20.4, 0.65, 0.13, 0.99]\n",
      "3.9e-05 [2.65, 20.66, 0.66, 0.13, 1.0]\n",
      "3.9e-05 [2.64, 20.69, 0.65, 0.13, 1.0]\n",
      "3.9e-05 [2.69, 6.06, 0.59, 0.13, 0.32]\n",
      "3.9e-05 [2.64, 20.92, 0.65, 0.13, 1.01]\n",
      "3.9e-05 [2.69, 6.03, 0.59, 0.13, 0.32]\n",
      "4e-05 [2.65, 20.92, 0.66, 0.13, 1.01]\n",
      "4e-05 [2.65, 20.96, 0.66, 0.13, 1.02]\n",
      "4e-05 [2.64, 20.97, 0.65, 0.13, 1.02]\n",
      "4e-05 [2.64, 20.97, 0.65, 0.13, 1.02]\n",
      "Best fit parameters: A=13.545790330643507, B=1219794.5001654897, E=1.8321804108342388, alpha=0.12475927715307902, beta=0.697048119911779\n",
      "Scaling: \n",
      "         compute parameters (B) tokens (B)  ratio predicted loss\n",
      "0  1.250000e+18           0.06       3.69  65.39           3.55\n",
      "1  5.010000e+18           0.18       4.56  24.87           3.32\n",
      "2  1.980000e+19           0.59       5.61   9.55           3.12\n",
      "3  1.000000e+21          16.37      10.18   0.62           2.68\n",
      "4  1.000000e+23         813.51      20.49   0.03           2.35\n",
      "Best loss: 3.785262501411214e-05\n"
     ]
    }
   ],
   "source": [
    "fit_from_scratch(N, D, losses, indices, obj=huber_loss_objective, method='L-BFGS-B', use_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best loss: 3.787865285492144e-05 : best params : [ 2.61536334 13.17836074  0.60802206  0.12569826  0.65869289]\n",
      "best loss: 3.7868732772649915e-05 : best params : [ 2.61919501 13.48844606  0.61195846  0.12609469  0.67303807]\n",
      "best loss: 3.785211688583612e-05 : best params : [ 2.59429889 13.88903306  0.59651292  0.12353386  0.69097174]\n",
      "best loss: 3.7851041770112685e-05 : best params : [ 2.6030215  14.00963747  0.60357835  0.12445792  0.69678976]\n",
      "best loss: 3.7851041770112685e-05 : best params : [ 2.6030215  14.00963747  0.60357835  0.12445792  0.69678976]\n",
      "best loss: 3.7851041770112685e-05 : best params : [ 2.6030215  14.00963747  0.60357835  0.12445792  0.69678976]\n",
      "best loss: 3.7851041770112685e-05 : best params : [ 2.6030215  14.00963747  0.60357835  0.12445792  0.69678976]\n",
      "best loss: 3.784998907087201e-05 : best params : [ 2.60202834 14.05801008  0.60305319  0.12436498  0.69892796]\n",
      "best loss: 3.784998907087201e-05 : best params : [ 2.60202834 14.05801008  0.60305319  0.12436498  0.69892796]\n",
      "best loss: 3.784998907087201e-05 : best params : [ 2.60202834 14.05801008  0.60305319  0.12436498  0.69892796]\n",
      "3.8e-05 [2.6, 14.06, 0.6, 0.12, 0.7]\n",
      "3.8e-05 [2.6, 14.01, 0.6, 0.12, 0.7]\n",
      "3.8e-05 [2.59, 13.89, 0.6, 0.12, 0.69]\n",
      "3.8e-05 [2.62, 13.99, 0.61, 0.13, 0.7]\n",
      "3.8e-05 [2.62, 14.11, 0.62, 0.13, 0.7]\n",
      "3.8e-05 [2.62, 13.92, 0.61, 0.13, 0.69]\n",
      "3.8e-05 [2.62, 13.49, 0.61, 0.13, 0.67]\n",
      "3.8e-05 [2.61, 14.46, 0.61, 0.13, 0.72]\n",
      "3.8e-05 [2.62, 13.98, 0.62, 0.13, 0.7]\n",
      "3.8e-05 [2.6, 14.97, 0.6, 0.12, 0.74]\n",
      "3.8e-05 [2.59, 15.12, 0.6, 0.12, 0.75]\n",
      "3.8e-05 [2.62, 13.24, 0.61, 0.13, 0.66]\n",
      "3.8e-05 [2.61, 14.8, 0.61, 0.13, 0.73]\n",
      "3.8e-05 [2.62, 13.18, 0.61, 0.13, 0.66]\n",
      "3.8e-05 [2.59, 15.18, 0.6, 0.12, 0.75]\n",
      "3.8e-05 [2.61, 15.03, 0.61, 0.13, 0.74]\n",
      "3.8e-05 [2.61, 14.97, 0.61, 0.13, 0.74]\n",
      "3.8e-05 [2.62, 13.02, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.62, 13.01, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.63, 13.02, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.63, 13.02, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.63, 13.0, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.62, 12.98, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.62, 14.97, 0.62, 0.13, 0.74]\n",
      "3.8e-05 [2.61, 13.01, 0.6, 0.13, 0.65]\n",
      "3.8e-05 [2.62, 15.02, 0.62, 0.13, 0.74]\n",
      "3.8e-05 [2.62, 15.05, 0.62, 0.13, 0.74]\n",
      "3.8e-05 [2.62, 15.08, 0.62, 0.13, 0.75]\n",
      "3.8e-05 [2.62, 15.13, 0.62, 0.13, 0.75]\n",
      "3.8e-05 [2.63, 12.89, 0.61, 0.13, 0.65]\n",
      "3.8e-05 [2.63, 14.98, 0.63, 0.13, 0.74]\n",
      "3.8e-05 [2.63, 12.74, 0.61, 0.13, 0.64]\n",
      "3.8e-05 [2.63, 12.73, 0.61, 0.13, 0.64]\n",
      "3.8e-05 [2.64, 13.01, 0.63, 0.13, 0.65]\n",
      "3.8e-05 [2.61, 15.68, 0.62, 0.13, 0.77]\n",
      "3.8e-05 [2.6, 15.98, 0.61, 0.12, 0.79]\n",
      "3.8e-05 [2.63, 15.95, 0.63, 0.13, 0.79]\n",
      "3.8e-05 [2.62, 16.01, 0.62, 0.13, 0.79]\n",
      "3.8e-05 [2.62, 16.01, 0.63, 0.13, 0.79]\n",
      "3.8e-05 [2.63, 12.02, 0.61, 0.13, 0.61]\n",
      "3.8e-05 [2.63, 12.02, 0.61, 0.13, 0.61]\n",
      "3.8e-05 [2.63, 11.98, 0.61, 0.13, 0.6]\n",
      "3.8e-05 [2.65, 12.3, 0.63, 0.13, 0.62]\n",
      "3.8e-05 [2.67, 15.0, 0.65, 0.13, 0.74]\n",
      "3.8e-05 [2.65, 15.98, 0.64, 0.13, 0.79]\n",
      "3.8e-05 [2.63, 16.57, 0.63, 0.13, 0.81]\n",
      "3.8e-05 [2.57, 16.99, 0.59, 0.12, 0.83]\n",
      "3.8e-05 [2.53, 16.95, 0.56, 0.12, 0.83]\n",
      "3.8e-05 [2.63, 16.95, 0.64, 0.13, 0.83]\n",
      "3.8e-05 [2.64, 16.9, 0.64, 0.13, 0.83]\n",
      "3.8e-05 [2.52, 17.01, 0.55, 0.12, 0.83]\n",
      "3.8e-05 [2.63, 17.0, 0.64, 0.13, 0.83]\n",
      "3.8e-05 [2.63, 17.01, 0.63, 0.13, 0.83]\n",
      "3.8e-05 [2.64, 17.01, 0.64, 0.13, 0.83]\n",
      "3.8e-05 [2.64, 17.05, 0.64, 0.13, 0.84]\n",
      "3.8e-05 [2.65, 10.98, 0.62, 0.13, 0.56]\n",
      "3.8e-05 [2.64, 17.26, 0.64, 0.13, 0.85]\n",
      "3.8e-05 [2.65, 10.72, 0.61, 0.13, 0.55]\n",
      "3.8e-05 [2.65, 10.58, 0.62, 0.13, 0.54]\n",
      "3.8e-05 [2.48, 15.97, 0.5, 0.11, 0.78]\n",
      "3.8e-05 [2.61, 17.97, 0.62, 0.13, 0.88]\n",
      "3.8e-05 [2.63, 17.98, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.64, 17.95, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.63, 17.98, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.64, 18.01, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.63, 18.02, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.63, 18.03, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.64, 18.05, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.63, 18.09, 0.64, 0.13, 0.88]\n",
      "3.8e-05 [2.54, 17.97, 0.57, 0.12, 0.88]\n",
      "3.8e-05 [2.65, 10.02, 0.62, 0.13, 0.51]\n",
      "3.8e-05 [2.66, 10.02, 0.62, 0.13, 0.51]\n",
      "3.8e-05 [2.65, 10.0, 0.62, 0.13, 0.51]\n",
      "3.8e-05 [2.47, 15.0, 0.49, 0.11, 0.74]\n",
      "3.8e-05 [2.66, 18.0, 0.66, 0.13, 0.88]\n",
      "3.8e-05 [2.63, 10.02, 0.6, 0.13, 0.51]\n",
      "3.9e-05 [2.66, 9.09, 0.61, 0.13, 0.47]\n",
      "3.9e-05 [2.64, 18.8, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.67, 9.06, 0.62, 0.13, 0.47]\n",
      "3.9e-05 [2.64, 18.91, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 18.96, 0.65, 0.13, 0.92]\n",
      "3.9e-05 [2.64, 18.99, 0.65, 0.13, 0.93]\n",
      "3.9e-05 [2.64, 19.0, 0.65, 0.13, 0.93]\n",
      "3.9e-05 [2.64, 19.01, 0.65, 0.13, 0.93]\n",
      "3.9e-05 [2.64, 19.03, 0.65, 0.13, 0.93]\n",
      "3.9e-05 [2.64, 19.02, 0.65, 0.13, 0.93]\n",
      "3.9e-05 [2.46, 18.02, 0.49, 0.11, 0.88]\n",
      "3.9e-05 [2.64, 19.26, 0.65, 0.13, 0.94]\n",
      "3.9e-05 [2.68, 8.37, 0.62, 0.13, 0.43]\n",
      "3.9e-05 [2.64, 19.45, 0.65, 0.13, 0.95]\n",
      "3.9e-05 [2.64, 19.47, 0.65, 0.13, 0.95]\n",
      "3.9e-05 [2.68, 8.02, 0.62, 0.13, 0.42]\n",
      "3.9e-05 [2.68, 8.01, 0.62, 0.13, 0.42]\n",
      "3.9e-05 [2.69, 8.01, 0.62, 0.13, 0.42]\n",
      "3.9e-05 [2.45, 14.02, 0.46, 0.11, 0.69]\n",
      "3.9e-05 [2.69, 7.95, 0.62, 0.13, 0.42]\n",
      "3.9e-05 [2.44, 17.95, 0.46, 0.11, 0.87]\n",
      "3.9e-05 [2.69, 7.63, 0.62, 0.13, 0.4]\n",
      "3.9e-05 [2.69, 7.62, 0.62, 0.13, 0.4]\n",
      "3.9e-05 [2.64, 19.74, 0.65, 0.13, 0.96]\n",
      "Best fit parameters: A=13.491074800067071, B=1274430.6377290722, E=1.8276905799841974, alpha=0.12436498024380671, beta=0.6989279632415041\n",
      "Scaling: \n",
      "         compute parameters (B) tokens (B)  ratio predicted loss\n",
      "0  1.250000e+18           0.06       3.71  66.24           3.56\n",
      "1  5.010000e+18           0.18       4.58  25.14           3.32\n",
      "2  1.980000e+19           0.59       5.64   9.63           3.12\n",
      "3  1.000000e+21          16.35      10.20   0.62           2.68\n",
      "4  1.000000e+23         815.22      20.44   0.03           2.35\n",
      "Best loss: 3.784998907087201e-05\n"
     ]
    }
   ],
   "source": [
    "fit_from_scratch(N, D, losses, indices, obj=huber_loss_objective, method='L-BFGS-B', add_sigma=True, use_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap step 1000 completed\n",
      "Bootstrap step 2000 completed\n",
      "Bootstrap step 3000 completed\n",
      "Bootstrap step 4000 completed\n",
      "Best fit parameters: A=10.80565184461938, B=303.218332151444, E=1.1907012653301834, alpha=0.09982740395511044, beta=0.2855258906813433\n",
      "Scaling: \n",
      "         compute parameters (B) tokens (B)  ratio predicted loss\n",
      "0  1.250000e+18           0.08       2.68  34.57           3.57\n",
      "1  5.010000e+18           0.22       3.85  17.71           3.34\n",
      "2  1.980000e+19           0.60       5.49   9.13           3.13\n",
      "3  1.000000e+21          10.99      15.16   1.38           2.64\n",
      "4  1.000000e+23         333.40      49.99   0.15           2.22\n",
      "Best loss: 4.745822146499232e-05\n"
     ]
    }
   ],
   "source": [
    "fit_from_chinchilla_random(N, D, losses, random_indices, obj=huber_loss_objective, method='BFGS', use_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfit_from_scratch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_sigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m, in \u001b[0;36mfit_from_scratch\u001b[0;34m(N, D, losses, indices, obj, method, use_grad, add_sigma)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_sigma:\n\u001b[1;32m     46\u001b[0m     init_params \u001b[38;5;241m=\u001b[39m init_params \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 47\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_grad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m results_dict[\u001b[38;5;28mtuple\u001b[39m(init_params)] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: result\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: result\u001b[38;5;241m.\u001b[39mfun}\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mfun \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "File \u001b[0;32m/fsx-onellm/margaretli/miniforge3/envs/mlexp/lib/python3.9/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/fsx-onellm/margaretli/miniforge3/envs/mlexp/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:307\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 307\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    313\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m/fsx-onellm/margaretli/miniforge3/envs/mlexp/lib/python3.9/site-packages/scipy/optimize/_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/fsx-onellm/margaretli/miniforge3/envs/mlexp/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m/fsx-onellm/margaretli/miniforge3/envs/mlexp/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/fsx-onellm/margaretli/miniforge3/envs/mlexp/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fsx-onellm/margaretli/miniforge3/envs/mlexp/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(params, N, D, losses, log_sum_exp_fn)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(params, N, D, losses, log_sum_exp_fn\u001b[38;5;241m=\u001b[39mlog_sum_exp_chinch):\n\u001b[0;32m---> 44\u001b[0m     a, b, e, alpha, beta, sigma \u001b[38;5;241m=\u001b[39m params\n\u001b[1;32m     45\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m log_sum_exp_fn(a, b, e, alpha, beta, N, D)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(huber_logpdf(np\u001b[38;5;241m.\u001b[39mlog(losses), loc\u001b[38;5;241m=\u001b[39mpredictions, scale\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mexp(sigma), delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 5)"
     ]
    }
   ],
   "source": [
    "fit_from_scratch(N, D, losses, indices, obj=objective, method='L-BFGS-B', add_sigma=True, use_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_from_chinchilla_random(N, D, losses, random_indices, obj=objective, method='BFGS', add_sigma=True, use_grad=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
