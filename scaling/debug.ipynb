{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import *\n",
    "from database import *\n",
    "from simulator import *\n",
    "from visualizer import *\n",
    "import pandas as pd\n",
    "import random\n",
    "from _metrics import asymmetric_mae, huber, log_huber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'mlexp (Python 3.9.15)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "param_grid = dict(\n",
    "        E=np.linspace(1, 5, 5),\n",
    "        A=np.linspace(100, 4000, 10),\n",
    "        B=np.linspace(100, 4000, 10),\n",
    "        alpha=np.linspace(0.05, 0.9, 10),\n",
    "        beta=np.linspace(0.05, 0.9, 10),\n",
    "    ),\n",
    "\n",
    "\n",
    "# _const = dict(\n",
    "#     log_N=np.log(_df.N.values.astype(DTYPE)),\n",
    "#     log_D=np.log(_df.D.values.astype(DTYPE)),\n",
    "#     # y_true=_df.loss.values.astype(DTYPE),\n",
    "#     y_true=_df[loss_name].values.astype(DTYPE),\n",
    "#     weights=weights,\n",
    "# )\n",
    "\n",
    "# The absolute value range affects the differential optimization\n",
    "_autoscale_range = np.array(list(map(np.ptp, param_grid.values())))\n",
    "# In case of any axis with a single initial value:\n",
    "_autoscale_range[_autoscale_range == 0] = 1.0\n",
    "\n",
    "if parallel:\n",
    "    # GLobal declarations for `multiprocessing`\n",
    "    global initial_guesses\n",
    "    global _optimize_params\n",
    "\n",
    "initial_guesses = list(itertools.product(*param_grid.values()))\n",
    "initial_guesses /= _autoscale_range\n",
    "\n",
    "print(_autoscale_range)\n",
    "\n",
    "print(initial_guesses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        lr  Avg Train Loss  Max Train Loss  C4 Eval PPL  Wiki Eval PPL  \\\n",
      "4    0.004           4.957           5.185      146.194        216.300   \n",
      "12   0.004           4.717           5.044      119.259        166.031   \n",
      "17   0.004           4.453           4.659       89.148        115.380   \n",
      "21   0.004           3.999           4.218       61.665         70.939   \n",
      "25   0.004           3.736           4.058       46.984         47.592   \n",
      "29   0.004           3.533           3.764       39.858         35.852   \n",
      "33   0.002           4.925           5.171      141.632        224.183   \n",
      "37   0.002           4.683           5.024      114.945        168.517   \n",
      "42   0.004           3.880           4.109       55.653         62.826   \n",
      "46   0.004           3.603           3.968       41.767         39.030   \n",
      "50   0.004           3.448           3.680       36.775         32.393   \n",
      "54   0.004           3.384           3.679       34.108         29.042   \n",
      "58   0.004           3.312           3.599       32.342         26.802   \n",
      "62   0.002           4.643           4.973      110.797        162.453   \n",
      "66   0.002           4.321           4.538       79.123        102.901   \n",
      "74   0.002           3.835           4.065       53.307         59.325   \n",
      "79   0.002           3.549           3.924       39.643         36.808   \n",
      "84   0.004           3.390           3.625       34.712         30.659   \n",
      "88   0.004           3.250           3.529       30.378         25.104   \n",
      "92   0.004           3.201           3.438       28.080         22.326   \n",
      "96   0.002           4.971           5.216      147.519        236.586   \n",
      "101  0.004           4.734           5.059      120.748        175.021   \n",
      "105  0.004           4.341           4.559       80.451        106.232   \n",
      "109  0.004           3.820           4.048       52.464         60.188   \n",
      "113  0.004           3.521           3.902       38.631         35.934   \n",
      "117  0.004           3.348           3.581       33.370         29.302   \n",
      "121  0.004           3.295           3.593       31.304         26.355   \n",
      "125  0.004           3.215           3.495       29.460         24.049   \n",
      "129  0.002           3.850           4.069       53.599         63.878   \n",
      "133  0.002           3.512           3.893       37.959         36.726   \n",
      "141  0.002           3.320           3.551       32.382         29.010   \n",
      "146  0.002           3.245           3.546       29.709         25.333   \n",
      "150  0.002           3.163           3.441       27.800         22.986   \n",
      "154  0.002           3.114           3.467       26.355         21.154   \n",
      "158  0.002           3.106           3.342       25.531         20.245   \n",
      "162  0.002           3.013           3.292       24.104         18.628   \n",
      "167  0.002           3.765           3.988       49.540         58.191   \n",
      "171  0.002           3.445           3.838       35.667         34.073   \n",
      "175  0.002           3.260           3.492       30.456         27.134   \n",
      "179  0.002           3.174           3.488       27.671         23.507   \n",
      "183  0.002           3.092           3.371       25.875         21.285   \n",
      "187  0.002           3.037           3.276       23.837         18.773   \n",
      "191  0.002           2.944           3.227       22.524         17.297   \n",
      "195  0.002           2.908           3.304       21.594         16.315   \n",
      "199  0.002           3.654           3.889       45.125         42.395   \n",
      "203  0.002           3.050           3.327       24.771         20.313   \n",
      "211  0.002           2.993           3.229       22.757         17.906   \n",
      "216  0.002           2.899           3.182       21.522         16.467   \n",
      "220  0.002           2.864           3.263       20.629         15.535   \n",
      "224  0.002           2.815           3.157       19.575         14.377   \n",
      "234  0.001           2.725           3.060       17.837         12.987   \n",
      "244  0.001           2.490           2.921       14.456          9.629   \n",
      "\n",
      "     C4 Eval Loss  Wiki Eval Loss             C             D            N  \n",
      "4           4.985           5.377  4.470000e+16  2.097152e+08   12047168.0  \n",
      "12          4.781           5.112  5.590000e+16  2.621440e+08   12047168.0  \n",
      "17          4.490           4.748  8.050000e+16  3.774874e+08   12047168.0  \n",
      "21          4.122           4.262  1.120000e+17  5.242880e+08   12047168.0  \n",
      "25          3.850           3.863  1.680000e+17  7.864320e+08   12047168.0  \n",
      "29          3.685           3.579  2.240000e+17  1.048576e+09   12047168.0  \n",
      "33          4.953           5.412  5.540000e+16  2.097152e+08   16865856.0  \n",
      "37          4.744           5.127  6.930000e+16  2.621440e+08   16865856.0  \n",
      "42          4.019           4.140  1.390000e+17  5.242880e+08   16865856.0  \n",
      "46          3.732           3.664  2.080000e+17  7.864320e+08   16865856.0  \n",
      "50          3.605           3.478  2.770000e+17  1.048576e+09   16865856.0  \n",
      "54          3.530           3.369  3.460000e+17  1.310720e+09   16865856.0  \n",
      "58          3.476           3.288  4.160000e+17  1.572864e+09   16865856.0  \n",
      "62          4.708           5.090  9.170000e+16  2.621440e+08   25174528.0  \n",
      "66          4.371           4.634  1.320000e+17  3.774874e+08   25174528.0  \n",
      "74          3.976           4.083  1.830000e+17  5.242880e+08   25174528.0  \n",
      "79          3.680           3.606  2.750000e+17  7.864320e+08   25174528.0  \n",
      "84          3.547           3.423  3.670000e+17  1.048576e+09   25174528.0  \n",
      "88          3.414           3.223  5.500000e+17  1.572864e+09   25174528.0  \n",
      "92          3.335           3.106  7.340000e+17  2.097152e+09   25174528.0  \n",
      "96          4.994           5.466  9.500000e+16  2.097152e+08   35842752.0  \n",
      "101         4.794           5.165  1.190000e+17  2.621440e+08   35842752.0  \n",
      "105         4.388           4.666  1.710000e+17  3.774874e+08   35842752.0  \n",
      "109         3.960           4.097  2.370000e+17  5.242880e+08   35842752.0  \n",
      "113         3.654           3.582  3.560000e+17  7.864320e+08   35842752.0  \n",
      "117         3.508           3.378  4.750000e+17  1.048576e+09   35842752.0  \n",
      "121         3.444           3.272  5.940000e+17  1.310720e+09   35842752.0  \n",
      "125         3.383           3.180  7.120000e+17  1.572864e+09   35842752.0  \n",
      "129         3.982           4.157  3.020000e+17  5.242880e+08   49165440.0  \n",
      "133         3.636           3.603  4.520000e+17  7.864320e+08   49165440.0  \n",
      "141         3.478           3.368  6.030000e+17  1.048576e+09   49165440.0  \n",
      "146         3.391           3.232  7.540000e+17  1.310720e+09   49165440.0  \n",
      "150         3.325           3.135  9.050000e+17  1.572864e+09   49165440.0  \n",
      "154         3.272           3.052  1.090000e+18  1.887437e+09   49165440.0  \n",
      "158         3.240           3.008  1.210000e+18  2.097152e+09   49165440.0  \n",
      "162         3.182           2.925  1.510000e+18  2.621440e+09   49165440.0  \n",
      "167         3.903           4.064  4.040000e+17  5.242880e+08   71386304.0  \n",
      "171         3.574           3.528  6.060000e+17  7.864320e+08   71386304.0  \n",
      "175         3.416           3.301  8.080000e+17  1.048576e+09   71386304.0  \n",
      "179         3.320           3.157  1.010000e+18  1.310720e+09   71386304.0  \n",
      "183         3.253           3.058  1.210000e+18  1.572864e+09   71386304.0  \n",
      "187         3.171           2.932  1.620000e+18  2.097152e+09   71386304.0  \n",
      "191         3.115           2.851  2.020000e+18  2.621440e+09   71386304.0  \n",
      "195         3.072           2.792  2.430000e+18  3.145728e+09   71386304.0  \n",
      "199         3.809           3.747  1.060000e+18  1.048576e+09   99112704.0  \n",
      "203         3.210           3.011  1.580000e+18  1.572864e+09   99112704.0  \n",
      "211         3.125           2.885  2.110000e+18  2.097152e+09   99112704.0  \n",
      "216         3.069           2.801  2.640000e+18  2.621440e+09   99112704.0  \n",
      "220         3.027           2.743  3.170000e+18  3.145728e+09   99112704.0  \n",
      "224         2.974           2.666  4.220000e+18  4.194304e+09   99112704.0  \n",
      "234         2.881           2.564  7.560000e+18  4.194304e+09  199101120.0  \n",
      "244         2.671           2.265  1.510000e+19  8.388608e+09  393268480.0  \n"
     ]
    }
   ],
   "source": [
    "loss_name = \"C4 Eval Loss\"\n",
    "csv_file = \"/fsx-onellm/margaretli/env_srcs/xlf/xlformers_n/scaling/data/data.csv\"\n",
    "\n",
    "col_names = ['C', 'D', 'N', 'lr', 'Avg Train Loss', 'Max Train Loss', 'C4 Eval PPL', 'Wiki Eval PPL', 'C4 Eval Loss', 'Wiki Eval Loss']\n",
    "\n",
    "def read_data(csv_file, loss_name='C4 Eval Loss', col_names=col_names):\n",
    "    mins_only = []\n",
    "    df = pd.read_csv(csv_file, usecols=col_names,)\n",
    "    df.dropna(subset=[loss_name], inplace=True)\n",
    "\n",
    "    df = df.loc[(df['lr'] >= 0)]\n",
    "    n_vals = df['N'].unique()\n",
    "    d_vals = sorted(df['D'].unique())\n",
    "    for n in n_vals:\n",
    "        for d in d_vals:\n",
    "            cd_df = df[(df['N'] == n) & (df['D'] == d)]\n",
    "            if cd_df.empty:\n",
    "                continue\n",
    "            min_index = cd_df['C4 Eval Loss'].idxmin()\n",
    "            # print(min_index)\n",
    "            # print(cd_df)\n",
    "            # print(cd_df.loc[min_index])\n",
    "            mins_only.append(cd_df.loc[min_index])\n",
    "\n",
    "    mins_only_df = pd.DataFrame(mins_only)\n",
    "\n",
    "    print(mins_only_df)\n",
    "    # df.rename(columns={})\n",
    "    return mins_only_df\n",
    "\n",
    "_df = read_data(csv_file=csv_file, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = asymmetric_mae\n",
    "\n",
    "FLOAT_TINY = np.finfo(np.single).tiny\n",
    "FLOAT_LOGMAX = np.log(np.finfo(np.single).max)\n",
    "\n",
    "_const = dict(\n",
    "    log_N=np.log(_df.N.values.astype(DTYPE)),\n",
    "    log_D=np.log(_df.D.values.astype(DTYPE)),\n",
    "    # y_true=_df.loss.values.astype(DTYPE),\n",
    "    y_true=_df[loss_name].values.astype(DTYPE),\n",
    ")\n",
    "\n",
    "def calc_loss(E, A, B, alpha, beta, loss_fn=loss_fn):\n",
    "    # # Ensure the log scale for `a` and `b` but `E`.\n",
    "    # # Inspect the user-specified initial keys requiring `exp`/`log` transformation\n",
    "    # if \"e\" in __param_grid_keys:\n",
    "    #     E = np.exp(E)\n",
    "    # if \"A\" in __param_grid_keys:\n",
    "    a = np.log(np.clip(A, FLOAT_TINY, None))\n",
    "    # if \"B\" in __param_grid_keys:\n",
    "    b = np.log(np.clip(B, FLOAT_TINY, None))\n",
    "\n",
    "    log_term_2nd = a - alpha * _const[\"log_N\"]\n",
    "    log_term_3rd = b - beta * _const[\"log_D\"]\n",
    "    log_term_2nd = np.clip(log_term_2nd, None, FLOAT_LOGMAX)\n",
    "    log_term_3rd = np.clip(log_term_3rd, None, FLOAT_LOGMAX)\n",
    "    y_pred = E + np.exp(log_term_2nd) + np.exp(log_term_3rd)\n",
    "\n",
    "    losses = loss_fn(_const[\"y_true\"], y_pred)\n",
    "\n",
    "    # if weight_fn:\n",
    "    #     losses = losses * _const[\"weights\"]\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40568422569214721568\n",
      "0.08190579904408484987\n"
     ]
    }
   ],
   "source": [
    "# chinch_dict=dict(\n",
    "#     E=1.69337368,\n",
    "#     A=406.401018,\n",
    "#     B=410.722827,\n",
    "#     alpha=0.33917084,\n",
    "#     beta=0.2849083,\n",
    "# )\n",
    "\n",
    "chinch_dict=dict(\n",
    "    E=1,\n",
    "    A=406.401018,\n",
    "    B=410.722827,\n",
    "    alpha=0.33917084,\n",
    "    beta=0.2839083,\n",
    ")\n",
    "\n",
    "print(calc_loss(**chinch_dict))\n",
    "\n",
    "\n",
    "found_params_dict=dict(\n",
    "    E=2.018,\n",
    "    A=42.89,\n",
    "    B=33920,\n",
    "    alpha=0.2608,\n",
    "    beta=0.5013,\n",
    ")\n",
    "\n",
    "print(calc_loss(**found_params_dict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinchilla_flops_per_tok(seq_len, vocab_size, d_model, num_heads, num_layers, ffw_size):\n",
    "    \"\"\" \n",
    "    Calculate total number of FLOPs, see Chinchilla \n",
    "    paper Appendix F as reference: https://arxiv.org/pdf/2203.15556.pdf\n",
    "    \"\"\" \n",
    "    key_size = d_model // num_heads\n",
    "\n",
    "    # embeddings\n",
    "    embeddings = 2 * vocab_size * d_model\n",
    "\n",
    "    # attention\n",
    "    # key, query, value projections\n",
    "    attention = 2 * 3 * d_model * (key_size * num_heads)\n",
    "    # key @ query logits\n",
    "    attlogits = 2 * seq_len * (key_size * num_heads)\n",
    "    # softmax\n",
    "    attsoftmax = 3 * num_heads * seq_len# 3* is for subtract (max), exp, divide (?)\n",
    "    # softmax @ value reductions\n",
    "    attvalue = 2 * seq_len * (key_size * num_heads)\n",
    "    # final linear\n",
    "    attlinear = 2 * (key_size * num_heads) * d_model\n",
    "    att = attention + attlogits + attsoftmax + attvalue + attlinear\n",
    "    # feed forward\n",
    "    dense = 2 * (d_model * ffw_size + d_model * ffw_size)\n",
    "\n",
    "    # logits\n",
    "    logits = 2 * d_model * vocab_size\n",
    "    \n",
    "    # this is what you'd expect:\n",
    "    # forward_flops = embeddings + num_layers * (att + dense) + logits\n",
    "    # but:\n",
    "    # per author correspondence apparently there is typo in the paper,\n",
    "    # they do not count embeddings and logits to repro table 4. So instead:\n",
    "    forward_flops = num_layers * (att + dense)\n",
    "    backward_flops = 2 * forward_flops # as in Kaplan et al. 2020\n",
    "    total_flops = forward_flops + backward_flops\n",
    "\n",
    "    return total_flops"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
